{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries for dataset management, model building, training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from json import JSONEncoder\n",
    "import random\n",
    "import re\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_vec(c, n_vocab=90):\n",
    "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
    "    return ord(c) % n_vocab\n",
    "\n",
    "def word_to_indices(word, n_vocab=90):\n",
    "    \"\"\"\n",
    "    Converts a word or list of words into a list of indices.\n",
    "    Each character is mapped to an index based on the vocabulary size.\n",
    "    \"\"\"\n",
    "    if isinstance(word, list):  # If input is a list of words\n",
    "        res = []\n",
    "        for stringa in word:\n",
    "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
    "        return res\n",
    "    else:  # If input is a single word\n",
    "        return [ord(c) % n_vocab for c in word]\n",
    "\n",
    "def process_x(raw_x_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw input data into padded sequences of indices.\n",
    "    Ensures all sequences are of uniform length.\n",
    "    \"\"\"\n",
    "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
    "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
    "    return torch.tensor(x_batch, dtype=torch.long)\n",
    "\n",
    "def process_y(raw_y_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw target data into padded sequences of indices.\n",
    "    Shifts the sequence by one character to the right.\n",
    "    y[1:seq_len + 1] takes the input data, right shift of an\n",
    "    element and uses the next element of the sequence to fill\n",
    "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
    "    added to reach the desired sequence length.\n",
    "    \"\"\"\n",
    "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
    "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
    "    return torch.tensor(y_batch, dtype=torch.long)\n",
    "\n",
    "def create_batches(data, batch_size, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Creates batches of input and target data from dialogues.\n",
    "    Each batch contains sequences of uniform length.\n",
    "    \"\"\"\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    dialogues = list(data.values())\n",
    "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
    "\n",
    "    batch = []\n",
    "    for dialogue in dialogues:\n",
    "        batch.append(dialogue)\n",
    "        if len(batch) == batch_size:\n",
    "            x_batch = process_x(batch, seq_len, n_vocab)\n",
    "            y_batch = process_y(batch, seq_len, n_vocab)\n",
    "            x_batches.append(x_batch)\n",
    "            y_batches.append(y_batch)\n",
    "            batch = []\n",
    "\n",
    "    # Add the last batch if it's not full\n",
    "    if batch:\n",
    "        x_batch = process_x(batch, seq_len, n_vocab)\n",
    "        y_batch = process_y(batch, seq_len, n_vocab)\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    return x_batches, y_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current script directory\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Set up paths relative to script location\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"processed_data\")\n",
    "\n",
    "# Create directorY if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "class NumpyTensorEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.ndarray, torch.Tensor)):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def save_results_federated(model, train_accuracies, train_losses, test_accuracy, test_loss, client_selection, filename):\n",
    "    \"\"\"\n",
    "    Save federated learning results in both .pth and .json formats.\n",
    "    Handles PyTorch tensors and NumPy arrays serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "        \n",
    "        # Define file paths\n",
    "        filepath_pth = os.path.join(subfolder_path, f\"{filename}.pth\")\n",
    "        filepath_json = os.path.join(subfolder_path, f\"{filename}.json\")\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'model_state': model.state_dict(),\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'train_losses': train_losses,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'client_count': client_selection\n",
    "        }\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        torch.save(results, filepath_pth)\n",
    "        \n",
    "        # Save JSON metrics with custom encoder\n",
    "        with open(filepath_json, 'w') as json_file:\n",
    "            json.dump(results, json_file, indent=4, cls=NumpyTensorEncoder)\n",
    "            \n",
    "        print(f\"Results saved successfully to {subfolder_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_federated(train_losses, train_accuracies, filename):   \n",
    "    # Plot federated training performance\n",
    "    subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "    # Create epochs list\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    \n",
    "    # Create subplot figure\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot Training Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Federated Training Loss', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Training Accuracy \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Federated Training Accuracy', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{file_path}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "def plot_sampling_distributions(client_sel_count, filename):\n",
    "    # Plot sampling distributions \n",
    "    subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, filename)\n",
    "    \n",
    "    \"\"\"Plot client selection distribution\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    num_clients = len(client_sel_count)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(num_clients), client_sel_count, alpha=0.7, edgecolor='blue')\n",
    "    plt.xlabel(\"Client ID\", fontsize=14)\n",
    "    plt.ylabel(\"Selection Count\", fontsize=14)\n",
    "    plt.title(\"Client Selection Distribution\", fontsize=16)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{file_path}.png\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to handle the Shakespeare dataset in a way suitable for PyTorch.\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, clients=None, seq_length=80, n_vocab=90):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading and preprocessing the data.\n",
    "        Args:\n",
    "        - data_path: Path to the JSON file containing the dataset.\n",
    "        - clients: List of client IDs to load data for (default: all clients).\n",
    "        - seq_length: Sequence length for character-level data.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length  # Sequence length for the model\n",
    "        self.n_vocab = n_vocab  # Vocabulary size\n",
    "\n",
    "        # Create character mappings\n",
    "        self.data = list(text.values())  # Convert the dictionary values to a list\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the input-target pair at the specified index.\n",
    "        \"\"\"\n",
    "        diag = self.data[idx]\n",
    "        x = process_x(diag, self.seq_length, self.n_vocab)\n",
    "        y = process_y(diag, self.seq_length, self.n_vocab)\n",
    "        return x[0], y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character-level LSTM model for Shakespeare data.\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab=90, embedding_dim=8, hidden_dim=256, seq_length=80, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        Args:\n",
    "        - n_vocab: Number of unique characters in the dataset.\n",
    "        - embedding_dim: Size of the character embedding.\n",
    "        - hidden_dim: Number of LSTM hidden units.\n",
    "        - num_layers: Number of LSTM layers.\n",
    "        - seq_length: Length of input sequences.\n",
    "        \"\"\"\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Character embedding layer: Maps indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_dim)  # Character embedding layer.\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_first = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM first layer\n",
    "        self.lstm_second = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM second layer.\n",
    "        \n",
    "        # Fully connected layer: Maps LSTM output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)  # Output layer (vocab_size outputs).\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "        - x: Input batch (character indices).\n",
    "        - hidden: Hidden state for LSTM (default: None, initialized internally).\n",
    "        Returns:\n",
    "        - Output logits and the updated hidden state.\n",
    "        \"\"\"\n",
    "        # Embedding layer: Convert indices to embeddings.\n",
    "        x = self.embedding(x)  \n",
    "\n",
    "        # First LSTM\n",
    "        output, _ = self.lstm_first(x)  # Process through first LSTM layer.\n",
    "        # Second LSTM\n",
    "        output, hidden = self.lstm_second(x)  # Process through second LSTM layer.\n",
    "        # Fully connected layer: Generate logits for each character.\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # Note: Softmax is not applied here because CrossEntropyLoss in PyTorch\n",
    "        # combines the softmax operation with the computation of the loss. \n",
    "        # Adding softmax here would be redundant and could introduce numerical instability.\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initializes hidden and cell states for the LSTM.\n",
    "        Args:\n",
    "        - batch_size: Number of sequences in the batch.\n",
    "        Returns:\n",
    "        - A tuple of zero-initialized hidden and cell states.\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device))\n",
    "\n",
    "# Evaluate model performance on a dataset.\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    Args:\n",
    "    - model: Trained model.\n",
    "    - data_loader: DataLoader for the evaluation dataset.\n",
    "    - criterion: Loss function.\n",
    "    - device: Device to evaluate on (CPU/GPU).\n",
    "    Returns:\n",
    "    - Average loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Initialize hidden state\n",
    "            state = model.init_hidden(inputs.size(0), device) \n",
    "            state = (state[0].to(device), state[1].to(device)) \n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.view(-1, model.n_vocab)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)  # Compute loss.\n",
    "            total_loss += loss.item()\n",
    "            _, predictions = outputs.max(1)\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)  # Compute average loss.\n",
    "    accuracy = (correct_predictions / total_samples ) * 100  # Compute accuracy.\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Clients and Create Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample clients uniformly for a round of training.\n",
    "def sample_clients_uniform(num_clients, fraction):\n",
    "    \"\"\"\n",
    "    Sample a fraction of clients uniformly.\n",
    "    Args:\n",
    "    - clients: List of all clients.\n",
    "    - fraction: Fraction of clients to sample.\n",
    "    Returns:\n",
    "    - A list of selected clients.\n",
    "    \"\"\"\n",
    "    num_selected = int(fraction * num_clients)  # Compute number of selected clients.\n",
    "    selected = np.random.choice(num_clients, num_selected, replace=False)\n",
    "    return selected.tolist()  # Convert to list for consistent indexing\n",
    "\n",
    "\n",
    "\n",
    "# Sample clients skewed using Dirichlet distribution.\n",
    "def sample_clients_skewed(num_clients, fraction, gamma):\n",
    "    \"\"\"\n",
    "    Sample a fraction of clients based on Dirichlet distribution.\n",
    "    Args:\n",
    "    - clients: List of all clients.\n",
    "    - fraction: Fraction of clients to sample.\n",
    "    - gamma: Skewness parameter for Dirichlet distribution.\n",
    "    Returns:\n",
    "    - List of selected clients and their probabilities.\n",
    "    \"\"\"\n",
    "    num_selected = int(fraction * num_clients)\n",
    "    probabilities = np.random.dirichlet([gamma] * num_clients)  # Generate skewed probabilities.\n",
    "    selected_indices = np.random.choice(num_clients, num_selected, replace=False, p=probabilities)\n",
    "    return selected_indices.tolist(), probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, data_loader, id_client, model, char_to_idx, device):\n",
    "        self.data = data_loader\n",
    "        self.id_client = id_client\n",
    "        self.model = model.to(device)\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.device = device\n",
    "    \n",
    "\n",
    "    def train_local_model(self, data_loader, criterion, optimizer, local_steps, device):\n",
    "        \"\"\"Train model locally with memory optimization\"\"\"\n",
    "        cudnn.benchmark  \n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        try:\n",
    "            for _ in range(local_steps):\n",
    "                for i, (inputs, targets) in enumerate(data_loader):\n",
    "                    # Move data to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Initialize hidden state\n",
    "                    state = self.model.init_hidden(inputs.size(0), device)\n",
    "                    state = tuple(s.to(device) for s in state)\n",
    "                    # Forward pass with memory efficiency\n",
    "                    outputs, _ = self.model(inputs, state)\n",
    "                    \n",
    "                    # Ensure targets have the correct shape\n",
    "                    if targets.dim() == 1:  # Targets might need an expansion\n",
    "                        targets = targets.unsqueeze(1).expand(-1, inputs.shape[1])\n",
    "\n",
    "                    # Reshape outputs and targets to align properly\n",
    "                    outputs = outputs.reshape(-1, outputs.size(-1))  # Flatten to [batch_size * seq_length, vocab_size]\n",
    "                    targets = targets.reshape(-1)  # Flatten to [batch_size * seq_length]\n",
    "\n",
    "                    # print(f\"Inputs shape: {inputs.shape}\")\n",
    "                    # print(f\"Outputs shape: {outputs.shape}\")\n",
    "                    # print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "                    assert outputs.size(0) == targets.size(0), f\"Shape mismatch: outputs={outputs.shape}, targets={targets.shape}\"\n",
    "\n",
    "                    loss = criterion(outputs, targets)\n",
    "                   \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                     \n",
    "                    optimizer.step()\n",
    "                   \n",
    "                    # Update metrics \n",
    "                    total_loss += loss.item() * targets.numel()  # Weight by batch\n",
    "                    _, predictions = outputs.max(1)  # Get predicted characters\n",
    "                    correct_predictions += (predictions == targets).sum().item()  # Compare at character level\n",
    "                    total_samples += targets.numel()  # Count characters, not sequences\n",
    "                \n",
    "            # Compute final metrics\n",
    "            avg_loss = (total_loss / total_samples ) \n",
    "            accuracy = ((correct_predictions / total_samples) * 100) \n",
    "            \n",
    "            print(f\"Client {self.id_client}: Loss={avg_loss:.4f}, Acc={accuracy:.4f}\")\n",
    "            return self.model.state_dict(), avg_loss, accuracy\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error training client {self.id_client}: {str(e)}\")\n",
    "            return None, float('inf'), 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, test_data, val_data, global_model, char_to_idx, device):\n",
    "        self.test_data = test_data\n",
    "        self.val_data = val_data\n",
    "        self.clients = None\n",
    "        self.global_model = global_model\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.device = device\n",
    "        self.losses_round = []\n",
    "        self.accuracies_round = []\n",
    "        self.client_selected = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "\n",
    "    # Federated training with FedAvg.\n",
    "    def train_federated(self, train_loader, criterion, rounds, num_classes, num_clients, fraction, device, lr, momentum, batch_size, wd, seq_length, C=0.1, local_steps=4, iid=True, participation=\"uniform\", gamma=None):\n",
    "        \"\"\"\n",
    "        Train the global model using federated averaging (FedAvg).\n",
    "        Args:\n",
    "        - self -> containing global_model: Global model to train.\n",
    "        - data_path: Path to dataset.\n",
    "        - criterion: Loss function.\n",
    "        - rounds: Number of communication rounds.\n",
    "        - num_clients: Number of all clients.\n",
    "        - fraction: Fraction of clients to select in each round.\n",
    "        - device: Device to train on (CPU/GPU).\n",
    "        - seq_length: Sequence length for local models.\n",
    "        - local_steps: Number of local training steps per client.\n",
    "        - participation: Participation scheme ('uniform' or 'skewed').\n",
    "        - gamma: Skewness parameter for Dirichlet distribution (if 'skewed').\n",
    "        Returns:\n",
    "        - List of global losses and sampling distributions (if skewed).\n",
    "        \"\"\"\n",
    "\n",
    "        self.global_model.to(device)\n",
    "\n",
    "        sampling_distributions = []  # Track sampling probabilities for skewed participation.\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        client_sel_count = np.zeros(num_clients)\n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # shards = create_sharding(train_loader.dataset, num_clients, num_classes, iid) #each shard represent the training data for one client\n",
    "        shards = self.sharding(train_loader)\n",
    "        assert len(shards) == num_clients, f\"Expected {num_clients} shards, got {len(shards)}\"\n",
    "        client_sizes = [len(shard) for shard in shards]\n",
    "\n",
    "        self.global_model.to(self.device)\n",
    "\n",
    "        for round_num in range(rounds):\n",
    "            client_states = []\n",
    "            client_losses = []\n",
    "            client_accuracies = []\n",
    "            print(f\"Round {round_num + 1}/{rounds}\")\n",
    "            if participation == \"uniform\":\n",
    "                selected_clients = sample_clients_uniform(num_clients, fraction)  # Uniform sampling.\n",
    "                sampling_distributions.append([1 / num_clients] * num_clients) # Uniform probabilities.\n",
    "            elif participation == \"skewed\":\n",
    "                selected_clients, probabilities = sample_clients_skewed(num_clients, fraction, gamma)  # Skewed sampling.\n",
    "                sampling_distributions.append(probabilities)  # Store probabilities.\n",
    "            \n",
    "            # Train each selected client.\n",
    "            for id_client in selected_clients:\n",
    "                client_sel_count[id_client] += 1\n",
    "                \n",
    "                local_model = deepcopy(self.global_model)\n",
    "                optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "                \n",
    "                # Load client's dataset.\n",
    "                client_loader = DataLoader(shards[id_client], batch_size, shuffle=True)\n",
    "\n",
    "                client = Client(client_loader, id_client, local_model, self.char_to_idx, self.device)\n",
    "\n",
    "                # Train local model.\n",
    "                client_local_state, client_loss, client_accuracy = client.train_local_model(client_loader, criterion, optimizer, local_steps, device)\n",
    "                client_states.append(client_local_state)\n",
    "                client_losses.append(client_loss)\n",
    "                client_accuracies.append(client_accuracy)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # FedAvg aggregation\n",
    "            # Aggregate client updates using FedAvg\n",
    "            if client_states:\n",
    "                global_dict = deepcopy(self.global_model.state_dict())\n",
    "\n",
    "                # Initialize empty tensors for aggregation\n",
    "                for k in global_dict.keys():\n",
    "                    global_dict[k] = torch.zeros_like(global_dict[k])\n",
    "\n",
    "                total_samples = sum(client_sizes[i] for i in selected_clients)\n",
    "\n",
    "                # Aggregate client updates based on data proportions\n",
    "                for state, id_client, loss, accuracy in zip(client_states, selected_clients, client_losses, client_accuracies):\n",
    "                    weight = client_sizes[id_client] / total_samples\n",
    "                    for k in global_dict:\n",
    "                        global_dict[k] += state[k] * weight  \n",
    "\n",
    "                # Update the global model\n",
    "                self.global_model.load_state_dict(global_dict)\n",
    "\n",
    "                # Calculate weighted global metrics\n",
    "                weighted_loss = sum(loss * (client_sizes[i] / total_samples) for i, loss in zip(selected_clients, client_losses))\n",
    "                weighted_accuracy = sum(accuracy * (client_sizes[i] / total_samples) for i, accuracy in zip(selected_clients, client_accuracies))\n",
    "\n",
    "                train_losses.append(weighted_loss)\n",
    "                train_accuracies.append(weighted_accuracy)\n",
    "\n",
    "                print(f\"Round {round_num + 1} - Global Loss: {weighted_loss:.4f}, Accuracy: {weighted_accuracy:.2f}%\")\n",
    "            \n",
    "                if weighted_loss < best_loss:\n",
    "                    best_loss = weighted_loss\n",
    "                    best_model = deepcopy(self.global_model.state_dict())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        self.global_model.load_state_dict(best_model)\n",
    "\n",
    "        return self.global_model, train_accuracies, train_losses, client_sel_count\n",
    "\n",
    "    def char_to_tensor(self, characters):\n",
    "        indices = [self.char_to_idx.get(char, self.char_to_idx['']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def sharding(self, data):\n",
    "        \"\"\"\n",
    "        Prepares individual shards for each user, returning a Subset for each.\n",
    "\n",
    "        Args:\n",
    "            data: Dataset containing user data.\n",
    "            char_to_idx: Character to index mapping dictionary for character conversion.\n",
    "\n",
    "        Returns:\n",
    "            List of Subsets, one per client.    \n",
    "        \"\"\"\n",
    "        subsets = []\n",
    "\n",
    "        for user in data['users']:\n",
    "            input_tensors = []\n",
    "            target_tensors = []\n",
    "\n",
    "            for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
    "              input_tensors.append(self.char_to_tensor(entry)) \n",
    "              target_tensors.append(self.char_to_tensor(target)) \n",
    "\n",
    "            padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=self.char_to_idx[''])\n",
    "            targets = torch.cat(target_tensors)\n",
    "\n",
    "            dataset = TensorDataset(padded_inputs, targets)\n",
    "\n",
    "            subsets.append(Subset(dataset, torch.arange(len(targets))))\n",
    "\n",
    "        return subsets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 253\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll experiments completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 53\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_data\u001b[39m\u001b[38;5;124m'\u001b[39m][user][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_data\u001b[39m\u001b[38;5;124m'\u001b[39m][user][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     52\u001b[0m         input_tensors\u001b[38;5;241m.\u001b[39mappend(char_to_tensor(entry))  \u001b[38;5;66;03m# Use the full sequence of x\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m         target_tensors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchar_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Directly use the corresponding y as target\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Padding e creazione di DataLoader\u001b[39;00m\n\u001b[0;32m     56\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(input_tensors, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mchar_to_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[96], line 45\u001b[0m, in \u001b[0;36mmain.<locals>.char_to_tensor\u001b[1;34m(characters)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchar_to_tensor\u001b[39m(characters):\n\u001b[0;32m     44\u001b[0m     indices \u001b[38;5;241m=\u001b[39m [char_to_idx\u001b[38;5;241m.\u001b[39mget(char, char_to_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m characters] \u001b[38;5;66;03m# Get the index for the character. If not found, use the index for padding.\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Dataset and training configurations\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "    epochs = 20  # Number of epochs for centralized training\n",
    "    fraction = 0.1  # Fraction of clients to select each round\n",
    "    seq_length = 80  # Sequence length for LSTM inputs   \n",
    "    batch_size = 4 # For local training\n",
    "    n_vocab = 90 # Character number in vobulary (ASCII)\n",
    "    embedding_size = 8\n",
    "    hidden_dim = 256\n",
    "    train_split = 0.8 # In LEAF Dataset the common split used is 80/20\n",
    "    momentum = 0\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 0.0001\n",
    "    C = 0.1\n",
    "\n",
    "    # Load data\n",
    "    base_path = os.path.join('leaf', 'data', 'shakespeare', 'data')\n",
    "    train_path = os.path.join(base_path, 'train', 'all_data_iid_01_1_keep_0_train_9.json')\n",
    "    test_path = os.path.join(base_path, 'test', 'all_data_iid_01_1_keep_0_test_9.json')\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_dataset = json.load(f)\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "\n",
    "    num_clients = len(train_dataset['users'])\n",
    "    print(\"Number of clients:\", num_clients) \n",
    "    users = train_dataset['users']\n",
    "    user_data = train_dataset['user_data']\n",
    "\n",
    "    all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
    "    chars = sorted(set(all_texts))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "    # Padding character\n",
    "    char_to_idx[''] = len(char_to_idx)\n",
    "\n",
    "    # Function to convert character values into indices\n",
    "    def char_to_tensor(characters):\n",
    "        indices = [char_to_idx.get(char, char_to_idx['']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    # Prepare the training data_loader\n",
    "    input_tensors = []\n",
    "    target_tensors = []\n",
    "    for user in train_dataset['users']:\n",
    "        for entry, target in zip(train_dataset['user_data'][user]['x'], train_dataset['user_data'][user]['y']):\n",
    "            input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
    "            target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
    "\n",
    "    # Padding and DataLoader creation\n",
    "    padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[''])\n",
    "    targets = torch.cat(target_tensors)\n",
    "    dataset = TensorDataset(padded_inputs, targets)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Prepare the validation data_loader\n",
    "    train_size = int(0.9 * len(train_dataset))  # 90% of data for training\n",
    "    val_size = len(train_dataset) - train_size  # 10% of data for validation\n",
    "    train_data, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Prepare the testing data_loader\n",
    "    input_tensors = []\n",
    "    target_tensors = []\n",
    "    for user in test_dataset['users']:\n",
    "        for entry, target in zip(test_dataset['user_data'][user]['x'], test_dataset['user_data'][user]['y']):\n",
    "            input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
    "            target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
    "\n",
    "    # Padding e creazione di DataLoader\n",
    "    padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[''])\n",
    "    targets = torch.cat(target_tensors)\n",
    "    dataset = TensorDataset(padded_inputs, targets)\n",
    "    \n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    # EXPERIMENTS\n",
    "\n",
    "    local_steps = [4, 8, 16] #what is called J -> # Number of local training steps\n",
    "    # Scale the number of rounds inversely with J to maintain a constant computational budget\n",
    "    num_rounds = {4: 200, 8: 100, 16: 50} # Number of federated communication rounds\n",
    "\n",
    "\n",
    "    # The first FL baseline\n",
    "    print(\"FIRST FL BASELINE\")\n",
    "\n",
    "    num_clients = num_clients\n",
    "    num_classes = 100 \n",
    "    iid = True #iid\n",
    "    C = 0.1\n",
    "    local_steps = 4\n",
    "\n",
    "    rounds = num_rounds[local_steps]\n",
    "\n",
    "    global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    server = Server(test_loader, validation_dataset, global_model, char_to_idx, device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    global_model, train_accuracies, train_losses, client_sel_count = server.train_federated(\n",
    "        train_dataset, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "        batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    filename = f\"First_baseline_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "    save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    # The impact of client participation\n",
    "    print(\"THE IMPACT OF CLIENT PARTECIPATION\")\n",
    "\n",
    "    num_clients = num_clients\n",
    "    num_classes = 100\n",
    "    iid = True #iid\n",
    "    C = 0.1\n",
    "\n",
    "    local_steps = 4\n",
    "\n",
    "    rounds = num_rounds[local_steps]\n",
    "\n",
    "    print(\"Uniform partecipation\")\n",
    "\n",
    "    global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    server = Server(test_loader, validation_dataset, global_model, device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    global_model, train_accuracies, train_losses, client_sel_count = server.train_federated(\n",
    "        train_dataset, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "        batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    filename = f\"Uniform_Client_partecipation_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "    save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    num_clients = num_clients\n",
    "    num_classes = 100\n",
    "    iid = True #iid\n",
    "    C = 0.1\n",
    "    local_steps = 4\n",
    "\n",
    "    rounds = num_rounds[local_steps]\n",
    "\n",
    "    print(\"Skewed partecipation\")\n",
    "\n",
    "    # Values of gamma to test\n",
    "    gamma_values = [0.1, 0.5, 1.0, 5.0]  # Skewness parameter for Dirichlet sampling\n",
    "\n",
    "    for gamma in gamma_values:\n",
    "        global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "        server = Server(test_loader, validation_dataset, global_model, device)\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "        global_model, train_accuracies, train_losses, client_sel_count = server.train_federated(\n",
    "            train_dataset, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "            batch_size, weight_decay, seq_length, C, local_steps, iid, \"skewed\", gamma)\n",
    "\n",
    "        # Test\n",
    "        test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "        print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        filename = f\"Skewed_Client_partecipation_Gamma_{gamma}_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "        save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "        plot_results_federated(train_losses, train_accuracies, filename)\n",
    "        plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    # Simulate heterogeneous distributions \n",
    "    print(\"SIMULATE HETEROGENEOUS DISTRIBUTIONS\")\n",
    "\n",
    "\n",
    "    print(\"Non-iid shardings\")\n",
    "    num_clients = 100\n",
    "    num_classes = [1, 5, 10, 50]\n",
    "    iid = False # non-iid\n",
    "    C = 0.1\n",
    "\n",
    "    local_steps_list = [4, 8, 16]  # Varying local steps\n",
    "\n",
    "    for nc in num_classes:\n",
    "        for local_steps in local_steps_list:\n",
    "            rounds = num_rounds[local_steps]\n",
    "            global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "            server = Server(test_loader, validation_dataset, global_model, device)\n",
    "            criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "            optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "            global_model, train_accuracies, train_losses, client_sel_count = server.train_federated(\n",
    "                train_dataset, criterion, rounds, nc, num_clients, fraction, device, learning_rate, momentum, \n",
    "                batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "            # Test\n",
    "            test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "            print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "            filename = f\"Non_iid_Num_classes_{nc}_local_steps_{local_steps}\"\n",
    "            save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "            plot_results_federated(train_losses, train_accuracies, filename)\n",
    "            plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    print(\"iid shardings\")\n",
    "    num_clients = 100\n",
    "    num_classes = 100 \n",
    "    iid = True # iid\n",
    "    C = 0.1\n",
    "\n",
    "    local_steps_list = [4, 8, 16]  # Varying local steps\n",
    "\n",
    "    for local_steps in local_steps_list:\n",
    "\n",
    "        rounds = num_rounds[local_steps]\n",
    "    \n",
    "        global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "        server = Server(test_loader, validation_dataset, global_model, device)\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "        global_model, train_accuracies, train_losses, client_sel_count = server.train_federated(\n",
    "            train_dataset, criterion, rounds, nc, num_clients, fraction, device, learning_rate, momentum, \n",
    "            batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "        # Test\n",
    "        test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "        print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        filename = f\"iid_Num_classes_{nc}_local_steps_{local_steps}\"\n",
    "        save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "        plot_results_federated(train_losses, train_accuracies, filename)\n",
    "        plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "    \n",
    "    print(\"All experiments completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-project-NFYoYubB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
