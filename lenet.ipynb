{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Project 5 - Federated Learning - Track B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %!pip install torch torchvision tqdm tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision.datasets.cifar import CIFAR100\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(\"\\r\\033[K\" + msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    COLORS = {\n",
    "        \"DEBUG\": \"\\033[1;34m\",\n",
    "        \"INFO\": \"\\033[1;32m\",\n",
    "        \"WARNING\": \"\\033[1;33m\",\n",
    "        \"ERROR\": \"\\033[1;31m\",\n",
    "        \"CRITICAL\": \"\\033[1;35m\",\n",
    "        \"RESET\": \"\\033[0m\",\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        levelname = record.levelname\n",
    "        if levelname in self.COLORS:\n",
    "            record.levelname = (\n",
    "                f\"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}\"\n",
    "            )\n",
    "        return super().format(record)\n",
    "\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()\n",
    "    root_logger.setLevel(level)\n",
    "\n",
    "    tqdm_handler = TqdmLoggingHandler()\n",
    "    formatter = ColoredFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    tqdm_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(tqdm_handler)\n",
    "\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BaseConfig:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CPU_COUNT = os.cpu_count() or 1\n",
    "    NUM_WORKERS = min(4, CPU_COUNT)\n",
    "    SEED = 42\n",
    "\n",
    "    # Paths\n",
    "    ROOT_DIR: Path = Path.cwd()\n",
    "    CONFIGS_DIR: Path = ROOT_DIR / \"configs\"\n",
    "    DATA_DIR: Path = ROOT_DIR / \"data\"\n",
    "    MODELS_DIR: Path = ROOT_DIR / \"models\"\n",
    "    RESULTS_DIR: Path = ROOT_DIR / \"results\"\n",
    "    RUNS_DIR: Path = ROOT_DIR / \"runs\"\n",
    "    OLD_RUNS_DIR: Path = RUNS_DIR / \"old_runs\"\n",
    "\n",
    "    # Training Parameters\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.01\n",
    "    NUM_EPOCHS = 20\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 4e-4\n",
    "    NUM_CLASSES = 100\n",
    "\n",
    "\n",
    "# Create directories\n",
    "config = BaseConfig()\n",
    "for dir_path in [\n",
    "    config.DATA_DIR,\n",
    "    config.MODELS_DIR,\n",
    "    config.RESULTS_DIR,\n",
    "    config.CONFIGS_DIR,\n",
    "    config.RUNS_DIR,\n",
    "    config.OLD_RUNS_DIR,\n",
    "]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FederatedConfig(BaseConfig):\n",
    "    \"\"\"Federated Learning specific configuration.\"\"\"\n",
    "\n",
    "    NUM_CLIENTS: int = 100\n",
    "    PARTICIPATION_RATE: float = 0.1\n",
    "    LOCAL_EPOCHS: int = 4\n",
    "    NUM_ROUNDS: int = 2000\n",
    "    CLASSES_PER_CLIENT: Optional[int] = None  # None for IID\n",
    "    PARTICIPATION_MODE: str = \"uniform\"\n",
    "    DIRICHLET_ALPHA: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.connected = nn.Sequential(\n",
    "            nn.Linear(5 * 5 * 64, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(192, config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.connected(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsManager:\n",
    "    \"\"\"Manages logging and visualization of training metrics.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BaseConfig,\n",
    "        model_name: str,\n",
    "        training_type: Literal[\"centralized\", \"federated\"],\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.config = config\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        # Archive old runs\n",
    "        old_runs = list(config.RUNS_DIR.glob(f\"{training_type}_{model_name}_*\"))\n",
    "        if old_runs:\n",
    "            archive_dir = config.OLD_RUNS_DIR\n",
    "            archive_dir.mkdir(exist_ok=True)\n",
    "            for run in old_runs:\n",
    "                run.rename(archive_dir / run.name)\n",
    "\n",
    "        # Create descriptive run name for FL experiments\n",
    "        if training_type == \"federated\" and isinstance(config, FederatedConfig):\n",
    "            distribution = (\n",
    "                \"iid\"\n",
    "                if config.CLASSES_PER_CLIENT is None\n",
    "                else f\"noniid_{config.CLASSES_PER_CLIENT}cls\"\n",
    "            )\n",
    "            participation = f\"{config.PARTICIPATION_MODE}\"\n",
    "            if config.PARTICIPATION_MODE == \"skewed\":\n",
    "                participation += f\"_alpha{config.DIRICHLET_ALPHA}\"\n",
    "            clients_info = f\"C{config.NUM_CLIENTS}_P{config.PARTICIPATION_RATE}_E{config.LOCAL_EPOCHS}\"\n",
    "            experiment_suffix = f\"{distribution}_{participation}_{clients_info}\"\n",
    "        else:\n",
    "            experiment_suffix = experiment_name if experiment_name else timestamp\n",
    "\n",
    "        run_name = f\"{training_type}_{model_name}_{experiment_suffix}\"\n",
    "        self.writer = SummaryWriter(config.RUNS_DIR / run_name)\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        split: Literal[\"train\", \"validation\", \"test\"],\n",
    "        loss: float,\n",
    "        accuracy: float,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Log metrics for specified split.\"\"\"\n",
    "        self.writer.add_scalars(\"metrics/loss\", {split: loss}, step)\n",
    "        self.writer.add_scalars(\"metrics/accuracy\", {split: accuracy}, step)\n",
    "\n",
    "    def log_fl_metrics(\n",
    "        self, round_idx: int, metrics: Dict, client_stats: Optional[Dict] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Log federated learning specific metrics.\"\"\"\n",
    "        # Log test metrics\n",
    "        self.log_metrics(\n",
    "            \"test\", metrics[\"test_loss\"], metrics[\"test_accuracy\"], round_idx\n",
    "        )\n",
    "\n",
    "        # Log client participation if available\n",
    "        if client_stats:\n",
    "            self.writer.add_scalars(\"federated/client_stats\", client_stats, round_idx)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close TensorBoard writer.\"\"\"\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar100DatasetManager:\n",
    "    config: BaseConfig\n",
    "    validation_split: float\n",
    "    train_transform: transforms.Compose\n",
    "    test_transform: transforms.Compose\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    val_loader: DataLoader[CIFAR100]\n",
    "    test_loader: DataLoader[CIFAR100]\n",
    "\n",
    "    def __init__(self, config: BaseConfig, validation_split: float = 0.1) -> None:\n",
    "        self.config = config\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_loader, self.val_loader, self.test_loader = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "    ) -> Tuple[DataLoader[CIFAR100], DataLoader[CIFAR100], DataLoader[CIFAR100]]:\n",
    "        full_trainset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "\n",
    "        train_size: int = int((1 - self.validation_split) * len(full_trainset))\n",
    "        val_size: int = len(full_trainset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_trainset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.config.SEED),\n",
    "        )\n",
    "\n",
    "        test_dataset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "\n",
    "        loader_kwargs = {\"num_workers\": self.config.NUM_WORKERS, \"pin_memory\": True}\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        test_loader: DataLoader[CIFAR100] = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.train_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.val_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.test_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedTrainer:\n",
    "    model: LeNet\n",
    "    config: BaseConfig\n",
    "    device: torch.device\n",
    "    metrics: MetricsManager\n",
    "\n",
    "    def __init__(self, model: LeNet, config: BaseConfig) -> None:\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"centralized\"\n",
    "        )\n",
    "\n",
    "    def evaluate_model(\n",
    "        self, model: LeNet, data_loader: DataLoader[CIFAR100]\n",
    "    ) -> Tuple[float, float]:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_loss: float = total_loss / total\n",
    "        accuracy: float = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        max_epochs: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, T_max=self.config.NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.config.NUM_EPOCHS\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        train_acc = 0.0\n",
    "        avg_train_loss = 0.0\n",
    "\n",
    "        epoch_pbar = tqdm(\n",
    "            range(max_epochs or self.config.NUM_EPOCHS),\n",
    "            desc=\"Training\",\n",
    "            unit=\"epoch\",\n",
    "            position=0,\n",
    "            leave=True,\n",
    "        )\n",
    "        epoch = 0\n",
    "\n",
    "        try:\n",
    "            for epoch in epoch_pbar:\n",
    "                self.model.train()\n",
    "                train_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                # batch_pbar = tqdm(\n",
    "                #     train_loader,\n",
    "                #     desc=f\"Epoch {epoch}\",\n",
    "                #     colour=\"yellow\",\n",
    "                #     unit=\"batch\",\n",
    "                #     leave=True,\n",
    "                #     position=1,\n",
    "                #     bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}'\n",
    "                # )\n",
    "                for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                    # Update metrics\n",
    "                    train_acc = 100.0 * correct / total\n",
    "                    avg_train_loss = train_loss / (batch_idx + 1)\n",
    "\n",
    "                    global_step = epoch * len(train_loader) + batch_idx\n",
    "                    self.metrics.log_metrics(\n",
    "                        \"train\", avg_train_loss, train_acc, global_step\n",
    "                    )\n",
    "\n",
    "                    # batch_pbar.set_postfix(\n",
    "                    #     {\"loss\": f\"{avg_train_loss:.3f}\", \"acc\": f\"{train_acc:.2f}%\"}\n",
    "                    # )\n",
    "\n",
    "                # Validation phase\n",
    "                val_loss, val_acc = self.evaluate_model(self.model, val_loader)\n",
    "                scheduler.step()\n",
    "\n",
    "                self.metrics.log_metrics(\"validation\", val_loss, val_acc, epoch)\n",
    "                epoch_pbar.set_postfix(\n",
    "                    {\n",
    "                        \"ep\": f\"{epoch+1}/{max_epochs or self.config.NUM_EPOCHS}\",\n",
    "                        \"tr_loss\": f\"{avg_train_loss:.3f}\",\n",
    "                        \"tr_acc\": f\"{train_acc:.1f}%\",\n",
    "                        \"val_loss\": f\"{val_loss:.3f}\",\n",
    "                        \"val_acc\": f\"{val_acc:.1f}%\",\n",
    "                    },\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                logging.info(\"Training completed!\")\n",
    "            # Final evaluation\n",
    "            if best_model_state is not None:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "            test_loss, test_acc = self.evaluate_model(self.model, test_loader)\n",
    "            self.metrics.log_metrics(\"test\", test_loss, test_acc, epoch)\n",
    "            logging.info(\n",
    "                f\"Final Test Results - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSharder:\n",
    "    def create_iid_shards(\n",
    "        self, dataset: Dataset, num_clients: int\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        # FIXME: use numpy as in the non-iid case\n",
    "        total_size = len(dataset)\n",
    "        shard_size = total_size // num_clients\n",
    "        indices = torch.randperm(total_size).tolist()\n",
    "        return [\n",
    "            Subset(dataset, indices[i : i + shard_size])\n",
    "            for i in range(0, total_size, shard_size)\n",
    "        ]\n",
    "\n",
    "    def create_noniid_shards(\n",
    "        self,\n",
    "        dataset: Dataset[CIFAR100] | Subset[CIFAR100],\n",
    "        num_clients: int,\n",
    "        classes_per_client: int,\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        # Get labels - handle both CIFAR100 and Subset cases\n",
    "        if isinstance(dataset, CIFAR100):\n",
    "            labels = torch.tensor(dataset.targets)\n",
    "        else:\n",
    "            # For Subset, get the dataset's targets using indices\n",
    "            labels = torch.tensor([dataset.dataset.targets[i] for i in dataset.indices])\n",
    "        # Get labels directly from CIFAR100 targets\n",
    "        labels = torch.tensor(dataset.targets)\n",
    "        class_indices = {i: [] for i in range(100)}\n",
    "\n",
    "        for idx, label in enumerate(labels):\n",
    "            class_indices[int(label)].append(idx)  # Convert to int explicitly\n",
    "\n",
    "        client_indices = [[] for _ in range(num_clients)]\n",
    "        classes = list(range(100))\n",
    "\n",
    "        for client_id in range(num_clients):\n",
    "            client_classes = np.random.choice(\n",
    "                classes, size=classes_per_client, replace=False\n",
    "            )\n",
    "            for class_id in client_classes:\n",
    "                # Convert numpy types to Python int\n",
    "                class_id_int = int(class_id)\n",
    "                samples_per_client = len(class_indices[class_id_int]) // num_clients\n",
    "\n",
    "                client_indices[client_id].extend(\n",
    "                    np.random.choice(\n",
    "                        class_indices[class_id_int],\n",
    "                        size=samples_per_client,\n",
    "                        replace=False,\n",
    "                    ).tolist()  # Convert to list for Subset\n",
    "                )\n",
    "\n",
    "        return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "\n",
    "class ClientManager:\n",
    "    def __init__(\n",
    "        self, num_clients, participation_rate, mode=\"uniform\", dirichlet_alpha=None\n",
    "    ) -> None:\n",
    "        self.num_clients = num_clients\n",
    "        self.num_selected = int(participation_rate * num_clients)\n",
    "\n",
    "        if mode == \"skewed\":\n",
    "            if dirichlet_alpha is None:\n",
    "                raise ValueError(\"dirichlet_alpha required for skewed mode\")\n",
    "            self.selection_probs = np.random.dirichlet([dirichlet_alpha] * num_clients)\n",
    "        else:\n",
    "            self.selection_probs = np.ones(num_clients) / num_clients\n",
    "\n",
    "    def select_clients(self) -> npt.NDArray[np.int64]:\n",
    "        return np.random.choice(\n",
    "            self.num_clients,\n",
    "            size=self.num_selected,\n",
    "            replace=False,\n",
    "            p=self.selection_probs,\n",
    "        )\n",
    "\n",
    "\n",
    "class FederatedClient:\n",
    "    config: FederatedConfig\n",
    "    model: LeNet\n",
    "    client_id: int\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    local_epochs: int\n",
    "    device: torch.device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model: LeNet,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "        local_epochs: int,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.client_id = client_id\n",
    "        self.train_loader = train_loader\n",
    "        self.local_epochs = local_epochs\n",
    "        self.device = config.DEVICE\n",
    "\n",
    "    # TODO: call the centralized trainer?\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class FederatedServer:\n",
    "    global_model: LeNet\n",
    "    client_manager: ClientManager\n",
    "    test_loader: DataLoader[CIFAR100]\n",
    "    config: FederatedConfig\n",
    "    device: torch.device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        client_manager: ClientManager,\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "    ) -> None:\n",
    "        self.global_model = model\n",
    "        self.client_manager = client_manager\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "\n",
    "    def aggregate_models(self, client_models):\n",
    "        global_dict = self.global_model.state_dict()\n",
    "\n",
    "        for k in global_dict.keys():\n",
    "            global_dict[k] = torch.stack(\n",
    "                [\n",
    "                    client_model.state_dict()[k].float()\n",
    "                    for client_model in client_models\n",
    "                ],\n",
    "                0,\n",
    "            ).mean(0)\n",
    "\n",
    "        self.global_model.load_state_dict(global_dict)\n",
    "\n",
    "    def evaluate(self):\n",
    "        return CentralizedTrainer(self.global_model, self.config).evaluate_model(\n",
    "            self.global_model, self.test_loader\n",
    "        )\n",
    "\n",
    "\n",
    "class EnhancedFederatedTrainer:\n",
    "    config: FederatedConfig\n",
    "    model: LeNet\n",
    "    device: torch.device\n",
    "    metrics: MetricsManager\n",
    "    sharder: DataSharder\n",
    "    client_manager: ClientManager\n",
    "    server: FederatedServer\n",
    "    client_loaders: List[DataLoader[CIFAR100]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        train_dataset: Dataset[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.device = config.DEVICE\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"federated\"\n",
    "        )\n",
    "\n",
    "        # Setup data sharding\n",
    "        self.sharder = DataSharder()\n",
    "        shards: List[Subset[CIFAR100]] = (\n",
    "            self.sharder.create_noniid_shards(\n",
    "                train_dataset, config.NUM_CLIENTS, config.CLASSES_PER_CLIENT\n",
    "            )\n",
    "            if config.CLASSES_PER_CLIENT\n",
    "            else self.sharder.create_iid_shards(train_dataset, config.NUM_CLIENTS)\n",
    "        )\n",
    "\n",
    "        self.client_loaders = [\n",
    "            DataLoader(\n",
    "                shard,\n",
    "                batch_size=config.BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=config.NUM_WORKERS,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            for shard in shards\n",
    "        ]\n",
    "\n",
    "        self.client_manager = ClientManager(\n",
    "            config.NUM_CLIENTS, config.PARTICIPATION_RATE\n",
    "        )\n",
    "\n",
    "        self.server = FederatedServer(model, self.client_manager, test_loader, config)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        round_pbar = tqdm(\n",
    "            range(self.config.NUM_ROUNDS),\n",
    "            desc=\"Training FL\",\n",
    "            colour=\"blue\",\n",
    "            unit=\"round\",\n",
    "            position=0,\n",
    "            leave=True,\n",
    "        )\n",
    "        try:\n",
    "            for round_idx in round_pbar:\n",
    "                selected_clients = self.client_manager.select_clients()\n",
    "\n",
    "                client_models = []\n",
    "                for client_idx in selected_clients:\n",
    "                    client = FederatedClient(\n",
    "                        client_idx,\n",
    "                        self.server.global_model,\n",
    "                        self.client_loaders[client_idx],\n",
    "                        self.config,\n",
    "                        self.config.LOCAL_EPOCHS,\n",
    "                    )\n",
    "                    client_models.append(client.train())\n",
    "\n",
    "                self.server.aggregate_models(client_models)\n",
    "\n",
    "                test_loss, accuracy = self.server.evaluate()\n",
    "                self.metrics.log_metrics(\"test\", test_loss, accuracy, round_idx)\n",
    "\n",
    "                if round_idx % 10 == 0:\n",
    "                    logging.info(\n",
    "                        f\"Round {round_idx}/{self.config.NUM_ROUNDS}: \"\n",
    "                        f\"test_loss: {test_loss:.4f}, \"\n",
    "                        f\"test_acc: {accuracy:.2f}%\"\n",
    "                    )\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = Cifar100DatasetManager(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [30:04<00:00, 90.25s/epoch, ep=20/20, tr_loss=1.574, tr_acc=56.4%, val_loss=2.083, val_acc=46.5%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K2025-01-14 13:38:19 [\u001b[1;32mINFO\u001b[0m] Training completed!\n",
      "\u001b[K2025-01-14 13:38:32 [\u001b[1;32mINFO\u001b[0m] Final Test Results - Loss: 1.9105, Accuracy: 50.30%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(config)\n",
    "trainer = CentralizedTrainer(model, config)\n",
    "trainer.train(data.train_loader, data.val_loader, data.test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_config = FederatedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|\u001b[34m          \u001b[0m| 0/2000 [05:26<?, ?round/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m fed_model \u001b[38;5;241m=\u001b[39m LeNet(config)\n\u001b[0;32m      2\u001b[0m fed_trainer \u001b[38;5;241m=\u001b[39m EnhancedFederatedTrainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mfed_model,\n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[0;32m      5\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest_loader,\n\u001b[0;32m      6\u001b[0m     config\u001b[38;5;241m=\u001b[39mfed_config,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mfed_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 240\u001b[0m, in \u001b[0;36mEnhancedFederatedTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_idx \u001b[38;5;129;01min\u001b[39;00m selected_clients:\n\u001b[0;32m    233\u001b[0m     client \u001b[38;5;241m=\u001b[39m FederatedClient(\n\u001b[0;32m    234\u001b[0m         client_idx,\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mglobal_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mLOCAL_EPOCHS,\n\u001b[0;32m    239\u001b[0m     )\n\u001b[1;32m--> 240\u001b[0m     client_models\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39maggregate_models(client_models)\n\u001b[0;32m    244\u001b[0m test_loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[1;32mIn[58], line 114\u001b[0m, in \u001b[0;36mFederatedClient.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m    108\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mLEARNING_RATE,\n\u001b[0;32m    109\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMOMENTUM,\n\u001b[0;32m    110\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mWEIGHT_DECAY,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_epochs):\n\u001b[1;32m--> 114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.8\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.8\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.8\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.8\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.12.8\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fed_model = LeNet(config)\n",
    "fed_trainer = EnhancedFederatedTrainer(\n",
    "    model=fed_model,\n",
    "    train_dataset=data.train_dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=fed_config,\n",
    ")\n",
    "fed_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_iid_config: FederatedConfig = dataclasses.replace(\n",
    "    fed_config,\n",
    "    CLASSES_PER_CLIENT=5,\n",
    ")\n",
    "\n",
    "# Train with non-iid distribution\n",
    "non_iid_model = LeNet(config)\n",
    "non_iid_trainer = EnhancedFederatedTrainer(\n",
    "    model=non_iid_model,\n",
    "    train_dataset=data.train_loader.dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=non_iid_config,\n",
    ")\n",
    "non_iid_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_config = dataclasses.replace(\n",
    "    fed_config, PARTICIPATION_MODE=\"skewed\", DIRICHLET_ALPHA=0.5\n",
    ")\n",
    "\n",
    "skewed_model = LeNet(config)\n",
    "skewed_trainer = EnhancedFederatedTrainer(\n",
    "    model=skewed_model,\n",
    "    train_dataset=data.train_loader.dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=skewed_config,\n",
    ")\n",
    "skewed_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_epochs_configs: list[FederatedConfig] = [\n",
    "    dataclasses.replace(fed_config, LOCAL_EPOCHS=e) for e in [4, 8, 16]\n",
    "]\n",
    "\n",
    "# Experiment with different client counts\n",
    "client_counts_configs = [\n",
    "    dataclasses.replace(fed_config, NUM_CLIENTS=c) for c in [50, 100, 200]\n",
    "]\n",
    "\n",
    "# Experiment with different participation rates\n",
    "participation_rates_configs = [\n",
    "    dataclasses.replace(fed_config, PARTICIPATION_RATE=r) for r in [0.05, 0.1, 0.2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "0 LOCAL_EPOCHS=4, NUM_CLIENTS=50, PARTICIPATION_RATE=0.05\n",
      "1 LOCAL_EPOCHS=4, NUM_CLIENTS=50, PARTICIPATION_RATE=0.1\n",
      "2 LOCAL_EPOCHS=4, NUM_CLIENTS=50, PARTICIPATION_RATE=0.2\n",
      "3 LOCAL_EPOCHS=4, NUM_CLIENTS=100, PARTICIPATION_RATE=0.05\n",
      "4 LOCAL_EPOCHS=4, NUM_CLIENTS=100, PARTICIPATION_RATE=0.1\n",
      "5 LOCAL_EPOCHS=4, NUM_CLIENTS=100, PARTICIPATION_RATE=0.2\n",
      "6 LOCAL_EPOCHS=4, NUM_CLIENTS=200, PARTICIPATION_RATE=0.05\n",
      "7 LOCAL_EPOCHS=4, NUM_CLIENTS=200, PARTICIPATION_RATE=0.1\n",
      "8 LOCAL_EPOCHS=4, NUM_CLIENTS=200, PARTICIPATION_RATE=0.2\n",
      "9 LOCAL_EPOCHS=8, NUM_CLIENTS=50, PARTICIPATION_RATE=0.05\n",
      "10 LOCAL_EPOCHS=8, NUM_CLIENTS=50, PARTICIPATION_RATE=0.1\n",
      "11 LOCAL_EPOCHS=8, NUM_CLIENTS=50, PARTICIPATION_RATE=0.2\n",
      "12 LOCAL_EPOCHS=8, NUM_CLIENTS=100, PARTICIPATION_RATE=0.05\n",
      "13 LOCAL_EPOCHS=8, NUM_CLIENTS=100, PARTICIPATION_RATE=0.1\n",
      "14 LOCAL_EPOCHS=8, NUM_CLIENTS=100, PARTICIPATION_RATE=0.2\n",
      "15 LOCAL_EPOCHS=8, NUM_CLIENTS=200, PARTICIPATION_RATE=0.05\n",
      "16 LOCAL_EPOCHS=8, NUM_CLIENTS=200, PARTICIPATION_RATE=0.1\n",
      "17 LOCAL_EPOCHS=8, NUM_CLIENTS=200, PARTICIPATION_RATE=0.2\n",
      "18 LOCAL_EPOCHS=16, NUM_CLIENTS=50, PARTICIPATION_RATE=0.05\n",
      "19 LOCAL_EPOCHS=16, NUM_CLIENTS=50, PARTICIPATION_RATE=0.1\n",
      "20 LOCAL_EPOCHS=16, NUM_CLIENTS=50, PARTICIPATION_RATE=0.2\n",
      "21 LOCAL_EPOCHS=16, NUM_CLIENTS=100, PARTICIPATION_RATE=0.05\n",
      "22 LOCAL_EPOCHS=16, NUM_CLIENTS=100, PARTICIPATION_RATE=0.1\n",
      "23 LOCAL_EPOCHS=16, NUM_CLIENTS=100, PARTICIPATION_RATE=0.2\n",
      "24 LOCAL_EPOCHS=16, NUM_CLIENTS=200, PARTICIPATION_RATE=0.05\n",
      "25 LOCAL_EPOCHS=16, NUM_CLIENTS=200, PARTICIPATION_RATE=0.1\n",
      "26 LOCAL_EPOCHS=16, NUM_CLIENTS=200, PARTICIPATION_RATE=0.2\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Combine all configurations\n",
    "\n",
    "# Get all combinations of parameters\n",
    "all_configs = list(\n",
    "    itertools.product(\n",
    "        [4, 8, 16],  # LOCAL_EPOCHS\n",
    "        [50, 100, 200],  # NUM_CLIENTS\n",
    "        [0.05, 0.1, 0.2],  # PARTICIPATION_RATE\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create FederatedConfig objects for each combination\n",
    "all_experiment_configs = [\n",
    "    dataclasses.replace(\n",
    "        fed_config, LOCAL_EPOCHS=epochs, NUM_CLIENTS=clients, PARTICIPATION_RATE=rate\n",
    "    )\n",
    "    for epochs, clients, rate in all_configs\n",
    "]\n",
    "print(len(all_experiment_configs))\n",
    "# Print all combinations\n",
    "for i, config in enumerate(all_experiment_configs):\n",
    "    print(i, \n",
    "        f\"LOCAL_EPOCHS={config.LOCAL_EPOCHS}, \"\n",
    "        f\"NUM_CLIENTS={config.NUM_CLIENTS}, \"\n",
    "        f\"PARTICIPATION_RATE={config.PARTICIPATION_RATE}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
