{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Project 5 - Federated Learning - Track B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision tqdm tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision.datasets.cifar import CIFAR100\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(\"\\r\\033[K\" + msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    COLORS = {\n",
    "        \"DEBUG\": \"\\033[1;34m\",\n",
    "        \"INFO\": \"\\033[1;32m\",\n",
    "        \"WARNING\": \"\\033[1;33m\",\n",
    "        \"ERROR\": \"\\033[1;31m\",\n",
    "        \"CRITICAL\": \"\\033[1;35m\",\n",
    "        \"RESET\": \"\\033[0m\",\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        levelname = record.levelname\n",
    "        if levelname in self.COLORS:\n",
    "            record.levelname = (\n",
    "                f\"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}\"\n",
    "            )\n",
    "        return super().format(record)\n",
    "\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()\n",
    "    root_logger.setLevel(level)\n",
    "\n",
    "    tqdm_handler = TqdmLoggingHandler()\n",
    "    formatter = ColoredFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    tqdm_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(tqdm_handler)\n",
    "\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BaseConfig:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CPU_COUNT = os.cpu_count() or 1\n",
    "    NUM_WORKERS = min(4, CPU_COUNT)\n",
    "    SEED = 42\n",
    "\n",
    "    # Paths\n",
    "    ROOT_DIR: Path = Path.cwd()\n",
    "    CONFIGS_DIR: Path = ROOT_DIR / \"configs\"\n",
    "    DATA_DIR: Path = ROOT_DIR / \"data\"\n",
    "    MODELS_DIR: Path = ROOT_DIR / \"models\"\n",
    "    RESULTS_DIR: Path = ROOT_DIR / \"results\"\n",
    "    RUNS_DIR: Path = ROOT_DIR / \"runs\"\n",
    "    OLD_RUNS_DIR: Path = RUNS_DIR / \"old_runs\"\n",
    "\n",
    "    # Training Parameters\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.01\n",
    "    NUM_EPOCHS = 50\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 4e-4\n",
    "    NUM_CLASSES = 100\n",
    "\n",
    "\n",
    "# Create directories\n",
    "config = BaseConfig()\n",
    "for dir_path in [\n",
    "    config.DATA_DIR,\n",
    "    config.MODELS_DIR,\n",
    "    config.RESULTS_DIR,\n",
    "    config.CONFIGS_DIR,\n",
    "    config.RUNS_DIR,\n",
    "    config.OLD_RUNS_DIR,\n",
    "]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FederatedConfig(BaseConfig):\n",
    "    \"\"\"Federated Learning specific configuration.\"\"\"\n",
    "\n",
    "    NUM_CLIENTS: int = 100\n",
    "    PARTICIPATION_RATE: float = 0.1\n",
    "    LOCAL_EPOCHS: int = 4\n",
    "    NUM_ROUNDS: int = 2000\n",
    "    CLASSES_PER_CLIENT: Optional[int] = None  # None for IID\n",
    "    PARTICIPATION_MODE: str = \"uniform\"\n",
    "    DIRICHLET_ALPHA: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.connected = nn.Sequential(\n",
    "            nn.Linear(5 * 5 * 64, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(192, config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.connected(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsManager:\n",
    "    \"\"\"Manages logging and visualization of training metrics.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BaseConfig,\n",
    "        model_name: str,\n",
    "        training_type: Literal[\"centralized\", \"federated\"],\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.config = config\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        # Archive old runs\n",
    "        old_runs = list(config.RUNS_DIR.glob(f\"{training_type}_{model_name}_*\"))\n",
    "        if old_runs:\n",
    "            archive_dir = config.OLD_RUNS_DIR\n",
    "            archive_dir.mkdir(exist_ok=True)\n",
    "            for run in old_runs:\n",
    "                run.rename(archive_dir / run.name)\n",
    "\n",
    "        # Create descriptive run name for FL experiments\n",
    "        if training_type == \"federated\" and isinstance(config, FederatedConfig):\n",
    "            distribution = (\n",
    "                \"iid\"\n",
    "                if config.CLASSES_PER_CLIENT is None\n",
    "                else f\"noniid_{config.CLASSES_PER_CLIENT}cls\"\n",
    "            )\n",
    "            participation = f\"{config.PARTICIPATION_MODE}\"\n",
    "            if config.PARTICIPATION_MODE == \"skewed\":\n",
    "                participation += f\"_alpha{config.DIRICHLET_ALPHA}\"\n",
    "            clients_info = f\"C{config.NUM_CLIENTS}_P{config.PARTICIPATION_RATE}_E{config.LOCAL_EPOCHS}\"\n",
    "            experiment_suffix = f\"{distribution}_{participation}_{clients_info}\"\n",
    "        else:\n",
    "            experiment_suffix = experiment_name if experiment_name else timestamp\n",
    "\n",
    "        run_name = f\"{training_type}_{model_name}_{experiment_suffix}\"\n",
    "        self.writer = SummaryWriter(config.RUNS_DIR / run_name)\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        split: Literal[\"train\", \"validation\", \"test\"],\n",
    "        loss: float,\n",
    "        accuracy: float,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Log metrics for specified split.\"\"\"\n",
    "        self.writer.add_scalars(\"metrics/loss\", {split: loss}, step)\n",
    "        self.writer.add_scalars(\"metrics/accuracy\", {split: accuracy}, step)\n",
    "\n",
    "    def log_fl_metrics(\n",
    "        self, round_idx: int, metrics: Dict, client_stats: Optional[Dict] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Log federated learning specific metrics.\"\"\"\n",
    "        # Log test metrics\n",
    "        self.log_metrics(\n",
    "            \"test\", metrics[\"test_loss\"], metrics[\"test_accuracy\"], round_idx\n",
    "        )\n",
    "\n",
    "        # Log client participation if available\n",
    "        if client_stats:\n",
    "            self.writer.add_scalars(\"federated/client_stats\", client_stats, round_idx)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close TensorBoard writer.\"\"\"\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar100DatasetManager:\n",
    "    config: BaseConfig\n",
    "    validation_split: float\n",
    "    train_transform: transforms.Compose\n",
    "    test_transform: transforms.Compose\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    val_loader: DataLoader[CIFAR100]\n",
    "    test_loader: DataLoader[CIFAR100]\n",
    "\n",
    "    def __init__(self, config: BaseConfig, validation_split: float = 0.1) -> None:\n",
    "        self.config = config\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_loader, self.val_loader, self.test_loader = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "    ) -> Tuple[DataLoader[CIFAR100], DataLoader[CIFAR100], DataLoader[CIFAR100]]:\n",
    "        full_trainset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "\n",
    "        train_size: int = int((1 - self.validation_split) * len(full_trainset))\n",
    "        val_size: int = len(full_trainset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_trainset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.config.SEED),\n",
    "        )\n",
    "\n",
    "        test_dataset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "\n",
    "        loader_kwargs = {\"num_workers\": self.config.NUM_WORKERS, \"pin_memory\": True}\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        test_loader: DataLoader[CIFAR100] = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.train_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.val_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.test_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedTrainer:\n",
    "    model: LeNet\n",
    "    config: BaseConfig\n",
    "    device: torch.device\n",
    "    metrics: MetricsManager\n",
    "\n",
    "    def __init__(self, model: LeNet, config: BaseConfig) -> None:\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"centralized\"\n",
    "        )\n",
    "\n",
    "    def evaluate_model(\n",
    "        self, model: LeNet, data_loader: DataLoader[CIFAR100]\n",
    "    ) -> Tuple[float, float]:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_loss: float = total_loss / total\n",
    "        accuracy: float = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        max_epochs: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, T_max=self.config.NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.config.NUM_EPOCHS\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "\n",
    "        epoch_pbar = tqdm(range(max_epochs or self.config.NUM_EPOCHS), desc=\"Training\")\n",
    "        epoch = 0\n",
    "\n",
    "        try:\n",
    "            for epoch in epoch_pbar:\n",
    "                self.model.train()\n",
    "                train_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                batch_pbar = tqdm(train_loader, leave=False)\n",
    "                for batch_idx, (inputs, targets) in enumerate(batch_pbar):\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                    # Update metrics\n",
    "                    train_acc = 100.0 * correct / total\n",
    "                    avg_loss = train_loss / (batch_idx + 1)\n",
    "\n",
    "                    global_step = epoch * len(train_loader) + batch_idx\n",
    "                    self.metrics.log_metrics(\"train\", avg_loss, train_acc, global_step)\n",
    "\n",
    "                    batch_pbar.set_postfix(\n",
    "                        {\"loss\": f\"{avg_loss:.3f}\", \"acc\": f\"{train_acc:.2f}%\"}\n",
    "                    )\n",
    "\n",
    "                # Validation phase\n",
    "                val_loss, val_acc = self.evaluate_model(self.model, val_loader)\n",
    "                scheduler.step()\n",
    "\n",
    "                self.metrics.log_metrics(\"validation\", val_loss, val_acc, epoch)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "                epoch_pbar.set_postfix(\n",
    "                    {\"val_loss\": f\"{val_loss:.3f}\", \"val_acc\": f\"{val_acc:.2f}%\"}\n",
    "                )\n",
    "            else:\n",
    "                logging.info(\"Training completed!\")\n",
    "            # Final evaluation\n",
    "            if best_model_state is not None:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "            test_loss, test_acc = self.evaluate_model(self.model, test_loader)\n",
    "            self.metrics.log_metrics(\"test\", test_loss, test_acc, epoch)\n",
    "            logging.info(\n",
    "                f\"Final Test Results - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSharder:\n",
    "    def create_iid_shards(\n",
    "        self, dataset: Dataset[CIFAR100], num_clients: int\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        # FIXME: use numpy as in the non-iid case\n",
    "        total_size = len(dataset)\n",
    "        shard_size = total_size // num_clients\n",
    "        indices = torch.randperm(total_size).tolist()\n",
    "        return [\n",
    "            Subset(dataset, indices[i : i + shard_size])\n",
    "            for i in range(0, total_size, shard_size)\n",
    "        ]\n",
    "\n",
    "    def create_noniid_shards(\n",
    "        self, dataset: Dataset[CIFAR100], num_clients: int, classes_per_client: int\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        labels = torch.tensor([y for _, y in dataset])\n",
    "        class_indices = {i: [] for i in range(100)}\n",
    "\n",
    "        for idx, label in enumerate(labels):\n",
    "            class_indices[label.item()].append(idx)\n",
    "\n",
    "        client_indices = [[] for _ in range(num_clients)]\n",
    "        classes = list(range(100))\n",
    "\n",
    "        for client_id in range(num_clients):\n",
    "            client_classes = np.random.choice(\n",
    "                classes, size=classes_per_client, replace=False\n",
    "            )\n",
    "            for class_id in client_classes:\n",
    "                client_indices[client_id].extend(\n",
    "                    np.random.choice(\n",
    "                        class_indices[class_id],\n",
    "                        size=len(class_indices[class_id]) // num_clients,\n",
    "                        replace=False,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "\n",
    "class ClientManager:\n",
    "    def __init__(\n",
    "        self, num_clients, participation_rate, mode=\"uniform\", dirichlet_alpha=None\n",
    "    ) -> None:\n",
    "        self.num_clients = num_clients\n",
    "        self.num_selected = int(participation_rate * num_clients)\n",
    "\n",
    "        if mode == \"skewed\":\n",
    "            if dirichlet_alpha is None:\n",
    "                raise ValueError(\"dirichlet_alpha required for skewed mode\")\n",
    "            self.selection_probs = np.random.dirichlet([dirichlet_alpha] * num_clients)\n",
    "        else:\n",
    "            self.selection_probs = np.ones(num_clients) / num_clients\n",
    "\n",
    "    def select_clients(self) -> npt.NDArray[np.int64]:\n",
    "        return np.random.choice(\n",
    "            self.num_clients,\n",
    "            size=self.num_selected,\n",
    "            replace=False,\n",
    "            p=self.selection_probs,\n",
    "        )\n",
    "\n",
    "\n",
    "class FederatedClient:\n",
    "    config: FederatedConfig\n",
    "    model: LeNet\n",
    "    client_id: int\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    local_epochs: int\n",
    "    device: torch.device\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model: LeNet,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "        local_epochs: int,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.client_id = client_id\n",
    "        self.train_loader = train_loader\n",
    "        self.local_epochs = local_epochs\n",
    "        self.device = config.DEVICE\n",
    "\n",
    "    # TODO: call the centralized trainer?\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class FederatedServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        client_manager: ClientManager,\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "    ) -> None:\n",
    "        self.global_model = model\n",
    "        self.client_manager = client_manager\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "\n",
    "    def aggregate_models(self, client_models):\n",
    "        global_dict = self.global_model.state_dict()\n",
    "\n",
    "        for k in global_dict.keys():\n",
    "            global_dict[k] = torch.stack(\n",
    "                [\n",
    "                    client_model.state_dict()[k].float()\n",
    "                    for client_model in client_models\n",
    "                ],\n",
    "                0,\n",
    "            ).mean(0)\n",
    "\n",
    "        self.global_model.load_state_dict(global_dict)\n",
    "\n",
    "    def evaluate(self):\n",
    "        return CentralizedTrainer(self.global_model, self.config).evaluate_model(\n",
    "            self.global_model, self.test_loader\n",
    "        )\n",
    "\n",
    "\n",
    "class EnhancedFederatedTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        train_dataset: Dataset[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.device = config.DEVICE\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"federated\"\n",
    "        )\n",
    "\n",
    "        # Setup data sharding\n",
    "        self.sharder = DataSharder()\n",
    "        shards: List[Subset[CIFAR100]] = (\n",
    "            self.sharder.create_noniid_shards(\n",
    "                train_dataset, config.NUM_CLIENTS, config.CLASSES_PER_CLIENT\n",
    "            )\n",
    "            if config.CLASSES_PER_CLIENT\n",
    "            else self.sharder.create_iid_shards(train_dataset, config.NUM_CLIENTS)\n",
    "        )\n",
    "\n",
    "        self.client_loaders: List[DataLoader[CIFAR100]] = [\n",
    "            DataLoader(\n",
    "                shard,\n",
    "                batch_size=config.BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=config.NUM_WORKERS,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            for shard in shards\n",
    "        ]\n",
    "\n",
    "        self.client_manager = ClientManager(\n",
    "            config.NUM_CLIENTS, config.PARTICIPATION_RATE\n",
    "        )\n",
    "\n",
    "        self.server = FederatedServer(model, self.client_manager, test_loader, config)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        try:\n",
    "            for round_idx in tqdm(range(self.config.NUM_ROUNDS)):\n",
    "                selected_clients = self.client_manager.select_clients()\n",
    "\n",
    "                client_models = []\n",
    "                for client_idx in selected_clients:\n",
    "                    client = FederatedClient(\n",
    "                        client_idx,\n",
    "                        self.server.global_model,\n",
    "                        self.client_loaders[client_idx],\n",
    "                        self.config,\n",
    "                        self.config.LOCAL_EPOCHS,\n",
    "                    )\n",
    "                    client_models.append(client.train())\n",
    "\n",
    "                self.server.aggregate_models(client_models)\n",
    "\n",
    "                test_loss, accuracy = self.server.evaluate()\n",
    "                self.metrics.log_metrics(\"test\", test_loss, accuracy, round_idx)\n",
    "\n",
    "                if round_idx % 10 == 0:\n",
    "                    logging.info(\n",
    "                        f\"Round {round_idx}/{self.config.NUM_ROUNDS}: \"\n",
    "                        f\"Test Loss: {test_loss:.4f}, \"\n",
    "                        f\"Test Accuracy: {accuracy:.2f}%\"\n",
    "                    )\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Cifar100DatasetManager(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(config)\n",
    "trainer = CentralizedTrainer(model, config)\n",
    "trainer.train(data.train_loader, data.val_loader, data.test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_config = FederatedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_model = LeNet(config)\n",
    "fed_trainer = EnhancedFederatedTrainer(\n",
    "    model=fed_model,\n",
    "    train_dataset=data.train_dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=fed_config,\n",
    ")\n",
    "fed_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_iid_config = dataclasses.replace(\n",
    "    fed_config,\n",
    "    CLASSES_PER_CLIENT=5,  # Each client gets 5 classes\n",
    ")\n",
    "\n",
    "# Train with non-iid distribution\n",
    "non_iid_model = LeNet(config)\n",
    "non_iid_trainer = EnhancedFederatedTrainer(\n",
    "    model=non_iid_model,\n",
    "    train_dataset=data.train_loader.dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=non_iid_config,\n",
    ")\n",
    "non_iid_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_config = dataclasses.replace(\n",
    "    fed_config, PARTICIPATION_MODE=\"skewed\", DIRICHLET_ALPHA=0.5\n",
    ")\n",
    "\n",
    "skewed_model = LeNet(config)\n",
    "skewed_trainer = EnhancedFederatedTrainer(\n",
    "    model=skewed_model,\n",
    "    train_dataset=data.train_loader.dataset,\n",
    "    test_loader=data.test_loader,\n",
    "    config=skewed_config,\n",
    ")\n",
    "skewed_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_epochs_configs = [\n",
    "    dataclasses.replace(fed_config, LOCAL_EPOCHS=e) for e in [4, 8, 16]\n",
    "]\n",
    "\n",
    "# Experiment with different client counts\n",
    "client_counts_configs = [\n",
    "    dataclasses.replace(fed_config, NUM_CLIENTS=c) for c in [50, 100, 200]\n",
    "]\n",
    "\n",
    "# Experiment with different participation rates\n",
    "participation_rates_configs = [\n",
    "    dataclasses.replace(fed_config, PARTICIPATION_RATE=r) for r in [0.05, 0.1, 0.2]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
