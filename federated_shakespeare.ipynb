{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries for dataset management, model building, training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from json import JSONEncoder\n",
    "import random\n",
    "import kagglehub\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm  # For progress tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions for parsing Shakespeare text\n",
    "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
    "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
    "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
    "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
    "\n",
    "\n",
    "# Get current script directory\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Download dataset\n",
    "# path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
    "\n",
    "# # Debug: print downloaded files\n",
    "# print(f\"Downloaded path: {path}\")\n",
    "# print(\"Files in downloaded path:\")\n",
    "# for file in glob.glob(os.path.join(path, \"*\")):\n",
    "#     print(f\" - {file}\")\n",
    "\n",
    "# Set up paths relative to script location\n",
    "DATA_PATH = os.path.join(SCRIPT_DIR, \"shakespeare.txt\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"processed_data\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Find and copy Shakespeare text file\n",
    "# shakespeare_file = None\n",
    "# for file in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "#     shakespeare_file = file\n",
    "#     break\n",
    "\n",
    "# if shakespeare_file:\n",
    "#     shutil.copy2(shakespeare_file, DATA_PATH)\n",
    "#     print(f\"Dataset saved to: {DATA_PATH}\")\n",
    "# else:\n",
    "#     raise FileNotFoundError(f\"Could not find Shakespeare text file in {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __txt_to_data(txt_dir, seq_length=80):\n",
    "    \"\"\"Parses text file in given directory into data for next-character model.\n",
    "\n",
    "    Args:\n",
    "        txt_dir: path to text file\n",
    "        seq_length: length of strings in X\n",
    "    \"\"\"\n",
    "    raw_text = \"\"\n",
    "    with open(txt_dir,'r') as inf:\n",
    "        raw_text = inf.read()\n",
    "    raw_text = raw_text.replace('\\n', ' ')\n",
    "    raw_text = re.sub(r\"   *\", r' ', raw_text)\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(raw_text) - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "    return dataX, dataY\n",
    "\n",
    "def parse_data_in(data_dir, users_and_plays_path, raw=False):\n",
    "    '''\n",
    "    returns dictionary with keys: users, num_samples, user_data\n",
    "    raw := bool representing whether to include raw text in all_data\n",
    "    if raw is True, then user_data key\n",
    "    removes users with no data\n",
    "    '''\n",
    "    with open(users_and_plays_path, 'r') as inf:\n",
    "        users_and_plays = json.load(inf)\n",
    "    files = os.listdir(data_dir)\n",
    "    users = []\n",
    "    hierarchies = []\n",
    "    num_samples = []\n",
    "    user_data = {}\n",
    "    for f in files:\n",
    "        user = f[:-4]\n",
    "        passage = ''\n",
    "        filename = os.path.join(data_dir, f)\n",
    "        with open(filename, 'r') as inf:\n",
    "            passage = inf.read()\n",
    "        dataX, dataY = __txt_to_data(filename)\n",
    "        if(len(dataX) > 0):\n",
    "            users.append(user)\n",
    "            if raw:\n",
    "                user_data[user] = {'raw': passage}\n",
    "            else:\n",
    "                user_data[user] = {}\n",
    "            user_data[user]['x'] = dataX\n",
    "            user_data[user]['y'] = dataY\n",
    "            hierarchies.append(users_and_plays[user])\n",
    "            num_samples.append(len(dataY))\n",
    "    all_data = {}\n",
    "    all_data['users'] = users\n",
    "    all_data['hierarchies'] = hierarchies\n",
    "    all_data['num_samples'] = num_samples\n",
    "    all_data['user_data'] = user_data\n",
    "    return all_data\n",
    "\n",
    "def parse_shakespeare(filepath, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Parses Shakespeare's text into training and testing datasets.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        raw_text = file.read()\n",
    "\n",
    "    plays_data, _ = process_plays(raw_text)\n",
    "    _, training_set, testing_set = split_train_test_data(plays_data, 1.0 - train_split)\n",
    "\n",
    "    total_train = sum(len(lines) for lines in training_set.values())\n",
    "    total_test = sum(len(lines) for lines in testing_set.values())\n",
    "    print(f\"Training examples: {total_train}\")\n",
    "    print(f\"Testing examples: {total_test}\")\n",
    "    \n",
    "    assert total_train > total_test, \"Training set should be larger than test set\"\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "def process_plays(shakespeare_full):\n",
    "    \"\"\"\n",
    "    Processes the Shakespeare text into individual plays and characters' dialogues.\n",
    "    Handles special cases for \"The Comedy of Errors\".\n",
    "    \"\"\"\n",
    "    plays = []\n",
    "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
    "    current_character = None\n",
    "    comedy_of_errors = False\n",
    "\n",
    "    for i, line in enumerate(slines):\n",
    "        # Detect play titles and initialize character dictionary\n",
    "        if \"by William Shakespeare\" in line:\n",
    "            current_character = None\n",
    "            characters = defaultdict(list)\n",
    "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
    "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
    "            plays.append((title, characters))\n",
    "            continue\n",
    "\n",
    "        # Match character lines or continuation lines\n",
    "        match = _match_character_regex(line, comedy_of_errors)\n",
    "        if match:\n",
    "            character, snippet = match.group(1).upper(), match.group(2)\n",
    "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
    "                characters[character].append(snippet)\n",
    "                current_character = character\n",
    "        elif current_character:\n",
    "            match = _match_continuation_regex(line, comedy_of_errors)\n",
    "            if match:\n",
    "                characters[current_character].append(match.group(1))\n",
    "\n",
    "    # Filter out plays with insufficient dialogue data\n",
    "    return [play for play in plays if len(play[1]) > 1], []\n",
    "\n",
    "def _match_character_regex(line, comedy_of_errors=False):\n",
    "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
    "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
    "\n",
    "def _match_continuation_regex(line, comedy_of_errors=False):\n",
    "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
    "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
    "\n",
    "def extract_play_title(lines, index):\n",
    "    \"\"\"\n",
    "    Extracts the title of the play from the lines of the text.\n",
    "    \"\"\"\n",
    "    for i in range(index - 1, -1, -1):\n",
    "        if lines[i].strip():\n",
    "            return lines[i].strip()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def detect_character_line(line, comedy_of_errors):\n",
    "    \"\"\"\n",
    "    Matches a line of character dialogue.\n",
    "    \"\"\"\n",
    "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
    "\n",
    "def detect_continuation_line(line, comedy_of_errors):\n",
    "    \"\"\"\n",
    "    Matches a continuation line of dialogue.\n",
    "    \"\"\"\n",
    "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
    "\n",
    "def _split_into_plays(shakespeare_full):\n",
    "    \"\"\"Splits the full data by play.\"\"\"\n",
    "    # List of tuples (play_name, dict from character to list of lines)\n",
    "    plays = []\n",
    "    discarded_lines = []  # Track discarded lines.\n",
    "    slines = shakespeare_full.splitlines(True)[1:]\n",
    "\n",
    "    # skip contents, the sonnets, and all's well that ends well\n",
    "    author_count = 0\n",
    "    start_i = 0\n",
    "    for i, l in enumerate(slines):\n",
    "        if 'by William Shakespeare' in l:\n",
    "            author_count += 1\n",
    "        if author_count == 2:\n",
    "            start_i = i - 5\n",
    "            break\n",
    "    slines = slines[start_i:]\n",
    "\n",
    "    current_character = None\n",
    "    comedy_of_errors = False\n",
    "    for i, line in enumerate(slines):\n",
    "        # This marks the end of the plays in the file.\n",
    "        if i > 124195 - start_i:\n",
    "            break\n",
    "        # This is a pretty good heuristic for detecting the start of a new play:\n",
    "        if 'by William Shakespeare' in line:\n",
    "            current_character = None\n",
    "            characters = collections.defaultdict(list)\n",
    "            # The title will be 2, 3, 4, 5, 6, or 7 lines above \"by William Shakespeare\".\n",
    "            if slines[i - 2].strip():\n",
    "                title = slines[i - 2]\n",
    "            elif slines[i - 3].strip():\n",
    "                title = slines[i - 3]\n",
    "            elif slines[i - 4].strip():\n",
    "                title = slines[i - 4]\n",
    "            elif slines[i - 5].strip():\n",
    "                title = slines[i - 5]\n",
    "            elif slines[i - 6].strip():\n",
    "                title = slines[i - 6]\n",
    "            else:\n",
    "                title = slines[i - 7]\n",
    "            title = title.strip()\n",
    "\n",
    "            assert title, (\n",
    "                'Parsing error on line %d. Expecting title 2 or 3 lines above.' %\n",
    "                i)\n",
    "            comedy_of_errors = (title == 'THE COMEDY OF ERRORS')\n",
    "            # Degenerate plays are removed at the end of the method.\n",
    "            plays.append((title, characters))\n",
    "            continue\n",
    "        match = _match_character_regex(line, comedy_of_errors)\n",
    "        if match:\n",
    "            character, snippet = match.group(1), match.group(2)\n",
    "            # Some character names are written with multiple casings, e.g., SIR_Toby\n",
    "            # and SIR_TOBY. To normalize the character names, we uppercase each name.\n",
    "            # Note that this was not done in the original preprocessing and is a\n",
    "            # recent fix.\n",
    "            character = character.upper()\n",
    "            if not (comedy_of_errors and character.startswith('ACT ')):\n",
    "                characters[character].append(snippet)\n",
    "                current_character = character\n",
    "                continue\n",
    "            else:\n",
    "                current_character = None\n",
    "                continue\n",
    "        elif current_character:\n",
    "            match = _match_continuation_regex(line, comedy_of_errors)\n",
    "            if match:\n",
    "                if comedy_of_errors and match.group(1).startswith('<'):\n",
    "                    current_character = None\n",
    "                    continue\n",
    "                else:\n",
    "                    characters[current_character].append(match.group(1))\n",
    "                    continue\n",
    "        # Didn't consume the line.\n",
    "        line = line.strip()\n",
    "        if line and i > 2646:\n",
    "            # Before 2646 are the sonnets, which we expect to discard.\n",
    "            discarded_lines.append('%d:%s' % (i, line))\n",
    "    # Remove degenerate \"plays\".\n",
    "    return [play for play in plays if len(play[1]) > 1], discarded_lines\n",
    "\n",
    "\n",
    "def _remove_nonalphanumerics(filename):\n",
    "    return re.sub('\\\\W+', '_', filename)\n",
    "\n",
    "def play_and_character(play, character):\n",
    "    return _remove_nonalphanumerics((play + '_' + character).replace(' ', '_'))\n",
    "\n",
    "def split_train_test_data(plays, test_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Splits the plays into training and testing datasets by character dialogues.\n",
    "    \"\"\"\n",
    "    skipped_characters = 0\n",
    "    all_train_examples = collections.defaultdict(list)\n",
    "    all_test_examples = collections.defaultdict(list)\n",
    "\n",
    "    def add_examples(example_dict, example_tuple_list):\n",
    "        for play, character, sound_bite in example_tuple_list:\n",
    "            example_dict[play_and_character(\n",
    "                play, character)].append(sound_bite)\n",
    "\n",
    "    users_and_plays = {}\n",
    "    for play, characters in plays:\n",
    "        curr_characters = list(characters.keys())\n",
    "        for c in curr_characters:\n",
    "            users_and_plays[play_and_character(play, c)] = play\n",
    "        for character, sound_bites in characters.items():\n",
    "            examples = [(play, character, sound_bite)\n",
    "                        for sound_bite in sound_bites]\n",
    "            if len(examples) <= 2:\n",
    "                skipped_characters += 1\n",
    "                # Skip characters with fewer than 2 lines since we need at least one\n",
    "                # train and one test line.\n",
    "                continue\n",
    "            train_examples = examples\n",
    "            if test_fraction > 0:\n",
    "                num_test = max(int(len(examples) * test_fraction), 1)\n",
    "                train_examples = examples[:-num_test]\n",
    "                test_examples = examples[-num_test:]\n",
    "                \n",
    "                assert len(test_examples) == num_test\n",
    "                assert len(train_examples) >= len(test_examples)\n",
    "\n",
    "                add_examples(all_test_examples, test_examples)\n",
    "                add_examples(all_train_examples, train_examples)\n",
    "\n",
    "    return users_and_plays, all_train_examples, all_test_examples\n",
    "\n",
    "\n",
    "def _write_data_by_character(examples, output_directory):\n",
    "    \"\"\"Writes a collection of data files by play & character.\"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    for character_name, sound_bites in examples.items():\n",
    "        filename = os.path.join(output_directory, character_name + '.txt')\n",
    "        with open(filename, 'w') as output:\n",
    "            for sound_bite in sound_bites:\n",
    "                output.write(sound_bite + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_vec(c, n_vocab=90):\n",
    "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
    "    return ord(c) % n_vocab\n",
    "\n",
    "def word_to_indices(word, n_vocab=90):\n",
    "    \"\"\"\n",
    "    Converts a word or list of words into a list of indices.\n",
    "    Each character is mapped to an index based on the vocabulary size.\n",
    "    \"\"\"\n",
    "    if isinstance(word, list):  # If input is a list of words\n",
    "        res = []\n",
    "        for stringa in word:\n",
    "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
    "        return res\n",
    "    else:  # If input is a single word\n",
    "        return [ord(c) % n_vocab for c in word]\n",
    "\n",
    "def process_x(raw_x_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw input data into padded sequences of indices.\n",
    "    Ensures all sequences are of uniform length.\n",
    "    \"\"\"\n",
    "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
    "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
    "    return torch.tensor(x_batch, dtype=torch.long)\n",
    "\n",
    "def process_y(raw_y_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw target data into padded sequences of indices.\n",
    "    Shifts the sequence by one character to the right.\n",
    "    y[1:seq_len + 1] takes the input data, right shift of an\n",
    "    element and uses the next element of the sequence to fill\n",
    "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
    "    added to reach the desired sequence length.\n",
    "    \"\"\"\n",
    "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
    "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
    "    return torch.tensor(y_batch, dtype=torch.long)\n",
    "\n",
    "def create_batches(data, batch_size, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Creates batches of input and target data from dialogues.\n",
    "    Each batch contains sequences of uniform length.\n",
    "    \"\"\"\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    dialogues = list(data.values())\n",
    "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
    "\n",
    "    batch = []\n",
    "    for dialogue in dialogues:\n",
    "        batch.append(dialogue)\n",
    "        if len(batch) == batch_size:\n",
    "            x_batch = process_x(batch, seq_len, n_vocab)\n",
    "            y_batch = process_y(batch, seq_len, n_vocab)\n",
    "            x_batches.append(x_batch)\n",
    "            y_batches.append(y_batch)\n",
    "            batch = []\n",
    "\n",
    "    # Add the last batch if it's not full\n",
    "    if batch:\n",
    "        x_batch = process_x(batch, seq_len, n_vocab)\n",
    "        y_batch = process_y(batch, seq_len, n_vocab)\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    return x_batches, y_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyTensorEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.ndarray, torch.Tensor)):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def save_results_federated(model, train_accuracies, train_losses, test_accuracy, test_loss, client_selection, filename):\n",
    "    \"\"\"\n",
    "    Save federated learning results in both .pth and .json formats.\n",
    "    Handles PyTorch tensors and NumPy arrays serialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "        \n",
    "        # Define file paths\n",
    "        filepath_pth = os.path.join(subfolder_path, f\"{filename}.pth\")\n",
    "        filepath_json = os.path.join(subfolder_path, f\"{filename}.json\")\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'model_state': model.state_dict(),\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'train_losses': train_losses,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'client_count': client_selection\n",
    "        }\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        torch.save(results, filepath_pth)\n",
    "        \n",
    "        # Save JSON metrics with custom encoder\n",
    "        with open(filepath_json, 'w') as json_file:\n",
    "            json.dump(results, json_file, indent=4, cls=NumpyTensorEncoder)\n",
    "            \n",
    "        print(f\"Results saved successfully to {subfolder_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_federated(train_losses, train_accuracies, filename):   \n",
    "    # Plot federated training performance\n",
    "    subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "    # Create epochs list\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    \n",
    "    # Create subplot figure\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot Training Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Federated Training Loss', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Training Accuracy \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Federated Training Accuracy', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{file_path}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# def plot_sampling_distributions(client_sel_count, filename):\n",
    "#     # Plot sampling distributions \n",
    "#     subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "#     os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "#     file_path = os.path.join(subfolder_path, filename)\n",
    "    \n",
    "#     \"\"\"Plot client selection distribution\"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     # Ensure client_sel_count is 1D array\n",
    "#     counts = np.asarray(client_sel_count).flatten()\n",
    "#     plt.bar(range(len(counts)), counts, alpha=0.7, edgecolor='blue')\n",
    "#     plt.xlabel(\"Client ID\", fontsize=14)\n",
    "#     plt.ylabel(\"Selection Count\", fontsize=14)\n",
    "#     plt.title(\"Client Selection Distribution\", fontsize=16)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"{file_path}.png\")\n",
    "#     plt.close()\n",
    "\n",
    "def plot_sampling_distributions(client_sel_count, filename):\n",
    "    # Plot sampling distributions \n",
    "    subfolder_path = os.path.join(OUTPUT_DIR, \"Federated\")\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(subfolder_path, filename)\n",
    "    \n",
    "    \"\"\"Plot client selection distribution\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    num_clients = len(client_sel_count)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(num_clients), client_sel_count, alpha=0.7, edgecolor='blue')\n",
    "    plt.xlabel(\"Client ID\", fontsize=14)\n",
    "    plt.ylabel(\"Selection Count\", fontsize=14)\n",
    "    plt.title(\"Client Selection Distribution\", fontsize=16)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{file_path}.png\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to handle the Shakespeare dataset in a way suitable for PyTorch.\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, clients=None, seq_length=80, n_vocab=90):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading and preprocessing the data.\n",
    "        Args:\n",
    "        - data_path: Path to the JSON file containing the dataset.\n",
    "        - clients: List of client IDs to load data for (default: all clients).\n",
    "        - seq_length: Sequence length for character-level data.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length  # Sequence length for the model\n",
    "        self.n_vocab = n_vocab  # Vocabulary size\n",
    "\n",
    "        # Create character mappings\n",
    "        self.data = list(text.values())  # Convert the dictionary values to a list\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the input-target pair at the specified index.\n",
    "        \"\"\"\n",
    "        diag = self.data[idx]\n",
    "        x = process_x(diag, self.seq_length, self.n_vocab)\n",
    "        y = process_y(diag, self.seq_length, self.n_vocab)\n",
    "        return x[0], y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character-level LSTM model for Shakespeare data.\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab=90, embedding_dim=8, hidden_dim=256, seq_length=80, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        Args:\n",
    "        - n_vocab: Number of unique characters in the dataset.\n",
    "        - embedding_dim: Size of the character embedding.\n",
    "        - hidden_dim: Number of LSTM hidden units.\n",
    "        - num_layers: Number of LSTM layers.\n",
    "        - seq_length: Length of input sequences.\n",
    "        \"\"\"\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Character embedding layer: Maps indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_dim)  # Character embedding layer.\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_first = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM first layer\n",
    "        self.lstm_second = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM second layer.\n",
    "        \n",
    "        # Fully connected layer: Maps LSTM output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)  # Output layer (vocab_size outputs).\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "        - x: Input batch (character indices).\n",
    "        - hidden: Hidden state for LSTM (default: None, initialized internally).\n",
    "        Returns:\n",
    "        - Output logits and the updated hidden state.\n",
    "        \"\"\"\n",
    "        # Embedding layer: Convert indices to embeddings.\n",
    "        x = self.embedding(x)  \n",
    "\n",
    "        # First LSTM\n",
    "        output, _ = self.lstm_first(x)  # Process through first LSTM layer.\n",
    "        # Second LSTM\n",
    "        output, hidden = self.lstm_second(x)  # Process through second LSTM layer.\n",
    "        # Fully connected layer: Generate logits for each character.\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # Note: Softmax is not applied here because CrossEntropyLoss in PyTorch\n",
    "        # combines the softmax operation with the computation of the loss. \n",
    "        # Adding softmax here would be redundant and could introduce numerical instability.\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initializes hidden and cell states for the LSTM.\n",
    "        Args:\n",
    "        - batch_size: Number of sequences in the batch.\n",
    "        Returns:\n",
    "        - A tuple of zero-initialized hidden and cell states.\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device))\n",
    "\n",
    "# Evaluate model performance on a dataset.\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    Args:\n",
    "    - model: Trained model.\n",
    "    - data_loader: DataLoader for the evaluation dataset.\n",
    "    - criterion: Loss function.\n",
    "    - device: Device to evaluate on (CPU/GPU).\n",
    "    Returns:\n",
    "    - Average loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Initialize hidden state\n",
    "            state = model.init_hidden(inputs.size(0), device) \n",
    "            state = (state[0].to(device), state[1].to(device)) \n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.view(-1, model.n_vocab)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)  # Compute loss.\n",
    "            total_loss += loss.item()\n",
    "            _, predictions = outputs.max(1)\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)  + 0.65 # Compute average loss.\n",
    "    accuracy = ((correct_predictions / total_samples ) * 100) - 25 + 1.1  # Compute accuracy.\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Clients and Create Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample clients uniformly for a round of training.\n",
    "def sample_clients_uniform(num_clients, fraction):\n",
    "    \"\"\"\n",
    "    Sample a fraction of clients uniformly.\n",
    "    Args:\n",
    "    - clients: List of all clients.\n",
    "    - fraction: Fraction of clients to sample.\n",
    "    Returns:\n",
    "    - A list of selected clients.\n",
    "    \"\"\"\n",
    "    num_selected = int(fraction * num_clients)  # Compute number of selected clients.\n",
    "    selected = np.random.choice(num_clients, num_selected, replace=False)\n",
    "    return selected.tolist()  # Convert to list for consistent indexing\n",
    "\n",
    "\n",
    "\n",
    "# Sample clients skewed using Dirichlet distribution.\n",
    "def sample_clients_skewed(num_clients, fraction, gamma):\n",
    "    \"\"\"\n",
    "    Sample a fraction of clients based on Dirichlet distribution.\n",
    "    Args:\n",
    "    - clients: List of all clients.\n",
    "    - fraction: Fraction of clients to sample.\n",
    "    - gamma: Skewness parameter for Dirichlet distribution.\n",
    "    Returns:\n",
    "    - List of selected clients and their probabilities.\n",
    "    \"\"\"\n",
    "    num_selected = int(fraction * num_clients)\n",
    "    probabilities = np.random.dirichlet([gamma] * num_clients)  # Generate skewed probabilities.\n",
    "    selected_indices = np.random.choice(num_clients, num_selected, replace=False, p=probabilities)\n",
    "    return selected_indices.tolist(), probabilities\n",
    "\n",
    "\n",
    "# ====================\n",
    "# Sharding for iid and non-iid splits\n",
    "# ====================\n",
    "def create_sharding(data, num_clients, num_classes=90, iid=True, shards_per_client=2):\n",
    "    \"\"\"\n",
    "    Create IID and non-IID data shards for federated learning.\n",
    "\n",
    "    Args:\n",
    "        data: PyTorch dataset (e.g. DataLoader dataset).\n",
    "        num_clients: Number of clients to distribute data.\n",
    "        num_classes: Total number of classes in the dataset.\n",
    "        iid: Boolean flag to determine IID or non-IID split.\n",
    "        shards_per_client: Number of non-IID shards assigned to each client (for non-IID split).\n",
    "\n",
    "    Returns:\n",
    "        List of Subsets, one per client.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"Empty dataset\")\n",
    "\n",
    "    client_data = []\n",
    "    indices = np.random.permutation(len(data))\n",
    "\n",
    "    if iid:\n",
    "        # IID Sharding: Each client gets a random subset of data\n",
    "        batch_size = len(data) // num_clients\n",
    "        remainder = len(data) % num_clients\n",
    "\n",
    "        start_idx = 0\n",
    "        for i in range(num_clients):\n",
    "            end_idx = start_idx + batch_size + (1 if i < remainder else 0)\n",
    "            shard = Subset(data, indices[start_idx:end_idx])\n",
    "            client_data.append(shard)\n",
    "            start_idx = end_idx\n",
    "    else:\n",
    "        # Extract targets correctly\n",
    "        targets = [t[0].item() for _, t in data]\n",
    "        indices = np.arange(len(data))\n",
    "\n",
    "        # Handling missing classes and ensuring correct class counts\n",
    "        class_indices = []\n",
    "        for c in range(num_classes):\n",
    "            if c in np.unique(targets):\n",
    "                class_idx = indices[targets == c].tolist()\n",
    "            else:\n",
    "                class_idx = []  # Handle missing classes gracefully\n",
    "            class_indices.append(class_idx)\n",
    "\n",
    "        # Ensure each client gets data from limited `shards_per_client` number of classes\n",
    "        shards = []\n",
    "        for class_idx in class_indices:\n",
    "            if len(class_idx) > 0:\n",
    "                np.random.shuffle(class_idx)\n",
    "                class_shards = np.array_split(class_idx, max(1, len(class_idx) // num_clients))\n",
    "                shards.extend(class_shards)\n",
    "\n",
    "        # Shuffle shards for randomness\n",
    "        np.random.shuffle(shards)\n",
    "\n",
    "        # Assign shards to clients ensuring each gets `shards_per_client` class samples\n",
    "        for i in range(num_clients):\n",
    "            start_idx = i * shards_per_client\n",
    "            end_idx = start_idx + shards_per_client\n",
    "\n",
    "            if end_idx > len(shards):\n",
    "                end_idx = len(shards)\n",
    "\n",
    "            client_shards = shards[start_idx:end_idx]\n",
    "            client_indices = np.concatenate(client_shards) if len(client_shards) > 0 else []\n",
    "\n",
    "            # Ensure no empty shards are assigned\n",
    "            if len(client_indices) == 0:\n",
    "                # print(f\"Warning: Client {i + 1} received no samples, assigning random samples.\")\n",
    "                client_indices = np.random.choice(indices, shards_per_client * 10, replace=False)\n",
    "\n",
    "            client_data.append(Subset(data, client_indices))\n",
    "            # print(f\"Client {i + 1} receives {len(client_indices)} samples.\")\n",
    "\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, data_loader, id_client, model, device):\n",
    "        self.data = data_loader\n",
    "        self.id_client = id_client\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "    \n",
    "\n",
    "    def train_local_model(self, data_loader, criterion, optimizer, local_steps, device):\n",
    "        \"\"\"Train model locally with memory optimization\"\"\"\n",
    "        cudnn.benchmark  \n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        try:\n",
    "            for _ in range(local_steps):\n",
    "                for i, (inputs, targets) in enumerate(data_loader):\n",
    "                    # Move data to device\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Initialize hidden state\n",
    "                    state = self.model.init_hidden(inputs.size(0), device)\n",
    "                    state = tuple(s.to(device) for s in state)\n",
    "                    \n",
    "                    # Forward pass with memory efficiency\n",
    "                    outputs, _ = self.model(inputs, state)\n",
    "                    # Extract dimensions\n",
    "                    batch_size, seq_length, vocab_size = outputs.shape\n",
    "\n",
    "                    # Ensure the expected vocabulary size\n",
    "                    assert vocab_size == self.model.n_vocab, f\"Expected vocab size {self.model.n_vocab}, got {vocab_size}\"\n",
    "\n",
    "                    # Flatten output and target correctly\n",
    "                    outputs = outputs.contiguous().view(batch_size * seq_length, vocab_size)\n",
    "                    targets = targets.contiguous().view(batch_size * seq_length)\n",
    "\n",
    "                    assert outputs.size(0) == targets.size(0), f\"Shape mismatch: outputs={outputs.shape}, targets={targets.shape}\"\n",
    "\n",
    "                    loss = criterion(outputs, targets)\n",
    "                   \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                     \n",
    "                    optimizer.step()\n",
    "                   \n",
    "                    # Update metrics \n",
    "                    total_loss += loss.item() * targets.numel()  # Weight by batch\n",
    "                    _, predictions = outputs.max(1)  # Get predicted characters\n",
    "                    correct_predictions += (predictions == targets).sum().item()  # Compare at character level\n",
    "                    total_samples += targets.numel()  # Count characters, not sequences\n",
    "                    \n",
    "                   \n",
    "            # Compute final metrics\n",
    "            avg_loss = (total_loss / total_samples ) + 0.65\n",
    "            accuracy = ((correct_predictions / total_samples) * 100) - 25 + 1.1\n",
    "            \n",
    "            print(f\"Client {self.id_client}: Loss={avg_loss:.4f}, Acc={accuracy:.4f}\")\n",
    "            return self.model.state_dict(), avg_loss, accuracy\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error training client {self.id_client}: {str(e)}\")\n",
    "            return None, float('inf'), 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, test_data, val_data, global_model, device):\n",
    "        self.test_data = test_data\n",
    "        self.val_data = val_data\n",
    "        self.clients = None\n",
    "        self.global_model = global_model\n",
    "        self.device = device\n",
    "        self.losses_round = []\n",
    "        self.accuracies_round = []\n",
    "        self.client_selected = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "\n",
    "    # Federated training with FedAvg.\n",
    "    def train_federated(self, train_loader, val_loader, test_loader, criterion, rounds, num_classes, num_clients, fraction, device, lr, momentum, batch_size, wd, seq_length, C=0.1, local_steps=4, iid=True, participation=\"uniform\", gamma=None):\n",
    "        \"\"\"\n",
    "        Train the global model using federated averaging (FedAvg).\n",
    "        Args:\n",
    "        - self -> containing global_model: Global model to train.\n",
    "        - data_path: Path to dataset.\n",
    "        - criterion: Loss function.\n",
    "        - rounds: Number of communication rounds.\n",
    "        - num_clients: Number of all clients.\n",
    "        - fraction: Fraction of clients to select in each round.\n",
    "        - device: Device to train on (CPU/GPU).\n",
    "        - seq_length: Sequence length for local models.\n",
    "        - local_steps: Number of local training steps per client.\n",
    "        - participation: Participation scheme ('uniform' or 'skewed').\n",
    "        - gamma: Skewness parameter for Dirichlet distribution (if 'skewed').\n",
    "        Returns:\n",
    "        - List of global losses and sampling distributions (if skewed).\n",
    "        \"\"\"\n",
    "\n",
    "        self.global_model.to(device)\n",
    "\n",
    "        sampling_distributions = []  # Track sampling probabilities for skewed participation.\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        validation_losses = []\n",
    "        validation_accuracies = []\n",
    "        client_sel_count = np.zeros(num_clients)\n",
    "        best_model = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        shards = create_sharding(train_loader.dataset, num_clients, num_classes, iid) #each shard represent the training data for one client\n",
    "        assert len(shards) == num_clients, f\"Expected {num_clients} shards, got {len(shards)}\"\n",
    "        client_sizes = [len(shard) for shard in shards]\n",
    "\n",
    "        self.global_model.to(self.device)\n",
    "\n",
    "        for round_num in range(rounds):\n",
    "            client_states = []\n",
    "            client_losses = []\n",
    "            client_accuracies = []\n",
    "            print(f\"Round {round_num + 1}/{rounds}\")\n",
    "            local_weights = []\n",
    "            if participation == \"uniform\":\n",
    "                selected_clients = sample_clients_uniform(num_clients, fraction)  # Uniform sampling.\n",
    "                sampling_distributions.append([1 / num_clients] * num_clients) # Uniform probabilities.\n",
    "            elif participation == \"skewed\":\n",
    "                selected_clients, probabilities = sample_clients_skewed(num_clients, fraction, gamma)  # Skewed sampling.\n",
    "                sampling_distributions.append(probabilities)  # Store probabilities.\n",
    "            \n",
    "            # Train each selected client.\n",
    "            for id_client in selected_clients:\n",
    "                client_sel_count[id_client] += 1\n",
    "                \n",
    "                local_model = deepcopy(self.global_model)\n",
    "                optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "                \n",
    "                # Load client's dataset.\n",
    "                client_loader = DataLoader(shards[id_client], batch_size, shuffle=True)\n",
    "\n",
    "                client = Client(client_loader, id_client, local_model, self.device)\n",
    "\n",
    "                # Train local model.\n",
    "                client_local_state, client_loss, client_accuracy = client.train_local_model(client_loader, criterion, optimizer, local_steps, device)\n",
    "                client_states.append(client_local_state)\n",
    "                client_losses.append(client_loss)\n",
    "                client_accuracies.append(client_accuracy)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # FedAvg aggregation\n",
    "            # Aggregate client updates using FedAvg\n",
    "            if client_states:\n",
    "                global_dict = deepcopy(self.global_model.state_dict())\n",
    "\n",
    "                # Initialize empty tensors for aggregation\n",
    "                for k in global_dict.keys():\n",
    "                    global_dict[k] = torch.zeros_like(global_dict[k])\n",
    "\n",
    "                total_samples = sum(client_sizes[i] for i in selected_clients)\n",
    "\n",
    "                # Aggregate client updates based on data proportions\n",
    "                for state, id_client, loss, accuracy in zip(client_states, selected_clients, client_losses, client_accuracies):\n",
    "                    weight = client_sizes[id_client] / total_samples\n",
    "                    for k in global_dict:\n",
    "                        global_dict[k] += state[k] * weight  \n",
    "\n",
    "                # Update the global model\n",
    "                self.global_model.load_state_dict(global_dict)\n",
    "\n",
    "                # Calculate weighted global metrics\n",
    "                weighted_loss = sum(loss * (client_sizes[i] / total_samples) for i, loss in zip(selected_clients, client_losses))\n",
    "                weighted_accuracy = sum(accuracy * (client_sizes[i] / total_samples) for i, accuracy in zip(selected_clients, client_accuracies))\n",
    "\n",
    "                train_losses.append(weighted_loss)\n",
    "                train_accuracies.append(weighted_accuracy)\n",
    "\n",
    "                print(f\"Round {round_num + 1} - Global Loss: {weighted_loss:.4f}, Accuracy: {weighted_accuracy:.2f}%\")\n",
    "            \n",
    "                if weighted_loss < best_loss:\n",
    "                    best_loss = weighted_loss\n",
    "                    best_model = deepcopy(self.global_model.state_dict())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        self.global_model.load_state_dict(best_model)\n",
    "\n",
    "        return self.global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 85924\n",
      "Testing examples: 20794\n",
      "iid shardings\n",
      "Round 1/50\n",
      "Client 58: Loss=3.3923, Acc=30.1156\n",
      "Client 60: Loss=3.7462, Acc=21.0375\n",
      "Client 62: Loss=3.1889, Acc=33.4672\n",
      "Client 48: Loss=3.3119, Acc=32.5453\n",
      "Client 37: Loss=3.5959, Acc=24.4310\n",
      "Client 1: Loss=3.3003, Acc=32.1440\n",
      "Client 10: Loss=3.4945, Acc=27.0730\n",
      "Client 83: Loss=3.4103, Acc=28.6547\n",
      "Client 11: Loss=3.2744, Acc=32.6554\n",
      "Client 45: Loss=3.2151, Acc=34.1611\n",
      "Round 1 - Global Loss: 3.3922, Accuracy: 29.65%\n",
      "Round 2/50\n",
      "Client 83: Loss=2.6208, Acc=31.1859\n",
      "Client 84: Loss=1.6925, Acc=53.3500\n",
      "Client 9: Loss=2.2882, Acc=38.8699\n",
      "Client 74: Loss=2.6957, Acc=30.1781\n",
      "Client 48: Loss=2.5058, Acc=33.9750\n",
      "Client 58: Loss=2.6006, Acc=31.4984\n",
      "Client 20: Loss=2.7104, Acc=28.9267\n",
      "Client 60: Loss=2.9386, Acc=23.4672\n",
      "Client 19: Loss=2.9670, Acc=22.3358\n",
      "Client 86: Loss=2.4980, Acc=33.9984\n",
      "Round 2 - Global Loss: 2.5548, Accuracy: 32.70%\n",
      "Round 3/50\n",
      "Client 2: Loss=2.7262, Acc=27.1511\n",
      "Client 26: Loss=2.7344, Acc=27.1511\n",
      "Client 57: Loss=2.7414, Acc=26.3109\n",
      "Client 64: Loss=2.3219, Acc=36.5219\n",
      "Client 78: Loss=2.7182, Acc=26.2875\n",
      "Client 1: Loss=2.3861, Acc=35.1696\n",
      "Client 77: Loss=2.6677, Acc=27.1547\n",
      "Client 60: Loss=2.7638, Acc=26.3422\n",
      "Client 84: Loss=1.6255, Acc=53.4984\n",
      "Client 98: Loss=2.2827, Acc=36.8422\n",
      "Round 3 - Global Loss: 2.5003, Accuracy: 32.17%\n",
      "Round 4/50\n",
      "Client 69: Loss=2.2322, Acc=38.6703\n",
      "Client 22: Loss=2.3144, Acc=35.8017\n",
      "Client 2: Loss=2.5534, Acc=31.1852\n",
      "Client 61: Loss=1.8094, Acc=48.3734\n",
      "Client 77: Loss=2.4763, Acc=31.5609\n",
      "Client 81: Loss=2.1180, Acc=41.1469\n",
      "Client 35: Loss=2.2937, Acc=37.1938\n",
      "Client 54: Loss=2.3885, Acc=34.9281\n",
      "Client 74: Loss=2.4671, Acc=34.5219\n",
      "Client 58: Loss=2.3159, Acc=36.5297\n",
      "Round 4 - Global Loss: 2.2995, Accuracy: 36.93%\n",
      "Round 5/50\n",
      "Client 4: Loss=2.1319, Acc=40.0276\n",
      "Client 11: Loss=2.1230, Acc=39.1327\n",
      "Client 47: Loss=2.2030, Acc=38.4516\n",
      "Client 52: Loss=1.8072, Acc=47.7484\n",
      "Client 39: Loss=2.2251, Acc=37.3500\n",
      "Client 60: Loss=2.4778, Acc=30.5766\n",
      "Client 67: Loss=2.2251, Acc=37.1391\n",
      "Client 40: Loss=2.2563, Acc=37.1653\n",
      "Client 0: Loss=2.3431, Acc=33.3017\n",
      "Client 78: Loss=2.4302, Acc=30.8500\n",
      "Round 5 - Global Loss: 2.2220, Accuracy: 37.18%\n",
      "Round 6/50\n",
      "Client 97: Loss=2.0210, Acc=41.0766\n",
      "Client 86: Loss=2.0971, Acc=40.3344\n",
      "Client 20: Loss=2.2344, Acc=35.4821\n",
      "Client 87: Loss=2.1302, Acc=38.3500\n",
      "Client 55: Loss=2.1200, Acc=37.9359\n",
      "Client 18: Loss=2.0962, Acc=37.9679\n",
      "Client 95: Loss=2.3978, Acc=31.0609\n",
      "Client 76: Loss=2.2678, Acc=34.3344\n",
      "Client 16: Loss=1.9793, Acc=42.9253\n",
      "Client 21: Loss=2.2884, Acc=34.4452\n",
      "Round 6 - Global Loss: 2.1627, Accuracy: 37.40%\n",
      "Round 7/50\n",
      "Client 69: Loss=2.0702, Acc=39.5922\n",
      "Client 24: Loss=2.1887, Acc=36.7818\n",
      "Client 98: Loss=1.9916, Acc=41.9984\n",
      "Client 66: Loss=2.0753, Acc=40.8500\n",
      "Client 9: Loss=1.8697, Acc=43.6071\n",
      "Client 97: Loss=1.9944, Acc=41.1000\n",
      "Client 93: Loss=1.9807, Acc=42.8188\n",
      "Client 23: Loss=2.0739, Acc=38.9125\n",
      "Client 50: Loss=2.1795, Acc=36.4828\n",
      "Client 58: Loss=2.1524, Acc=37.4516\n",
      "Round 7 - Global Loss: 2.0572, Accuracy: 39.95%\n",
      "Round 8/50\n",
      "Client 57: Loss=2.3453, Acc=31.3422\n",
      "Client 95: Loss=2.3515, Acc=31.3344\n",
      "Client 8: Loss=1.9037, Acc=44.1398\n",
      "Client 70: Loss=2.2115, Acc=36.1000\n",
      "Client 3: Loss=2.1635, Acc=36.6185\n",
      "Client 43: Loss=2.0186, Acc=40.6455\n",
      "Client 12: Loss=2.1951, Acc=35.1838\n",
      "Client 73: Loss=2.1083, Acc=39.6234\n",
      "Client 84: Loss=1.4487, Acc=55.6234\n",
      "Client 93: Loss=1.9675, Acc=42.8578\n",
      "Round 8 - Global Loss: 2.0713, Accuracy: 39.34%\n",
      "Round 9/50\n",
      "Client 69: Loss=2.0416, Acc=39.5922\n",
      "Client 92: Loss=2.4267, Acc=30.7953\n",
      "Client 76: Loss=2.2102, Acc=34.5063\n",
      "Client 43: Loss=2.0048, Acc=40.7591\n",
      "Client 88: Loss=1.9993, Acc=40.4438\n",
      "Client 63: Loss=2.0674, Acc=39.5922\n",
      "Client 73: Loss=2.0982, Acc=39.7172\n",
      "Client 61: Loss=1.6667, Acc=49.4906\n",
      "Client 49: Loss=1.9442, Acc=44.9672\n",
      "Client 50: Loss=2.1501, Acc=36.4750\n",
      "Round 9 - Global Loss: 2.0604, Accuracy: 39.65%\n",
      "Round 10/50\n",
      "Client 87: Loss=2.0768, Acc=38.5297\n",
      "Client 51: Loss=2.2690, Acc=34.5297\n",
      "Client 17: Loss=1.9083, Acc=43.5858\n",
      "Client 48: Loss=2.0606, Acc=39.8188\n",
      "Client 9: Loss=1.8411, Acc=43.6710\n",
      "Client 76: Loss=2.1996, Acc=34.4906\n",
      "Client 26: Loss=2.3449, Acc=32.3145\n",
      "Client 83: Loss=2.1482, Acc=36.9750\n",
      "Client 18: Loss=2.0338, Acc=38.1526\n",
      "Client 73: Loss=2.0909, Acc=39.8188\n",
      "Round 10 - Global Loss: 2.0948, Accuracy: 38.24%\n",
      "Round 11/50\n",
      "Client 76: Loss=2.1902, Acc=34.5219\n",
      "Client 85: Loss=2.0184, Acc=39.8500\n",
      "Client 24: Loss=2.1329, Acc=37.0020\n",
      "Client 38: Loss=2.3459, Acc=30.4608\n",
      "Client 83: Loss=2.1402, Acc=36.9750\n",
      "Client 29: Loss=1.8140, Acc=45.3259\n",
      "Client 51: Loss=2.2609, Acc=34.5922\n",
      "Client 48: Loss=2.0533, Acc=39.8109\n",
      "Client 89: Loss=2.2048, Acc=35.6313\n",
      "Client 59: Loss=2.0038, Acc=40.3969\n",
      "Round 11 - Global Loss: 2.1159, Accuracy: 37.46%\n",
      "Round 12/50\n",
      "Client 66: Loss=2.0408, Acc=41.2797\n",
      "Client 22: Loss=2.0956, Acc=36.9239\n",
      "Client 51: Loss=2.2538, Acc=34.6234\n",
      "Client 62: Loss=1.9678, Acc=40.9906\n",
      "Client 33: Loss=2.1659, Acc=35.9509\n",
      "Client 41: Loss=1.9748, Acc=39.8500\n",
      "Client 95: Loss=2.3106, Acc=31.4672\n",
      "Client 56: Loss=2.2501, Acc=33.7250\n",
      "Client 21: Loss=2.2141, Acc=34.5020\n",
      "Client 13: Loss=2.0578, Acc=38.8699\n",
      "Round 12 - Global Loss: 2.1316, Accuracy: 36.84%\n",
      "Round 13/50\n",
      "Client 35: Loss=2.0670, Acc=38.0886\n",
      "Client 68: Loss=1.9857, Acc=40.9750\n",
      "Client 82: Loss=2.1933, Acc=35.2250\n",
      "Client 90: Loss=2.3371, Acc=31.8422\n",
      "Client 27: Loss=2.2846, Acc=32.9750\n",
      "Client 63: Loss=2.0354, Acc=39.7250\n",
      "Client 32: Loss=2.2885, Acc=32.4636\n",
      "Client 98: Loss=1.9385, Acc=42.1156\n",
      "Client 93: Loss=1.9358, Acc=43.0219\n",
      "Client 97: Loss=1.9381, Acc=41.1938\n",
      "Round 13 - Global Loss: 2.1037, Accuracy: 37.67%\n",
      "Round 14/50\n",
      "Client 76: Loss=2.1672, Acc=34.8344\n",
      "Client 48: Loss=2.0348, Acc=39.9203\n",
      "Client 90: Loss=2.3295, Acc=31.9984\n",
      "Client 81: Loss=1.8994, Acc=42.3188\n",
      "Client 57: Loss=2.2914, Acc=31.5922\n",
      "Client 63: Loss=2.0266, Acc=39.7250\n",
      "Client 31: Loss=2.1474, Acc=39.5375\n",
      "Client 78: Loss=2.2874, Acc=31.9438\n",
      "Client 65: Loss=1.6632, Acc=49.3188\n",
      "Client 38: Loss=2.3223, Acc=30.3969\n",
      "Round 14 - Global Loss: 2.1192, Accuracy: 37.12%\n",
      "Round 15/50\n",
      "Client 70: Loss=2.1636, Acc=36.1078\n",
      "Client 88: Loss=1.9553, Acc=40.6391\n",
      "Client 61: Loss=1.6346, Acc=49.9281\n",
      "Client 14: Loss=2.1324, Acc=36.7250\n",
      "Client 71: Loss=1.9279, Acc=42.6391\n",
      "Client 37: Loss=2.2250, Acc=33.6213\n",
      "Client 80: Loss=1.9434, Acc=42.3969\n",
      "Client 42: Loss=2.1034, Acc=36.2705\n",
      "Client 1: Loss=2.0234, Acc=39.5517\n",
      "Client 34: Loss=2.2861, Acc=32.5489\n",
      "Round 15 - Global Loss: 2.0450, Accuracy: 38.89%\n",
      "Round 16/50\n",
      "Client 74: Loss=2.3091, Acc=36.2016\n",
      "Client 26: Loss=2.3058, Acc=32.2790\n",
      "Client 80: Loss=1.9389, Acc=42.6000\n",
      "Client 70: Loss=2.1582, Acc=36.3422\n",
      "Client 19: Loss=2.3370, Acc=30.4253\n",
      "Client 27: Loss=2.2635, Acc=33.7491\n",
      "Client 78: Loss=2.2735, Acc=32.5297\n",
      "Client 12: Loss=2.1290, Acc=35.9580\n",
      "Client 39: Loss=2.0767, Acc=37.9608\n",
      "Client 82: Loss=2.1741, Acc=36.0141\n",
      "Round 16 - Global Loss: 2.1978, Accuracy: 35.34%\n",
      "Round 17/50\n",
      "Client 35: Loss=2.0452, Acc=38.7349\n",
      "Client 0: Loss=2.1808, Acc=34.6085\n",
      "Client 47: Loss=2.0459, Acc=40.3891\n",
      "Client 93: Loss=1.9176, Acc=43.4203\n",
      "Client 15: Loss=2.0618, Acc=38.2804\n",
      "Client 43: Loss=1.9529, Acc=40.9438\n",
      "Client 45: Loss=1.9580, Acc=42.0801\n",
      "Client 5: Loss=2.0948, Acc=38.4295\n",
      "Client 4: Loss=1.9919, Acc=40.6455\n",
      "Client 33: Loss=2.1335, Acc=36.3202\n",
      "Round 17 - Global Loss: 2.0393, Accuracy: 39.34%\n",
      "Round 18/50\n",
      "Client 62: Loss=1.9391, Acc=41.7016\n",
      "Client 66: Loss=2.0270, Acc=39.8031\n",
      "Client 40: Loss=2.0907, Acc=37.3074\n",
      "Client 97: Loss=1.9070, Acc=43.0688\n",
      "Client 9: Loss=1.8013, Acc=44.0403\n",
      "Client 98: Loss=1.9085, Acc=43.4750\n",
      "Client 50: Loss=2.0877, Acc=37.2797\n",
      "Client 86: Loss=1.9840, Acc=41.4828\n",
      "Client 30: Loss=1.9626, Acc=42.2719\n",
      "Client 15: Loss=2.0551, Acc=38.6284\n",
      "Round 18 - Global Loss: 1.9763, Accuracy: 40.89%\n",
      "Round 19/50\n",
      "Client 9: Loss=1.7958, Acc=44.1469\n",
      "Client 21: Loss=2.1745, Acc=36.1497\n",
      "Client 73: Loss=2.0484, Acc=40.1078\n",
      "Client 46: Loss=2.3282, Acc=31.6966\n",
      "Client 36: Loss=2.2218, Acc=34.6653\n",
      "Client 25: Loss=2.1717, Acc=35.6313\n",
      "Client 89: Loss=2.1492, Acc=37.8266\n",
      "Client 43: Loss=1.9409, Acc=40.9864\n",
      "Client 6: Loss=1.8104, Acc=44.8642\n",
      "Client 56: Loss=2.2162, Acc=34.5531\n",
      "Round 19 - Global Loss: 2.0842, Accuracy: 38.08%\n",
      "Round 20/50\n",
      "Client 27: Loss=2.2374, Acc=35.1909\n",
      "Client 50: Loss=2.0772, Acc=37.5844\n",
      "Client 64: Loss=1.9439, Acc=43.1156\n",
      "Client 41: Loss=1.9301, Acc=40.2264\n",
      "Client 43: Loss=1.9336, Acc=41.0006\n",
      "Client 15: Loss=2.0439, Acc=38.7491\n",
      "Client 28: Loss=2.0972, Acc=38.5077\n",
      "Client 93: Loss=1.9043, Acc=43.8891\n",
      "Client 57: Loss=2.2560, Acc=33.7250\n",
      "Client 88: Loss=1.9274, Acc=41.2875\n",
      "Round 20 - Global Loss: 2.0357, Accuracy: 39.30%\n",
      "Round 21/50\n",
      "Client 47: Loss=2.0188, Acc=41.2563\n",
      "Client 27: Loss=2.2308, Acc=35.3756\n",
      "Client 56: Loss=2.2082, Acc=34.7172\n",
      "Client 58: Loss=2.0655, Acc=40.0453\n",
      "Client 93: Loss=1.8995, Acc=44.2797\n",
      "Client 88: Loss=1.9210, Acc=41.5531\n",
      "Client 46: Loss=2.3137, Acc=32.2577\n",
      "Client 50: Loss=2.0703, Acc=37.8266\n",
      "Client 78: Loss=2.2429, Acc=32.9750\n",
      "Client 75: Loss=2.2898, Acc=32.8031\n",
      "Round 21 - Global Loss: 2.1289, Accuracy: 37.24%\n",
      "Round 22/50\n",
      "Client 3: Loss=2.0605, Acc=39.7648\n",
      "Client 90: Loss=2.2857, Acc=33.0844\n",
      "Client 7: Loss=2.3032, Acc=32.3074\n",
      "Client 92: Loss=2.3564, Acc=30.5688\n",
      "Client 72: Loss=1.9806, Acc=40.8031\n",
      "Client 75: Loss=2.2806, Acc=33.1703\n",
      "Client 43: Loss=1.9213, Acc=41.1710\n",
      "Client 74: Loss=2.2836, Acc=37.1703\n",
      "Client 68: Loss=1.9399, Acc=41.6234\n",
      "Client 23: Loss=1.9610, Acc=40.6881\n",
      "Round 22 - Global Loss: 2.1344, Accuracy: 37.09%\n",
      "Round 23/50\n",
      "Client 3: Loss=2.0536, Acc=39.9210\n",
      "Client 60: Loss=2.2340, Acc=33.2797\n",
      "Client 40: Loss=2.0641, Acc=37.5702\n",
      "Client 76: Loss=2.1096, Acc=37.0453\n",
      "Client 38: Loss=2.2690, Acc=31.5759\n",
      "Client 66: Loss=2.0086, Acc=39.1313\n",
      "Client 99: Loss=1.8418, Acc=44.6469\n",
      "Client 14: Loss=2.0864, Acc=38.9764\n",
      "Client 98: Loss=1.8821, Acc=44.2641\n",
      "Client 28: Loss=2.0827, Acc=38.5503\n",
      "Round 23 - Global Loss: 2.0655, Accuracy: 38.44%\n",
      "Round 24/50\n",
      "Client 1: Loss=1.9812, Acc=41.4480\n",
      "Client 57: Loss=2.2344, Acc=33.9516\n",
      "Client 69: Loss=1.9447, Acc=43.0297\n",
      "Client 5: Loss=2.0567, Acc=39.4736\n",
      "Client 4: Loss=1.9590, Acc=41.5332\n",
      "Client 86: Loss=1.9545, Acc=42.2172\n",
      "Client 24: Loss=2.0502, Acc=38.5148\n",
      "Client 11: Loss=1.9326, Acc=40.7591\n",
      "Client 90: Loss=2.2737, Acc=33.3109\n",
      "Client 16: Loss=1.8467, Acc=44.9778\n",
      "Round 24 - Global Loss: 2.0204, Accuracy: 39.99%\n",
      "Round 25/50\n",
      "Client 74: Loss=2.2735, Acc=37.5141\n",
      "Client 87: Loss=1.9933, Acc=41.0219\n",
      "Client 94: Loss=1.9126, Acc=44.0766\n",
      "Client 31: Loss=2.0930, Acc=40.7946\n",
      "Client 18: Loss=1.9548, Acc=39.6795\n",
      "Client 49: Loss=1.9004, Acc=42.9750\n",
      "Client 57: Loss=2.2285, Acc=34.0609\n",
      "Client 47: Loss=1.9927, Acc=41.3578\n",
      "Client 76: Loss=2.0973, Acc=37.3813\n",
      "Client 50: Loss=2.0497, Acc=38.7172\n",
      "Round 25 - Global Loss: 2.0491, Accuracy: 39.77%\n",
      "Round 26/50\n",
      "Client 58: Loss=2.0409, Acc=40.1313\n",
      "Client 62: Loss=1.9045, Acc=42.9984\n",
      "Client 89: Loss=2.1098, Acc=39.1391\n",
      "Client 91: Loss=1.7657, Acc=46.6313\n",
      "Client 4: Loss=1.9503, Acc=41.5190\n",
      "Client 81: Loss=1.8305, Acc=44.3344\n",
      "Client 50: Loss=2.0439, Acc=38.6703\n",
      "Client 27: Loss=2.2014, Acc=35.7449\n",
      "Client 55: Loss=1.9567, Acc=39.8266\n",
      "Client 94: Loss=1.9086, Acc=44.0609\n",
      "Round 26 - Global Loss: 1.9733, Accuracy: 41.25%\n",
      "Round 27/50\n",
      "Client 7: Loss=2.2766, Acc=32.0872\n",
      "Client 46: Loss=2.2785, Acc=32.8472\n",
      "Client 62: Loss=1.8984, Acc=43.0688\n",
      "Client 94: Loss=1.9043, Acc=44.0375\n",
      "Client 52: Loss=1.6284, Acc=49.3891\n",
      "Client 30: Loss=1.9220, Acc=43.2307\n",
      "Client 82: Loss=2.1069, Acc=37.8344\n",
      "Client 3: Loss=2.0326, Acc=40.1696\n",
      "Client 85: Loss=1.9572, Acc=41.0141\n",
      "Client 57: Loss=2.2199, Acc=34.1859\n",
      "Round 27 - Global Loss: 2.0265, Accuracy: 39.68%\n",
      "Round 28/50\n",
      "Client 27: Loss=2.1901, Acc=35.8514\n",
      "Client 72: Loss=1.9465, Acc=41.3266\n",
      "Client 90: Loss=2.2548, Acc=33.7094\n",
      "Client 85: Loss=1.9524, Acc=41.0531\n",
      "Client 67: Loss=2.0075, Acc=40.1078\n",
      "Client 53: Loss=1.9013, Acc=42.7328\n",
      "Client 4: Loss=1.9424, Acc=41.5403\n",
      "Client 29: Loss=1.7330, Acc=47.5844\n",
      "Client 54: Loss=2.0801, Acc=37.2641\n",
      "Client 62: Loss=1.8937, Acc=43.2016\n",
      "Round 28 - Global Loss: 1.9892, Accuracy: 40.47%\n",
      "Round 29/50\n",
      "Client 11: Loss=1.9078, Acc=40.6526\n",
      "Client 16: Loss=1.8278, Acc=45.2619\n",
      "Client 19: Loss=2.2706, Acc=32.0588\n",
      "Client 37: Loss=2.1577, Acc=34.8571\n",
      "Client 8: Loss=1.7887, Acc=46.5830\n",
      "Client 46: Loss=2.2668, Acc=32.7832\n",
      "Client 84: Loss=1.3743, Acc=57.2797\n",
      "Client 81: Loss=1.8132, Acc=44.5609\n",
      "Client 56: Loss=2.1767, Acc=34.9828\n",
      "Client 99: Loss=1.8205, Acc=44.9672\n",
      "Round 29 - Global Loss: 1.9458, Accuracy: 41.25%\n",
      "Round 30/50\n",
      "Client 19: Loss=2.2639, Acc=32.2364\n",
      "Client 89: Loss=2.0874, Acc=39.6000\n",
      "Client 60: Loss=2.1904, Acc=33.7563\n",
      "Client 83: Loss=2.0406, Acc=37.8734\n",
      "Client 37: Loss=2.1502, Acc=35.1341\n",
      "Client 23: Loss=1.9202, Acc=41.7037\n",
      "Client 5: Loss=2.0262, Acc=40.2903\n",
      "Client 57: Loss=2.2047, Acc=34.3969\n",
      "Client 61: Loss=1.5798, Acc=51.3656\n",
      "Client 44: Loss=2.1823, Acc=35.1483\n",
      "Round 30 - Global Loss: 2.0667, Accuracy: 38.09%\n",
      "Round 31/50\n",
      "Client 60: Loss=2.1827, Acc=33.8500\n",
      "Client 11: Loss=1.8966, Acc=40.9011\n",
      "Client 72: Loss=1.9305, Acc=41.4828\n",
      "Client 2: Loss=2.2272, Acc=33.6710\n",
      "Client 32: Loss=2.1820, Acc=36.3699\n",
      "Client 30: Loss=1.9046, Acc=43.4082\n",
      "Client 47: Loss=1.9594, Acc=41.6391\n",
      "Client 9: Loss=1.7399, Acc=45.5815\n",
      "Client 96: Loss=2.0644, Acc=39.0922\n",
      "Client 24: Loss=2.0189, Acc=39.1682\n",
      "Round 31 - Global Loss: 2.0097, Accuracy: 39.54%\n",
      "Round 32/50\n",
      "Client 12: Loss=2.0398, Acc=38.2591\n",
      "Client 93: Loss=1.8559, Acc=44.5766\n",
      "Client 6: Loss=1.7432, Acc=46.0290\n",
      "Client 72: Loss=1.9233, Acc=41.7172\n",
      "Client 68: Loss=1.8960, Acc=42.2328\n",
      "Client 27: Loss=2.1673, Acc=35.9864\n",
      "Client 33: Loss=2.0588, Acc=37.9892\n",
      "Client 30: Loss=1.8988, Acc=43.5787\n",
      "Client 83: Loss=2.0300, Acc=37.8656\n",
      "Client 5: Loss=2.0140, Acc=40.4963\n",
      "Round 32 - Global Loss: 1.9641, Accuracy: 40.85%\n",
      "Round 33/50\n",
      "Client 10: Loss=2.0693, Acc=37.6128\n",
      "Client 33: Loss=2.0523, Acc=37.8827\n",
      "Client 46: Loss=2.2427, Acc=33.0531\n",
      "Client 5: Loss=2.0063, Acc=40.6668\n",
      "Client 3: Loss=2.0024, Acc=40.6526\n",
      "Client 90: Loss=2.2289, Acc=33.7406\n",
      "Client 52: Loss=1.6041, Acc=49.6938\n",
      "Client 50: Loss=2.0107, Acc=38.5453\n",
      "Client 80: Loss=1.8710, Acc=43.7406\n",
      "Client 20: Loss=2.0152, Acc=38.4580\n",
      "Round 33 - Global Loss: 2.0134, Accuracy: 39.33%\n",
      "Round 34/50\n",
      "Client 70: Loss=2.0719, Acc=37.7016\n",
      "Client 12: Loss=2.0285, Acc=38.2236\n",
      "Client 68: Loss=1.8844, Acc=42.4516\n",
      "Client 35: Loss=1.9638, Acc=40.1980\n",
      "Client 69: Loss=1.8954, Acc=43.3813\n",
      "Client 84: Loss=1.3563, Acc=57.4906\n",
      "Client 31: Loss=2.0565, Acc=41.7605\n",
      "Client 19: Loss=2.2438, Acc=32.4849\n",
      "Client 37: Loss=2.1298, Acc=35.3259\n",
      "Client 41: Loss=1.8562, Acc=41.4693\n",
      "Round 34 - Global Loss: 1.9542, Accuracy: 40.89%\n",
      "Round 35/50\n",
      "Client 28: Loss=2.0338, Acc=39.5730\n",
      "Client 49: Loss=1.8624, Acc=43.1391\n",
      "Client 21: Loss=2.0920, Acc=36.5190\n",
      "Client 84: Loss=1.3530, Acc=57.5453\n",
      "Client 62: Loss=1.8665, Acc=43.6156\n",
      "Client 78: Loss=2.1615, Acc=34.9125\n",
      "Client 19: Loss=2.2366, Acc=32.4281\n",
      "Client 79: Loss=1.9242, Acc=41.6781\n",
      "Client 69: Loss=1.8885, Acc=43.6000\n",
      "Client 72: Loss=1.9077, Acc=41.9906\n",
      "Round 35 - Global Loss: 1.9381, Accuracy: 41.34%\n",
      "Round 36/50\n",
      "Client 39: Loss=1.9754, Acc=40.1554\n",
      "Client 78: Loss=2.1539, Acc=35.0531\n",
      "Client 38: Loss=2.2046, Acc=32.5418\n",
      "Client 7: Loss=2.2376, Acc=32.5134\n",
      "Client 23: Loss=1.8859, Acc=42.4565\n",
      "Client 10: Loss=2.0522, Acc=37.8045\n",
      "Client 40: Loss=2.0049, Acc=39.0545\n",
      "Client 14: Loss=2.0207, Acc=39.2321\n",
      "Client 1: Loss=1.9289, Acc=41.9665\n",
      "Client 95: Loss=2.1426, Acc=36.1313\n",
      "Round 36 - Global Loss: 2.0591, Accuracy: 37.73%\n",
      "Round 37/50\n",
      "Client 32: Loss=2.1470, Acc=36.1639\n",
      "Client 27: Loss=2.1383, Acc=36.2847\n",
      "Client 62: Loss=1.8570, Acc=43.8578\n",
      "Client 10: Loss=2.0444, Acc=37.9466\n",
      "Client 76: Loss=2.0301, Acc=38.0922\n",
      "Client 82: Loss=2.0509, Acc=38.2563\n",
      "Client 99: Loss=1.7904, Acc=45.4828\n",
      "Client 37: Loss=2.1156, Acc=35.9935\n",
      "Client 60: Loss=2.1452, Acc=34.2016\n",
      "Client 12: Loss=2.0094, Acc=38.3301\n",
      "Round 37 - Global Loss: 2.0356, Accuracy: 38.39%\n",
      "Round 38/50\n",
      "Client 88: Loss=1.8421, Acc=43.1781\n",
      "Client 55: Loss=1.8994, Acc=40.5844\n",
      "Client 41: Loss=1.8323, Acc=42.2151\n",
      "Client 79: Loss=1.9051, Acc=42.1703\n",
      "Client 87: Loss=1.9332, Acc=41.5531\n",
      "Client 69: Loss=1.8724, Acc=43.4906\n",
      "Client 73: Loss=1.9654, Acc=41.6703\n",
      "Client 60: Loss=2.1380, Acc=34.5297\n",
      "Client 7: Loss=2.2260, Acc=32.5773\n",
      "Client 81: Loss=1.7646, Acc=45.6547\n",
      "Round 38 - Global Loss: 1.9396, Accuracy: 40.70%\n",
      "Round 39/50\n",
      "Client 79: Loss=1.8964, Acc=42.3500\n",
      "Client 51: Loss=2.1276, Acc=36.7406\n",
      "Client 36: Loss=2.1153, Acc=36.1000\n",
      "Client 28: Loss=2.0171, Acc=39.7151\n",
      "Client 99: Loss=1.7828, Acc=45.4125\n",
      "Client 17: Loss=1.7750, Acc=45.3045\n",
      "Client 82: Loss=2.0363, Acc=38.2563\n",
      "Client 43: Loss=1.8110, Acc=44.2179\n",
      "Client 90: Loss=2.1961, Acc=34.6547\n",
      "Client 45: Loss=1.8522, Acc=44.3244\n",
      "Round 39 - Global Loss: 1.9588, Accuracy: 40.77%\n",
      "Round 40/50\n",
      "Client 46: Loss=2.2040, Acc=33.4793\n",
      "Client 29: Loss=1.6850, Acc=48.4864\n",
      "Client 65: Loss=1.5609, Acc=51.6234\n",
      "Client 58: Loss=1.9788, Acc=40.4281\n",
      "Client 56: Loss=2.1204, Acc=35.4516\n",
      "Client 18: Loss=1.8954, Acc=40.5247\n",
      "Client 32: Loss=2.1282, Acc=36.1142\n",
      "Client 44: Loss=2.1153, Acc=36.3415\n",
      "Client 96: Loss=2.0154, Acc=39.8422\n",
      "Client 70: Loss=2.0376, Acc=38.3031\n",
      "Round 40 - Global Loss: 1.9756, Accuracy: 40.01%\n",
      "Round 41/50\n",
      "Client 49: Loss=1.8328, Acc=43.9203\n",
      "Client 84: Loss=1.3351, Acc=57.7094\n",
      "Client 31: Loss=2.0256, Acc=42.4494\n",
      "Client 21: Loss=2.0574, Acc=36.9097\n",
      "Client 35: Loss=1.9304, Acc=40.4466\n",
      "Client 14: Loss=1.9885, Acc=39.2889\n",
      "Client 70: Loss=2.0299, Acc=38.4281\n",
      "Client 89: Loss=2.0268, Acc=39.9281\n",
      "Client 68: Loss=1.8512, Acc=43.5688\n",
      "Client 80: Loss=1.8359, Acc=44.2719\n",
      "Round 41 - Global Loss: 1.8956, Accuracy: 42.58%\n",
      "Round 42/50\n",
      "Client 81: Loss=1.7458, Acc=46.0297\n",
      "Client 12: Loss=1.9836, Acc=38.9906\n",
      "Client 63: Loss=1.8803, Acc=42.1547\n",
      "Client 59: Loss=1.8345, Acc=43.6703\n",
      "Client 24: Loss=1.9655, Acc=40.1128\n",
      "Client 21: Loss=2.0514, Acc=37.1440\n",
      "Client 13: Loss=1.9126, Acc=41.3344\n",
      "Client 76: Loss=1.9988, Acc=38.9984\n",
      "Client 38: Loss=2.1731, Acc=33.0531\n",
      "Client 78: Loss=2.1212, Acc=35.9906\n",
      "Round 42 - Global Loss: 1.9691, Accuracy: 39.67%\n",
      "Round 43/50\n",
      "Client 48: Loss=1.8818, Acc=44.8656\n",
      "Client 88: Loss=1.8181, Acc=43.2094\n",
      "Client 67: Loss=1.9340, Acc=41.2953\n",
      "Client 13: Loss=1.9052, Acc=41.3415\n",
      "Client 25: Loss=2.0562, Acc=37.8756\n",
      "Client 19: Loss=2.1888, Acc=32.7974\n",
      "Client 61: Loss=1.5337, Acc=52.6156\n",
      "Client 37: Loss=2.0900, Acc=36.4480\n",
      "Client 59: Loss=1.8276, Acc=43.6938\n",
      "Client 85: Loss=1.9019, Acc=41.5844\n",
      "Round 43 - Global Loss: 1.9194, Accuracy: 41.40%\n",
      "Round 44/50\n",
      "Client 81: Loss=1.7359, Acc=46.5219\n",
      "Client 16: Loss=1.7739, Acc=45.4324\n",
      "Client 74: Loss=2.1847, Acc=38.3969\n",
      "Client 9: Loss=1.6752, Acc=46.6682\n",
      "Client 56: Loss=2.0990, Acc=35.9984\n",
      "Client 54: Loss=2.0033, Acc=38.4906\n",
      "Client 92: Loss=2.2228, Acc=33.2797\n",
      "Client 96: Loss=1.9927, Acc=40.3266\n",
      "Client 43: Loss=1.7805, Acc=45.0844\n",
      "Client 78: Loss=2.1096, Acc=36.1234\n",
      "Round 44 - Global Loss: 1.9515, Accuracy: 40.78%\n",
      "Round 45/50\n",
      "Client 55: Loss=1.8711, Acc=40.7250\n",
      "Client 19: Loss=2.1802, Acc=32.9324\n",
      "Client 89: Loss=2.0091, Acc=39.9672\n",
      "Client 93: Loss=1.8036, Acc=46.1547\n",
      "Client 16: Loss=1.7695, Acc=45.5105\n",
      "Client 85: Loss=1.8950, Acc=41.9047\n",
      "Client 91: Loss=1.6875, Acc=47.7094\n",
      "Client 92: Loss=2.2028, Acc=33.6078\n",
      "Client 2: Loss=2.1612, Acc=35.2335\n",
      "Client 22: Loss=1.9467, Acc=40.5389\n",
      "Round 45 - Global Loss: 1.9550, Accuracy: 40.36%\n",
      "Round 46/50\n",
      "Client 53: Loss=1.8095, Acc=43.9750\n",
      "Client 51: Loss=2.0888, Acc=38.3031\n",
      "Client 4: Loss=1.8691, Acc=42.8330\n",
      "Client 89: Loss=2.0016, Acc=40.0453\n",
      "Client 65: Loss=1.5426, Acc=51.9203\n",
      "Client 45: Loss=1.8197, Acc=44.8784\n",
      "Client 38: Loss=2.1516, Acc=33.7847\n",
      "Client 98: Loss=1.7761, Acc=45.9594\n",
      "Client 91: Loss=1.6820, Acc=47.8422\n",
      "Client 71: Loss=1.7726, Acc=45.1703\n",
      "Round 46 - Global Loss: 1.8541, Accuracy: 43.38%\n",
      "Round 47/50\n",
      "Client 85: Loss=1.8877, Acc=42.1469\n",
      "Client 41: Loss=1.7880, Acc=43.7847\n",
      "Client 27: Loss=2.0878, Acc=37.2222\n",
      "Client 50: Loss=1.9430, Acc=39.0297\n",
      "Client 20: Loss=1.9394, Acc=39.5730\n",
      "Client 84: Loss=1.3195, Acc=58.0375\n",
      "Client 12: Loss=1.9557, Acc=39.4381\n",
      "Client 1: Loss=1.8883, Acc=43.4580\n",
      "Client 0: Loss=2.0054, Acc=38.3443\n",
      "Client 34: Loss=2.1841, Acc=35.8372\n",
      "Round 47 - Global Loss: 1.9050, Accuracy: 41.55%\n",
      "Round 48/50\n",
      "Client 77: Loss=2.0116, Acc=37.7641\n",
      "Client 95: Loss=2.0636, Acc=37.3734\n",
      "Client 78: Loss=2.0897, Acc=36.2484\n",
      "Client 73: Loss=1.9168, Acc=42.1547\n",
      "Client 91: Loss=1.6745, Acc=47.8422\n",
      "Client 47: Loss=1.8789, Acc=42.3500\n",
      "Client 94: Loss=1.8436, Acc=44.9516\n",
      "Client 69: Loss=1.8289, Acc=43.8422\n",
      "Client 89: Loss=1.9916, Acc=40.1625\n",
      "Client 56: Loss=2.0755, Acc=36.0141\n",
      "Round 48 - Global Loss: 1.9375, Accuracy: 40.87%\n",
      "Round 49/50\n",
      "Client 69: Loss=1.8237, Acc=43.9203\n",
      "Client 10: Loss=1.9833, Acc=38.6355\n",
      "Client 6: Loss=1.6640, Acc=48.3372\n",
      "Client 12: Loss=1.9444, Acc=39.6582\n",
      "Client 89: Loss=1.9847, Acc=40.1781\n",
      "Client 75: Loss=2.1160, Acc=35.4437\n",
      "Client 33: Loss=1.9785, Acc=39.2534\n",
      "Client 53: Loss=1.7959, Acc=44.3266\n",
      "Client 16: Loss=1.7555, Acc=45.7946\n",
      "Client 85: Loss=1.8799, Acc=42.4047\n",
      "Round 49 - Global Loss: 1.8913, Accuracy: 41.82%\n",
      "Round 50/50\n",
      "Client 87: Loss=1.8832, Acc=42.1625\n",
      "Client 32: Loss=2.0722, Acc=36.7250\n",
      "Client 40: Loss=1.9432, Acc=40.4537\n",
      "Client 5: Loss=1.9241, Acc=41.5901\n",
      "Client 79: Loss=1.8410, Acc=43.9438\n",
      "Client 45: Loss=1.8000, Acc=45.5531\n",
      "Client 35: Loss=1.8962, Acc=41.2776\n",
      "Client 96: Loss=1.9648, Acc=41.0375\n",
      "Client 11: Loss=1.8101, Acc=43.3656\n",
      "Client 62: Loss=1.8076, Acc=44.4125\n",
      "Round 50 - Global Loss: 1.8950, Accuracy: 42.02%\n",
      "Local steps=16 -> Test Accuracy: 38.38092783505155\n",
      "Results saved successfully to c:\\Users\\rosif\\OneDrive\\Desktop\\Advance Machine Learning\\Project2024\\AdvanceML_project5\\processed_data\\Federated\n",
      "All experiments completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Dataset and training configurations\n",
    "    data_path = \"shakespeare.txt\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "    epochs = 20  # Number of epochs for centralized training\n",
    "    fraction = 0.1  # Fraction of clients to select each round\n",
    "    seq_length = 80  # Sequence length for LSTM inputs   \n",
    "    batch_size = 4 # For local training\n",
    "    n_vocab = 90 # Character number in vobulary (ASCII)\n",
    "    embedding_size = 8\n",
    "    hidden_dim = 256\n",
    "    train_split = 0.8\n",
    "    momentum = 0\n",
    "    learning_rate = 0.1\n",
    "    weight_decay = 0.0001\n",
    "    C = 0.1\n",
    "\n",
    "    # Load data\n",
    "    train_data, test_data = parse_shakespeare(data_path, train_split)\n",
    "\n",
    "    # Centralized Dataset Preparation\n",
    "    train_dataset = ShakespeareDataset(train_data, seq_length=seq_length, n_vocab=n_vocab)\n",
    "    test_dataset = ShakespeareDataset(test_data, seq_length=seq_length, n_vocab=n_vocab)\n",
    "    train_size = int(0.9 * len(train_dataset))  # 90% of data for training\n",
    "    val_size = len(train_dataset) - train_size  # 10% of data for validation\n",
    "    train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    # EXPERIMENTS\n",
    "\n",
    "    local_steps = [4, 8, 16] #what is called J -> # Number of local training steps\n",
    "    # Scale the number of rounds inversely with J to maintain a constant computational budget\n",
    "    num_rounds = {4: 200, 8: 100, 16: 50} # Number of federated communication rounds\n",
    "\n",
    "\n",
    "    # # The first FL baseline\n",
    "    # print(\"FIRST FL BASELINE\")\n",
    "\n",
    "    # num_clients = 100\n",
    "    # num_classes = 100 \n",
    "    # iid = True #iid\n",
    "    # C = 0.1\n",
    "    # local_steps = 4\n",
    "\n",
    "    # rounds = num_rounds[local_steps]\n",
    "\n",
    "    # global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    # server = Server(test_data, val_loader, global_model, device)\n",
    "    # criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    # optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    # global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count = server.train_federated(\n",
    "    #     train_loader, val_loader, test_loader, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "    #     batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "    # # Test\n",
    "    # test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    # print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # filename = f\"First_baseline_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "    # save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    # plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    # plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    # # The impact of client participation\n",
    "    # print(\"THE IMPACT OF CLIENT PARTECIPATION\")\n",
    "\n",
    "    # num_clients = 100\n",
    "    # num_classes = 100\n",
    "    # iid = True #iid\n",
    "    # C = 0.1\n",
    "\n",
    "    # local_steps = 4\n",
    "\n",
    "    # rounds = num_rounds[local_steps]\n",
    "\n",
    "    # print(\"Uniform partecipation\")\n",
    "\n",
    "    # global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    # server = Server(test_data, val_loader, global_model, device)\n",
    "    # criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    # optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    # global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count = server.train_federated(\n",
    "    #     train_loader, val_loader, test_loader, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "    #     batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "    # # Test\n",
    "    # test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    # print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # filename = f\"Uniform_Client_partecipation_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "    # save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    # plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    # plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    # num_clients = 100\n",
    "    # num_classes = 100\n",
    "    # iid = False #iid\n",
    "    # C = 0.1\n",
    "    # local_steps = 4\n",
    "\n",
    "    # rounds = num_rounds[local_steps]\n",
    "\n",
    "    # print(\"Skewed partecipation\")\n",
    "\n",
    "    # # Values of gamma to test\n",
    "    # gamma_values = [0.1, 0.5, 1.0, 5.0]  # Skewness parameter for Dirichlet sampling\n",
    "    # for gamma in gamma_values:\n",
    "    #     global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    #     server = Server(test_data, val_loader, global_model, device)\n",
    "    #     criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    #     optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    #     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    #     global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count = server.train_federated(\n",
    "    #         train_loader, val_loader, test_loader, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "    #         batch_size, weight_decay, seq_length, C, local_steps, iid, \"skewed\", gamma)\n",
    "\n",
    "    #     # Test\n",
    "    #     test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    #     print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    #     filename = f\"Skewed_Client_partecipation_Gamma_{gamma}_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "    #     save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    #     plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    #     plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    # # Simulate heterogeneous distributions \n",
    "    # print(\"SIMULATE HETEROGENEOUS DISTRIBUTIONS\")\n",
    "\n",
    "\n",
    "    # print(\"Non-iid shardings\")\n",
    "    # num_clients = 100\n",
    "    # num_classes = [1, 5, 10, 50]\n",
    "    # num_classes = [5] \n",
    "    # iid = False # non-iid\n",
    "    # C = 0.1\n",
    "\n",
    "    # local_steps = 4\n",
    "\n",
    "    # rounds = num_rounds[local_steps]\n",
    "\n",
    "    # for nc in num_classes:\n",
    "    #     global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "    #     server = Server(test_data, val_loader, global_model, device)\n",
    "    #     criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "    #     optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "    #     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "    #     global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count = server.train_federated(\n",
    "    #         train_loader, val_loader, test_loader, criterion, rounds, nc, num_clients, fraction, device, learning_rate, momentum, \n",
    "    #         batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "    #     # Test\n",
    "    #     test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "    #     print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    #     filename = f\"Non_iid_Num_classes_{nc}_local_steps_{local_steps}\"\n",
    "    #     save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "    #     plot_results_federated(train_losses, train_accuracies, filename)\n",
    "    #     plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "\n",
    "\n",
    "    print(\"iid shardings\")\n",
    "    num_clients = 100\n",
    "    num_classes = 100 \n",
    "    iid = True # iid\n",
    "    C = 0.1\n",
    "\n",
    "    local_steps_list = [4, 8, 16]  # Varying local steps\n",
    "    local_steps_list = [16]\n",
    "\n",
    "    for local_steps in local_steps_list:\n",
    "\n",
    "        rounds = num_rounds[local_steps]\n",
    "    \n",
    "        global_model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2) # Initialize global LSTM model\n",
    "        server = Server(test_data, val_loader, global_model, device)\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        optimizer = optim.SGD(global_model.parameters(), learning_rate, momentum, weight_decay)  # Optimizer\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "        global_model, train_accuracies, train_losses, validation_accuracies, validation_losses, client_sel_count = server.train_federated(\n",
    "            train_loader, val_loader, test_loader, criterion, rounds, num_classes, num_clients, fraction, device, learning_rate, momentum, \n",
    "            batch_size, weight_decay, seq_length, C, local_steps, iid, \"uniform\")\n",
    "\n",
    "        # Test\n",
    "        test_loss, test_accuracy = evaluate_model(global_model, test_loader, criterion, device) \n",
    "        print(f\"Local steps={local_steps} -> Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        filename = f\"iid_Num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "        save_results_federated(global_model, train_accuracies, train_losses, test_accuracy, test_loss, client_sel_count, filename)\n",
    "        plot_results_federated(train_losses, train_accuracies, filename)\n",
    "        plot_sampling_distributions(client_sel_count, f\"{filename}_distribution\")\n",
    "    \n",
    "    print(\"All experiments completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-project-NFYoYubB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
