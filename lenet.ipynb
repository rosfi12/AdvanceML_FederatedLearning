{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Project 5 - Federated Learning - Track B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "cuda\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision.datasets.cifar import CIFAR100\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def emit(self, record) -> None:\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(\"\\r\\033[K\" + msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    COLORS = {\n",
    "        \"DEBUG\": \"\\033[1;34m\",\n",
    "        \"INFO\": \"\\033[1;32m\",\n",
    "        \"WARNING\": \"\\033[1;33m\",\n",
    "        \"ERROR\": \"\\033[1;31m\",\n",
    "        \"CRITICAL\": \"\\033[1;35m\",\n",
    "        \"RESET\": \"\\033[0m\",\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        levelname = record.levelname\n",
    "        if levelname in self.COLORS:\n",
    "            record.levelname = (\n",
    "                f\"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}\"\n",
    "            )\n",
    "        return super().format(record)\n",
    "\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()\n",
    "    root_logger.setLevel(level)\n",
    "\n",
    "    tqdm_handler = TqdmLoggingHandler()\n",
    "    formatter = ColoredFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    tqdm_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(tqdm_handler)\n",
    "\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BaseConfig:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CPU_COUNT = os.cpu_count() or 1\n",
    "    NUM_WORKERS = min(1, CPU_COUNT)\n",
    "    SEED = 42\n",
    "\n",
    "    # Paths\n",
    "    ROOT_DIR: Path = Path.cwd()\n",
    "    CONFIGS_DIR: Path = ROOT_DIR / \"configs\"\n",
    "    DATA_DIR: Path = ROOT_DIR / \"data\"\n",
    "    MODELS_DIR: Path = ROOT_DIR / \"models\"\n",
    "    RESULTS_DIR: Path = ROOT_DIR / \"results\"\n",
    "    RUNS_DIR: Path = ROOT_DIR / \"runs\"\n",
    "    OLD_RUNS_DIR: Path = RUNS_DIR / \"old_runs\"\n",
    "\n",
    "    # Training Parameters\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.01\n",
    "    NUM_EPOCHS = 20\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 4e-4\n",
    "    NUM_CLASSES = 100\n",
    "\n",
    "\n",
    "# Create directories\n",
    "config = BaseConfig()\n",
    "for dir_path in [\n",
    "    config.DATA_DIR,\n",
    "    config.MODELS_DIR,\n",
    "    config.RESULTS_DIR,\n",
    "    config.CONFIGS_DIR,\n",
    "    config.RUNS_DIR,\n",
    "    config.OLD_RUNS_DIR,\n",
    "]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FederatedConfig(BaseConfig):\n",
    "    \"\"\"Federated Learning specific configuration.\"\"\"\n",
    "\n",
    "    NUM_CLIENTS: int = 100  # K\n",
    "    PARTICIPATION_RATE: float = 0.1  # C\n",
    "    LOCAL_EPOCHS: int = 4  # J\n",
    "    NUM_ROUNDS: int = 2000\n",
    "    CLASSES_PER_CLIENT: Optional[int] = None  # None for IID\n",
    "    PARTICIPATION_MODE: str = \"uniform\"\n",
    "    DIRICHLET_ALPHA: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.connected = nn.Sequential(\n",
    "            nn.Linear(5 * 5 * 64, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(192, config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.connected(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Logger\n",
    "\n",
    "This class is used to save the model training in a way to be analyzed using tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsManager:\n",
    "    \"\"\"Manages logging and visualization of training metrics.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BaseConfig,\n",
    "        model_name: str,\n",
    "        training_type: Literal[\"centralized\", \"federated\"],\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.config = config\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        # Archive old runs\n",
    "        old_runs = list(config.RUNS_DIR.glob(f\"{training_type}_{model_name}_*\"))\n",
    "        if old_runs:\n",
    "            archive_dir = config.OLD_RUNS_DIR\n",
    "            archive_dir.mkdir(exist_ok=True)\n",
    "            for run in old_runs:\n",
    "                run.rename(archive_dir / run.name)\n",
    "\n",
    "        # Create descriptive run name for FL experiments\n",
    "        if experiment_name:\n",
    "            experiment_suffix = f\"{experiment_name}_{timestamp}\"\n",
    "        elif training_type == \"federated\" and isinstance(config, FederatedConfig):\n",
    "            distribution = (\n",
    "                \"iid\"\n",
    "                if config.CLASSES_PER_CLIENT is None\n",
    "                else f\"noniid_{config.CLASSES_PER_CLIENT}cls\"\n",
    "            )\n",
    "            participation = f\"{config.PARTICIPATION_MODE}\"\n",
    "            if config.PARTICIPATION_MODE == \"skewed\":\n",
    "                participation += f\"_alpha{config.DIRICHLET_ALPHA}\"\n",
    "            clients_info = f\"C{config.NUM_CLIENTS}_P{config.PARTICIPATION_RATE}_E{config.LOCAL_EPOCHS}\"\n",
    "            experiment_suffix = (\n",
    "                f\"{distribution}_{participation}_{clients_info}_{timestamp}\"\n",
    "            )\n",
    "        else:\n",
    "            experiment_suffix = timestamp\n",
    "\n",
    "        run_name = f\"{training_type}_{model_name}_{experiment_suffix}\"\n",
    "        self.writer = SummaryWriter(config.RUNS_DIR / run_name)\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        split: Literal[\"train\", \"validation\", \"test\"],\n",
    "        loss: float,\n",
    "        accuracy: float,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Log metrics for specified split.\"\"\"\n",
    "        self.writer.add_scalars(\"metrics/loss\", {split: loss}, step)\n",
    "        self.writer.add_scalars(\"metrics/accuracy\", {split: accuracy}, step)\n",
    "\n",
    "    def log_fl_metrics(\n",
    "        self, round_idx: int, metrics: Dict, client_stats: Optional[Dict] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Log federated learning specific metrics.\"\"\"\n",
    "        # Log test metrics\n",
    "\n",
    "        # The get method return None if the key is not found,\n",
    "        # thus we can safely unpack and check if the logging is for the validation or test set\n",
    "        val_loss, val_accuracy, test_loss, test_accuracy = (\n",
    "            metrics.get(\"val_loss\"),\n",
    "            metrics.get(\"val_accuracy\"),\n",
    "            metrics.get(\"test_loss\"),\n",
    "            metrics.get(\"test_accuracy\"),\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            val_loss is None\n",
    "            and val_accuracy is None\n",
    "            and test_loss is None\n",
    "            and test_accuracy is None\n",
    "        ):\n",
    "            raise ValueError(\"No validation or test metrics provided.\")\n",
    "\n",
    "        if val_loss and val_accuracy and test_loss and test_accuracy:\n",
    "            raise ValueError(\n",
    "                \"Both validation and test metrics provided. Provide only one.\"\n",
    "            )\n",
    "\n",
    "        if val_loss and val_accuracy:\n",
    "            self.log_metrics(\"validation\", val_loss, val_accuracy, round_idx)\n",
    "\n",
    "        if test_loss and test_accuracy:\n",
    "            self.log_metrics(\"test\", test_loss, test_accuracy, round_idx)\n",
    "\n",
    "        # Log client participation if available\n",
    "        if client_stats:\n",
    "            self.writer.add_scalars(\n",
    "                \"federated/client_stats\",\n",
    "                client_stats,\n",
    "                round_idx,\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close TensorBoard writer.\"\"\"\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Manager\n",
    "\n",
    "Here the CIFAR100 is downloaded and the train, validation and test split are constructed to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar100DatasetManager:\n",
    "    config: BaseConfig\n",
    "    validation_split: float\n",
    "    train_transform: transforms.Compose\n",
    "    test_transform: transforms.Compose\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    val_loader: DataLoader[CIFAR100]\n",
    "    test_loader: DataLoader[CIFAR100]\n",
    "\n",
    "    def __init__(self, config: BaseConfig, validation_split: float = 0.1) -> None:\n",
    "        self.config = config\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_loader, self.val_loader, self.test_loader = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "    ) -> Tuple[DataLoader[CIFAR100], DataLoader[CIFAR100], DataLoader[CIFAR100]]:\n",
    "        full_trainset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "\n",
    "        train_size: int = int((1 - self.validation_split) * len(full_trainset))\n",
    "        val_size: int = len(full_trainset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_trainset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.config.SEED),\n",
    "        )\n",
    "\n",
    "        test_dataset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "\n",
    "        loader_kwargs = {\"num_workers\": self.config.NUM_WORKERS, \"pin_memory\": True}\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        test_loader: DataLoader[CIFAR100] = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.train_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.val_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.test_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Trainer\n",
    "\n",
    "This class is responsible to train and evaluate the model (here only considered for typing the LeNet model) in the traditional sense.\n",
    "Local training with normal train and evaluate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedTrainer:\n",
    "    model: LeNet\n",
    "    config: BaseConfig\n",
    "    device: torch.device\n",
    "    metrics: MetricsManager\n",
    "\n",
    "    def __init__(self, model: LeNet, config: BaseConfig) -> None:\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"centralized\"\n",
    "        )\n",
    "\n",
    "    def evaluate_model(\n",
    "        self, model: LeNet, data_loader: DataLoader[CIFAR100]\n",
    "    ) -> Tuple[float, float]:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_loss: float = total_loss / total\n",
    "        accuracy: float = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        max_epochs: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, T_max=self.config.NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "        if max_epochs is None:\n",
    "            max_epochs = self.config.NUM_EPOCHS\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        train_acc = 0.0\n",
    "        avg_train_loss = 0.0\n",
    "        device_type = str(self.device)\n",
    "\n",
    "        epoch_pbar = tqdm(\n",
    "            range(max_epochs or self.config.NUM_EPOCHS),\n",
    "            desc=\"Training\",\n",
    "            unit=\"epoch\",\n",
    "            position=0,\n",
    "            leave=True,\n",
    "        )\n",
    "        epoch = 0\n",
    "\n",
    "        try:\n",
    "            for epoch in epoch_pbar:\n",
    "                self.model.train()\n",
    "                train_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                    inputs = inputs.to(self.device, non_blocking=True)\n",
    "                    targets = targets.to(self.device, non_blocking=True)\n",
    "\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                    with torch.amp.autocast_mode.autocast(device_type=device_type):\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                    # Update metrics\n",
    "                    train_acc = 100.0 * correct / total\n",
    "                    avg_train_loss = train_loss / (batch_idx + 1)\n",
    "\n",
    "                    global_step = epoch * len(train_loader) + batch_idx\n",
    "                    self.metrics.log_metrics(\n",
    "                        \"train\", avg_train_loss, train_acc, global_step\n",
    "                    )\n",
    "\n",
    "                    del inputs, targets, outputs, loss\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Validation phase\n",
    "                val_loss, val_acc = self.evaluate_model(self.model, val_loader)\n",
    "                scheduler.step()\n",
    "\n",
    "                self.metrics.log_metrics(\"validation\", val_loss, val_acc, epoch)\n",
    "                epoch_pbar.set_postfix(\n",
    "                    {\n",
    "                        \"ep\": f\"{epoch+1}/{max_epochs or self.config.NUM_EPOCHS}\",\n",
    "                        \"tr_loss\": f\"{avg_train_loss:.3f}\",\n",
    "                        \"tr_acc\": f\"{train_acc:.1f}%\",\n",
    "                        \"val_loss\": f\"{val_loss:.3f}\",\n",
    "                        \"val_acc\": f\"{val_acc:.1f}%\",\n",
    "                    },\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                logging.info(\"Training completed!\")\n",
    "            # Final evaluation\n",
    "            if best_model_state is not None:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "            test_loss, test_acc = self.evaluate_model(self.model, test_loader)\n",
    "            self.metrics.log_metrics(\"test\", test_loss, test_acc, epoch)\n",
    "            logging.info(\n",
    "                f\"Final Test Results - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Class\n",
    "\n",
    "The core of this project.\n",
    "After setting a baseline using the CentralizedTrainer in the Cell above, the following class is responsible to generate, train, aggregate and evaluate the distributed model on client, with different settings:\n",
    "\n",
    "### Dataset modularity:\n",
    "* iid distribution of the classes for the clients\n",
    "* non_iid distribution\n",
    "* custom number of classes per clients\n",
    "\n",
    "### Clients Participation\n",
    "* Uniform selection of the clients\n",
    "* Dirichlet distribution to select the clients\n",
    "\n",
    "### Others\n",
    "This class also consider creating checkpoint after each generation if the global stats improves to resume training at a later time. This may be useful, for example, in the following cases:\n",
    "1. Federated Learning takes a really long times, thus if the real scenario the amount of client that can participate is too low, the training can be paused and resumed when the availability is increased.\n",
    "2. Academic reasons:\n",
    "   1. The code takes 50h to complete, pause it if you need your pc for something else, and then resume it when it is free to compute other generations.\n",
    "   2. Google Colab limit the amount of resources per day, when the GPU session end, wait until the resource returns available to resume with the latest best model found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        train_dataset: Dataset[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.global_model = model.to(config.DEVICE)\n",
    "        self.device = config.DEVICE\n",
    "        self.device_type = str(config.DEVICE)\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        # Pre-create all client shards and dataloaders\n",
    "        self.num_clients = config.NUM_CLIENTS\n",
    "        self.client_loaders = self._setup_client_data(train_dataset)\n",
    "\n",
    "        # Setup client selection\n",
    "        if config.PARTICIPATION_MODE == \"skewed\":\n",
    "            if config.DIRICHLET_ALPHA is None:\n",
    "                raise ValueError(\"dirichlet_alpha required for skewed mode\")\n",
    "            self.selection_probs = np.random.dirichlet(\n",
    "                [config.DIRICHLET_ALPHA] * config.NUM_CLIENTS\n",
    "            )\n",
    "        else:\n",
    "            self.selection_probs = np.ones(config.NUM_CLIENTS) / config.NUM_CLIENTS\n",
    "\n",
    "        self.metrics = MetricsManager(\n",
    "            config, model.__class__.__name__.lower(), \"federated\", experiment_name\n",
    "        )\n",
    "\n",
    "        # Pre-allocate client models\n",
    "        self.client_models = [copy.deepcopy(model) for _ in range(config.NUM_CLIENTS)]\n",
    "        self.checkpoint_dir = config.MODELS_DIR / \"federated\"\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.checkpoint_name = (\n",
    "            f\"{'iid' if config.CLASSES_PER_CLIENT is None else f'noniid_{config.CLASSES_PER_CLIENT}cls'}\"\n",
    "            f\"_{config.PARTICIPATION_MODE}\"\n",
    "            f\"_C{config.NUM_CLIENTS}_P{config.PARTICIPATION_RATE}_E{config.LOCAL_EPOCHS}.pt\"\n",
    "        )\n",
    "        self.checkpoint_path = self.checkpoint_dir / self.checkpoint_name\n",
    "\n",
    "    def _setup_client_data(\n",
    "        self, dataset: Dataset[CIFAR100]\n",
    "    ) -> List[DataLoader[CIFAR100]]:\n",
    "        shards: List[Subset[CIFAR100]] = (\n",
    "            self._create_iid_shards(dataset)\n",
    "            if self.config.CLASSES_PER_CLIENT is None\n",
    "            else self._create_noniid_shards(dataset)\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            DataLoader(\n",
    "                shard,\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=self.config.NUM_WORKERS,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                prefetch_factor=3,\n",
    "                drop_last=True,\n",
    "            )\n",
    "            for shard in shards\n",
    "        ]\n",
    "\n",
    "    def _create_iid_shards(self, dataset: Dataset[CIFAR100]) -> List[Subset[CIFAR100]]:\n",
    "        \"\"\"Create IID data shards.\"\"\"\n",
    "        if len(dataset) == 0:\n",
    "            raise ValueError(\"Empty dataset\")\n",
    "\n",
    "        indices = np.random.permutation(len(dataset))\n",
    "        shard_size = len(dataset) // self.num_clients\n",
    "\n",
    "        return [\n",
    "            Subset(dataset, indices[i : i + shard_size])\n",
    "            for i in range(0, len(indices), shard_size)\n",
    "        ]\n",
    "\n",
    "    def _create_noniid_shards(\n",
    "        self, dataset: Dataset[CIFAR100]\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        \"\"\"Create non-IID data shards using class distribution.\"\"\"\n",
    "        if not hasattr(dataset, \"targets\"):\n",
    "            raise ValueError(\n",
    "                \"Dataset must have 'targets' attribute for non-IID sharding\"\n",
    "            )\n",
    "\n",
    "        targets = np.array(dataset.targets)\n",
    "        class_indices = {\n",
    "            label: np.where(targets == label)[0]\n",
    "            for label in range(len(dataset.classes))\n",
    "        }\n",
    "\n",
    "        client_indices = []\n",
    "        for i in range(self.num_clients):\n",
    "            indices = []\n",
    "            # Select random classes for this client\n",
    "            selected_classes = np.random.choice(\n",
    "                list(class_indices.keys()),\n",
    "                size=min(5, len(class_indices)),  # Default to 5 classes per client\n",
    "                replace=False,\n",
    "            )\n",
    "\n",
    "            # Add samples from each selected class\n",
    "            for class_label in selected_classes:\n",
    "                class_samples = np.random.choice(\n",
    "                    class_indices[class_label],\n",
    "                    size=len(class_indices[class_label]) // self.num_clients,\n",
    "                    replace=False,\n",
    "                )\n",
    "                indices.extend(class_samples)\n",
    "\n",
    "            client_indices.append(Subset(dataset, indices))\n",
    "\n",
    "        return client_indices\n",
    "\n",
    "    def _evaluate(self, loader: DataLoader[CIFAR100]) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate model on given data loader.\n",
    "        # Returns\n",
    "            Tuple of (loss, accuracy)\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with (\n",
    "            torch.no_grad(),\n",
    "            torch.amp.autocast_mode.autocast(device_type=self.device_type),\n",
    "        ):\n",
    "            for inputs, targets in loader:\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                targets = targets.to(self.device, non_blocking=True)\n",
    "\n",
    "                outputs = self.global_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "    def _aggregate_models(self, selected_clients: List[int] | npt.NDArray) -> None:\n",
    "        \"\"\"Aggregate models using weighted average based on dataset sizes.\"\"\"\n",
    "        with (\n",
    "            torch.no_grad(),\n",
    "            torch.amp.autocast_mode.autocast(device_type=self.device_type),\n",
    "        ):\n",
    "            # Calculate total samples across selected clients\n",
    "            total_samples = sum(\n",
    "                len(self.client_loaders[idx].dataset) for idx in selected_clients\n",
    "            )\n",
    "\n",
    "            # Initialize aggregated parameters\n",
    "            for k, v in self.global_model.state_dict().items():\n",
    "                weighted_sum = torch.zeros_like(v)\n",
    "                for idx in selected_clients:\n",
    "                    # Get client's weight based on dataset size\n",
    "                    client_weight = (\n",
    "                        len(self.client_loaders[idx].dataset) / total_samples\n",
    "                    )\n",
    "                    client_params = (\n",
    "                        self.client_models[idx].state_dict()[k].to(self.device)\n",
    "                    )\n",
    "                    weighted_sum.add_(client_params * client_weight)\n",
    "\n",
    "                # Update global model\n",
    "                v.copy_(weighted_sum)\n",
    "\n",
    "    def save_checkpoint(self, round_idx: int, best_val_loss: float) -> None:\n",
    "        \"\"\"Save model checkpoint with training state.\"\"\"\n",
    "        checkpoint = {\n",
    "            \"round\": round_idx,\n",
    "            \"model_state_dict\": self.global_model.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"config\": self.config,\n",
    "        }\n",
    "        torch.save(checkpoint, self.checkpoint_path)\n",
    "        logging.info(f\"Checkpoint saved: {self.checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self) -> Tuple[int, float]:\n",
    "        \"\"\"Load model checkpoint and return training state.\"\"\"\n",
    "        if not self.checkpoint_path.exists():\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        checkpoint = torch.load(self.checkpoint_path)\n",
    "\n",
    "        # Verify config matches\n",
    "        saved_config = checkpoint[\"config\"]\n",
    "        if (\n",
    "            saved_config.NUM_CLIENTS != self.config.NUM_CLIENTS\n",
    "            or saved_config.CLASSES_PER_CLIENT != self.config.CLASSES_PER_CLIENT\n",
    "            or saved_config.PARTICIPATION_MODE != self.config.PARTICIPATION_MODE\n",
    "        ):\n",
    "            logging.warning(\"Config mismatch in checkpoint, starting fresh training\")\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        self.global_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        logging.info(f\"Resumed from checkpoint: {self.checkpoint_path}\")\n",
    "        return checkpoint[\"round\"], checkpoint[\"best_val_loss\"]\n",
    "\n",
    "    def train_client(self, client_idx: int, model: LeNet) -> None:\n",
    "        \"\"\"Train a single client in-place.\"\"\"\n",
    "        model.train()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "            nesterov=True,\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = torch.amp.grad_scaler.GradScaler(device=self.device_type)\n",
    "\n",
    "        for _ in range(self.config.LOCAL_EPOCHS):\n",
    "            for inputs, targets in self.client_loaders[client_idx]:\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                targets = targets.to(self.device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast_mode.autocast(device_type=self.device_type):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                del inputs, targets, outputs, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self) -> None:\n",
    "        # Load existing checkpoint if available\n",
    "        start_round, best_val_loss = self.load_checkpoint()\n",
    "        best_model_state = (\n",
    "            self.global_model.state_dict().copy() if start_round > 0 else None\n",
    "        )\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        if start_round > 0:\n",
    "            logging.info(f\"Resuming training from round {start_round}\")\n",
    "\n",
    "        round_pbar = tqdm(\n",
    "            range(start_round, self.config.NUM_ROUNDS),\n",
    "            desc=\"Training\",\n",
    "            unit=\"round\",\n",
    "            initial=start_round,\n",
    "            colour=\"green\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            for round_idx in round_pbar:\n",
    "                # Select clients\n",
    "                num_selected = max(\n",
    "                    1, int(self.config.PARTICIPATION_RATE * self.config.NUM_CLIENTS)\n",
    "                )\n",
    "                selected_clients = np.random.choice(\n",
    "                    self.config.NUM_CLIENTS,\n",
    "                    size=num_selected,\n",
    "                    replace=False,\n",
    "                    p=self.selection_probs,\n",
    "                )\n",
    "\n",
    "                # Train selected clients in parallel\n",
    "                for idx in selected_clients:\n",
    "                    self.client_models[idx].load_state_dict(\n",
    "                        self.global_model.state_dict()\n",
    "                    )\n",
    "                    self.train_client(idx, self.client_models[idx])\n",
    "\n",
    "                # Aggregate models\n",
    "                self._aggregate_models(selected_clients)\n",
    "\n",
    "                # Evaluate\n",
    "                val_loss, val_acc = self._evaluate(self.val_loader)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = self.global_model.state_dict().copy()\n",
    "                    self.save_checkpoint(round_idx, best_val_loss)\n",
    "\n",
    "                round_pbar.set_postfix(\n",
    "                    {\n",
    "                        \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                        \"val_acc\": f\"{val_acc:.2f}%\",\n",
    "                        \"best_val_loss\": f\"{best_val_loss:.4f}\",\n",
    "                        \"best_val_acc\": f\"{best_val_acc:.2f}%\",\n",
    "                    },\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "                self.metrics.log_fl_metrics(\n",
    "                    round_idx,\n",
    "                    {\"val_loss\": val_loss, \"val_accuracy\": val_acc},\n",
    "                    {\"num_selected\": len(selected_clients)},\n",
    "                )\n",
    "\n",
    "            # Final evaluation\n",
    "            if best_model_state:\n",
    "                self.global_model.load_state_dict(best_model_state)\n",
    "            test_loss, test_acc = self._evaluate(self.test_loader)\n",
    "            self.metrics.log_metrics(\n",
    "                \"test\", test_loss, test_acc, self.config.NUM_ROUNDS\n",
    "            )\n",
    "\n",
    "            self.save_checkpoint(self.config.NUM_ROUNDS, best_val_loss)\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = Cifar100DatasetManager(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized - Baseline\n",
    "\n",
    "The model is able to achieve the following result:\n",
    "* Test Loss: 1.9331\n",
    "* Test Accuracy: 49.29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LeNet(config)\n",
    "# trainer = CentralizedTrainer(model, config)\n",
    "# trainer.train(data.train_loader, data.val_loader, data.test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|\u001b[32m          \u001b[0m| 1/2000 [00:44<24:53:24, 44.82s/round, val_loss=4.6015, val_acc=1.90%, best_val_loss=4.6015, best_val_acc=1.90%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K2025-01-18 16:59:26 [\u001b[1;32mINFO\u001b[0m] Checkpoint saved: c:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\models\\federated\\iid_uniform_C100_P0.1_E4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|\u001b[32m          \u001b[0m| 2/2000 [01:24<23:10:57, 41.77s/round, val_loss=4.5917, val_acc=2.32%, best_val_loss=4.5917, best_val_acc=2.32%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K2025-01-18 17:00:05 [\u001b[1;32mINFO\u001b[0m] Checkpoint saved: c:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\models\\federated\\iid_uniform_C100_P0.1_E4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|\u001b[32m          \u001b[0m| 3/2000 [02:07<23:29:16, 42.34s/round, val_loss=4.5596, val_acc=1.84%, best_val_loss=4.5596, best_val_acc=1.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K2025-01-18 17:00:48 [\u001b[1;32mINFO\u001b[0m] Checkpoint saved: c:\\Users\\nick\\Documents\\GitHub\\AdvanceML_project5\\models\\federated\\iid_uniform_C100_P0.1_E4.pt\n"
     ]
    }
   ],
   "source": [
    "# IID baseline (K=100, C=0.1, J=4)\n",
    "iid_config = FederatedConfig(\n",
    "    NUM_CLIENTS=100,\n",
    "    PARTICIPATION_RATE=0.1,\n",
    "    LOCAL_EPOCHS=4,\n",
    "    NUM_ROUNDS=2000,\n",
    "    CLASSES_PER_CLIENT=None,  # IID\n",
    ")\n",
    "\n",
    "# Non-IID with Nc=1\n",
    "noniid_config = dataclasses.replace(\n",
    "    iid_config,\n",
    "    CLASSES_PER_CLIENT=1,  # Most heterogeneous case\n",
    ")\n",
    "\n",
    "# Run experiments\n",
    "for config, name in [(iid_config, \"iid\"), (noniid_config, \"noniid_1class\")]:\n",
    "    model = LeNet(config)\n",
    "    trainer = FederatedTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data.train_dataset,\n",
    "        val_loader=data.val_loader,\n",
    "        test_loader=data.test_loader,\n",
    "        config=config,\n",
    "        experiment_name=name,\n",
    "    )\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
