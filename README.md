# ğŸš€ Project Overview

This project explores **Federated Learning (FL)**, a decentralized learning paradigm where multiple clients collaboratively train a shared global model **without sharing their local data**. The primary focus is to evaluate the performance of **FedAvg** (Federated Averaging) under different FL scenarios and compare it with centralized learning.

## ğŸ“‚ Datasets Used
The experiments are conducted on two datasets:

1. ğŸ“œ **Shakespeare dataset** â€“ Character-level language modeling, representing a realistic **non-IID** FL setting where clients correspond to different Shakespearean characters.
2. ğŸ–¼ï¸ **CIFAR-100 dataset** â€“ Image classification task, exploring the impact of **data heterogeneity, client selection strategies, and local update frequencies**.

The project follows **Track B: Federated Learning** and is inspired by the methodologies described in **McMahan et al. (2017) and Reddi et al. (2021)**.

---

# ğŸ¯ Motivation and Background

## ğŸ¤– Why Federated Learning?
Traditional deep learning models require **centralized data aggregation**, which poses challenges related to **privacy, data ownership, and communication efficiency**. Federated Learning mitigates these concerns by enabling clients (e.g., mobile devices, edge nodes) to **train models locally and share only model updates** with a central server.

## âš ï¸ Challenges in Federated Learning
- ğŸ“Š **Data Heterogeneity (Non-IID)** â€“ Clients may have different data distributions, making global model convergence difficult.
- ğŸƒ **Client Participation Variability** â€“ Not all clients participate equally, leading to imbalanced updates.
- ğŸŒ **Communication Constraints** â€“ Frequent model synchronization can be expensive.

This project investigates these aspects by implementing and analyzing **different FL settings and configurations**.

---

## ğŸ“Š Dataset and Preprocessing

### ğŸ–¼ï¸ CIFAR-100 Dataset
- **Task:** Image classification across 100 classes.
- **Clients:** Each client is assigned a subset of the dataset.
- **Challenges:**
    - **Non-IID settings** where clients receive only a fraction of the 100 classes.
    - **Class imbalance** across different clients.

### ğŸ“œ Shakespeare Dataset (LEAF Benchmark)
- **Task:** Character-level next-character prediction.
- **Clients:** Each client represents a different Shakespearean character.
- **Challenges:**
    - **Non-IID data distribution** due to unique writing styles.
    - **Imbalanced data** â€“ some characters have more text than others.

---

## âš™ï¸ Implementation Details
The project is based on **FedAvg (Federated Averaging)**, a widely used optimization algorithm for FL. The key steps include:

### ğŸ” 1. Centralized Learning Baseline
- A **single global model** is trained using all available data in a traditional supervised learning manner.
- This serves as the **upper bound** for performance comparison.

### ğŸ¤ 2. Federated Learning with FedAvg
- Each client trains a **local model** on its own data.
- A **central server aggregates updates** and improves the global model.
- The FL process consists of:
    1. **Client Selection** ğŸ¯: A subset of clients (C%) is chosen per round.
    2. **Local Training** ğŸ‹ï¸: Each selected client updates the model locally for J steps using Stochastic Gradient Descent (SGD).
    3. **Global Aggregation** ğŸŒ: The central server averages model updates.

---

## ğŸ§ª Experiments
This project systematically evaluates different federated learning configurations:

### ğŸ‘¥ 1. Effect of Client Participation
- **Uniform Participation**: Every client has an equal probability of being selected at each round.
- **Skewed Participation**: Clients participate based on a probability distribution (e.g., Dirichlet distribution with hyperparameter Î³), simulating real-world variability.

### ğŸ”€ 2. Data Distribution (IID vs Non-IID)
- **IID Setting**: Each client receives randomly sampled data from all classes.
- **Non-IID Setting**: Clients receive data from a limited number of classes (Nc), where Nc = {1, 5, 10, 50}.

### ğŸ”§ 3. Impact of Local Training Steps (J)
- J = {4, 8, 16} represents how many local updates clients perform before synchronization.
- **Larger J reduces communication costs but may cause local models to drift away from the global optimum.**

ğŸ“Œ **Scaling Rules**: When J doubles, the **number of FL rounds is halved** to balance computation and communication.

---

## ğŸ† Contribution: Federated Learning with Two-Phase Training

### ğŸŒŸ Overview
The implementation extends standard **Federated Averaging (FedAvg)** by introducing an **innovative two-phase training approach**. This modification enhances model performance through intermediate model shuffling between clients.

### ğŸ”„ Two-Phase Innovation
Each federation round is split into two distinct training phases:

1. **Phase 1 (Initial Training) ğŸ‹ï¸**
   - Selected clients receive the global model.
   - Each client trains for J/2 epochs.
   - Models capture initial client-specific features.

2. **Intermediate Shuffling ğŸ”„**
   - Client models are randomly redistributed.
   - Each client receives a different model.
   - Ensures knowledge sharing across clients.

3. **Phase 2 (Extended Training) ğŸ”¥**
   - Clients train received models for J/2 epochs.
   - Models benefit from different data distributions.

4. **Final Aggregation ğŸ“Š**
   - Twice-trained models are collected.
   - **FedAvg is applied with dataset size weighting**.
   - Results update the global model.

### ğŸ“ˆ Benefits
1. **Enhanced Knowledge Sharing** ğŸŒ â€“ Models exposed to multiple data distributions for better feature generalization.
2. **Improved Non-IID Handling** ğŸ“Š â€“ Significant gains in non-IID scenarios (+3.08% with 50 classes).
3. **Flexible Configuration** ğŸ”§ â€“ Adjustable **local epochs (J parameter)** and compatible with various **client selection strategies**.

---

## ğŸ› ï¸ Installation

> âš ï¸ **Before installing, ensure PyTorch is using a compatible CUDA version for your machine.**

This project uses **Poetry** to manage the environment.

### ğŸ” Check CUDA Compatibility
A friendly script is available to update the `pyproject.toml` file to best fit your machine hardware capabilities.

```bash
python detect_cuda.py
```

For more details, run:
```bash
python detect_cuda.py -h
```

After running the script, install the environment with:
```bash
poetry install
```

### ğŸ› ï¸ Manual Installation
If the script fails, manually set up PyTorch based on your CUDA capabilities by referring to the official [PyTorch installation page](https://pytorch.org/get-started/locally/). Alternatively, modify the `pyproject.toml` file:

âœ… **With CUDA support:**
```toml
[tool.poetry.dependencies]
python = "^3.12"
torch = { version = "^2.5.1", source = "pytorch" }
torchvision = { version = "^0.20.1", source = "pytorch" }
```

âŒ **Without CUDA (CPU version):**
```toml
[tool.poetry.dependencies]
python = "^3.12"
torch = "^2.5.1"
torchvision = "^0.20.1"
```

> ğŸ“ **Tip:** Removing `source="pytorch"` achieves the same result!

---

ğŸ‰ **Now you're all set to run your Federated Learning experiments!** ğŸš€

