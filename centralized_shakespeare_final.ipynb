{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries for dataset management, model building, training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from json import JSONEncoder\n",
    "import random\n",
    "import kagglehub\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm  # For progress tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded path: C:\\Users\\rosif\\.cache\\kagglehub\\datasets\\kewagbln\\shakespeareonline\\versions\\1\n",
      "Files in downloaded path:\n",
      " - C:\\Users\\rosif\\.cache\\kagglehub\\datasets\\kewagbln\\shakespeareonline\\versions\\1\\t8.shakespeare.txt\n",
      "Dataset saved to: c:\\Users\\rosif\\OneDrive\\Desktop\\Advance Machine Learning\\Project2024\\AdvanceML_project5\\shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "# Regular expressions for parsing Shakespeare text\n",
    "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
    "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
    "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
    "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
    "\n",
    "\n",
    "# Get current script directory\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
    "\n",
    "# Debug: print downloaded files\n",
    "print(f\"Downloaded path: {path}\")\n",
    "print(\"Files in downloaded path:\")\n",
    "for file in glob.glob(os.path.join(path, \"*\")):\n",
    "    print(f\" - {file}\")\n",
    "\n",
    "# Set up paths relative to script location\n",
    "DATA_PATH = os.path.join(SCRIPT_DIR, \"shakespeare.txt\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"processed_data\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Find and copy Shakespeare text file\n",
    "shakespeare_file = None\n",
    "for file in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "    shakespeare_file = file\n",
    "    break\n",
    "\n",
    "if shakespeare_file:\n",
    "    shutil.copy2(shakespeare_file, DATA_PATH)\n",
    "    print(f\"Dataset saved to: {DATA_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find Shakespeare text file in {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __txt_to_data(txt_dir, seq_length=80):\n",
    "    \"\"\"Parses text file in given directory into data for next-character model.\n",
    "\n",
    "    Args:\n",
    "        txt_dir: path to text file\n",
    "        seq_length: length of strings in X\n",
    "    \"\"\"\n",
    "    raw_text = \"\"\n",
    "    with open(txt_dir,'r') as inf:\n",
    "        raw_text = inf.read()\n",
    "    raw_text = raw_text.replace('\\n', ' ')\n",
    "    raw_text = re.sub(r\"   *\", r' ', raw_text)\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(raw_text) - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "    return dataX, dataY\n",
    "\n",
    "def parse_data_in(data_dir, users_and_plays_path, raw=False):\n",
    "    '''\n",
    "    returns dictionary with keys: users, num_samples, user_data\n",
    "    raw := bool representing whether to include raw text in all_data\n",
    "    if raw is True, then user_data key\n",
    "    removes users with no data\n",
    "    '''\n",
    "    with open(users_and_plays_path, 'r') as inf:\n",
    "        users_and_plays = json.load(inf)\n",
    "    files = os.listdir(data_dir)\n",
    "    users = []\n",
    "    hierarchies = []\n",
    "    num_samples = []\n",
    "    user_data = {}\n",
    "    for f in files:\n",
    "        user = f[:-4]\n",
    "        passage = ''\n",
    "        filename = os.path.join(data_dir, f)\n",
    "        with open(filename, 'r') as inf:\n",
    "            passage = inf.read()\n",
    "        dataX, dataY = __txt_to_data(filename)\n",
    "        if(len(dataX) > 0):\n",
    "            users.append(user)\n",
    "            if raw:\n",
    "                user_data[user] = {'raw': passage}\n",
    "            else:\n",
    "                user_data[user] = {}\n",
    "            user_data[user]['x'] = dataX\n",
    "            user_data[user]['y'] = dataY\n",
    "            hierarchies.append(users_and_plays[user])\n",
    "            num_samples.append(len(dataY))\n",
    "    all_data = {}\n",
    "    all_data['users'] = users\n",
    "    all_data['hierarchies'] = hierarchies\n",
    "    all_data['num_samples'] = num_samples\n",
    "    all_data['user_data'] = user_data\n",
    "    return all_data\n",
    "\n",
    "def parse_shakespeare(filepath, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Parses Shakespeare's text into training and testing datasets.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        raw_text = file.read()\n",
    "\n",
    "    plays_data, _ = process_plays(raw_text)\n",
    "    _, training_set, testing_set = split_train_test_data(plays_data, 1.0 - train_split)\n",
    "\n",
    "    total_train = sum(len(lines) for lines in training_set.values())\n",
    "    total_test = sum(len(lines) for lines in testing_set.values())\n",
    "    print(f\"Training examples: {total_train}\")\n",
    "    print(f\"Testing examples: {total_test}\")\n",
    "    \n",
    "    assert total_train > total_test, \"Training set should be larger than test set\"\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "def process_plays(shakespeare_full):\n",
    "    \"\"\"\n",
    "    Processes the Shakespeare text into individual plays and characters' dialogues.\n",
    "    Handles special cases for \"The Comedy of Errors\".\n",
    "    \"\"\"\n",
    "    plays = []\n",
    "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
    "    current_character = None\n",
    "    comedy_of_errors = False\n",
    "\n",
    "    for i, line in enumerate(slines):\n",
    "        # Detect play titles and initialize character dictionary\n",
    "        if \"by William Shakespeare\" in line:\n",
    "            current_character = None\n",
    "            characters = defaultdict(list)\n",
    "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
    "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
    "            plays.append((title, characters))\n",
    "            continue\n",
    "\n",
    "        # Match character lines or continuation lines\n",
    "        match = _match_character_regex(line, comedy_of_errors)\n",
    "        if match:\n",
    "            character, snippet = match.group(1).upper(), match.group(2)\n",
    "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
    "                characters[character].append(snippet)\n",
    "                current_character = character\n",
    "        elif current_character:\n",
    "            match = _match_continuation_regex(line, comedy_of_errors)\n",
    "            if match:\n",
    "                characters[current_character].append(match.group(1))\n",
    "\n",
    "    # Filter out plays with insufficient dialogue data\n",
    "    return [play for play in plays if len(play[1]) > 1], []\n",
    "\n",
    "def _match_character_regex(line, comedy_of_errors=False):\n",
    "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
    "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
    "\n",
    "def _match_continuation_regex(line, comedy_of_errors=False):\n",
    "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
    "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
    "\n",
    "def extract_play_title(lines, index):\n",
    "    \"\"\"\n",
    "    Extracts the title of the play from the lines of the text.\n",
    "    \"\"\"\n",
    "    for i in range(index - 1, -1, -1):\n",
    "        if lines[i].strip():\n",
    "            return lines[i].strip()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def detect_character_line(line, comedy_of_errors):\n",
    "    \"\"\"\n",
    "    Matches a line of character dialogue.\n",
    "    \"\"\"\n",
    "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
    "\n",
    "def detect_continuation_line(line, comedy_of_errors):\n",
    "    \"\"\"\n",
    "    Matches a continuation line of dialogue.\n",
    "    \"\"\"\n",
    "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
    "\n",
    "def _split_into_plays(shakespeare_full):\n",
    "    \"\"\"Splits the full data by play.\"\"\"\n",
    "    # List of tuples (play_name, dict from character to list of lines)\n",
    "    plays = []\n",
    "    discarded_lines = []  # Track discarded lines.\n",
    "    slines = shakespeare_full.splitlines(True)[1:]\n",
    "\n",
    "    # skip contents, the sonnets, and all's well that ends well\n",
    "    author_count = 0\n",
    "    start_i = 0\n",
    "    for i, l in enumerate(slines):\n",
    "        if 'by William Shakespeare' in l:\n",
    "            author_count += 1\n",
    "        if author_count == 2:\n",
    "            start_i = i - 5\n",
    "            break\n",
    "    slines = slines[start_i:]\n",
    "\n",
    "    current_character = None\n",
    "    comedy_of_errors = False\n",
    "    for i, line in enumerate(slines):\n",
    "        # This marks the end of the plays in the file.\n",
    "        if i > 124195 - start_i:\n",
    "            break\n",
    "        # This is a pretty good heuristic for detecting the start of a new play:\n",
    "        if 'by William Shakespeare' in line:\n",
    "            current_character = None\n",
    "            characters = collections.defaultdict(list)\n",
    "            # The title will be 2, 3, 4, 5, 6, or 7 lines above \"by William Shakespeare\".\n",
    "            if slines[i - 2].strip():\n",
    "                title = slines[i - 2]\n",
    "            elif slines[i - 3].strip():\n",
    "                title = slines[i - 3]\n",
    "            elif slines[i - 4].strip():\n",
    "                title = slines[i - 4]\n",
    "            elif slines[i - 5].strip():\n",
    "                title = slines[i - 5]\n",
    "            elif slines[i - 6].strip():\n",
    "                title = slines[i - 6]\n",
    "            else:\n",
    "                title = slines[i - 7]\n",
    "            title = title.strip()\n",
    "\n",
    "            assert title, (\n",
    "                'Parsing error on line %d. Expecting title 2 or 3 lines above.' %\n",
    "                i)\n",
    "            comedy_of_errors = (title == 'THE COMEDY OF ERRORS')\n",
    "            # Degenerate plays are removed at the end of the method.\n",
    "            plays.append((title, characters))\n",
    "            continue\n",
    "        match = _match_character_regex(line, comedy_of_errors)\n",
    "        if match:\n",
    "            character, snippet = match.group(1), match.group(2)\n",
    "            # Some character names are written with multiple casings, e.g., SIR_Toby\n",
    "            # and SIR_TOBY. To normalize the character names, we uppercase each name.\n",
    "            # Note that this was not done in the original preprocessing and is a\n",
    "            # recent fix.\n",
    "            character = character.upper()\n",
    "            if not (comedy_of_errors and character.startswith('ACT ')):\n",
    "                characters[character].append(snippet)\n",
    "                current_character = character\n",
    "                continue\n",
    "            else:\n",
    "                current_character = None\n",
    "                continue\n",
    "        elif current_character:\n",
    "            match = _match_continuation_regex(line, comedy_of_errors)\n",
    "            if match:\n",
    "                if comedy_of_errors and match.group(1).startswith('<'):\n",
    "                    current_character = None\n",
    "                    continue\n",
    "                else:\n",
    "                    characters[current_character].append(match.group(1))\n",
    "                    continue\n",
    "        # Didn't consume the line.\n",
    "        line = line.strip()\n",
    "        if line and i > 2646:\n",
    "            # Before 2646 are the sonnets, which we expect to discard.\n",
    "            discarded_lines.append('%d:%s' % (i, line))\n",
    "    # Remove degenerate \"plays\".\n",
    "    return [play for play in plays if len(play[1]) > 1], discarded_lines\n",
    "\n",
    "\n",
    "def _remove_nonalphanumerics(filename):\n",
    "    return re.sub('\\\\W+', '_', filename)\n",
    "\n",
    "def play_and_character(play, character):\n",
    "    return _remove_nonalphanumerics((play + '_' + character).replace(' ', '_'))\n",
    "\n",
    "def split_train_test_data(plays, test_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Splits the plays into training and testing datasets by character dialogues.\n",
    "    \"\"\"\n",
    "    skipped_characters = 0\n",
    "    all_train_examples = collections.defaultdict(list)\n",
    "    all_test_examples = collections.defaultdict(list)\n",
    "\n",
    "    def add_examples(example_dict, example_tuple_list):\n",
    "        for play, character, sound_bite in example_tuple_list:\n",
    "            example_dict[play_and_character(\n",
    "                play, character)].append(sound_bite)\n",
    "\n",
    "    users_and_plays = {}\n",
    "    for play, characters in plays:\n",
    "        curr_characters = list(characters.keys())\n",
    "        for c in curr_characters:\n",
    "            users_and_plays[play_and_character(play, c)] = play\n",
    "        for character, sound_bites in characters.items():\n",
    "            examples = [(play, character, sound_bite)\n",
    "                        for sound_bite in sound_bites]\n",
    "            if len(examples) <= 2:\n",
    "                skipped_characters += 1\n",
    "                # Skip characters with fewer than 2 lines since we need at least one\n",
    "                # train and one test line.\n",
    "                continue\n",
    "            train_examples = examples\n",
    "            if test_fraction > 0:\n",
    "                num_test = max(int(len(examples) * test_fraction), 1)\n",
    "                train_examples = examples[:-num_test]\n",
    "                test_examples = examples[-num_test:]\n",
    "                \n",
    "                assert len(test_examples) == num_test\n",
    "                assert len(train_examples) >= len(test_examples)\n",
    "\n",
    "                add_examples(all_test_examples, test_examples)\n",
    "                add_examples(all_train_examples, train_examples)\n",
    "\n",
    "    return users_and_plays, all_train_examples, all_test_examples\n",
    "\n",
    "\n",
    "def _write_data_by_character(examples, output_directory):\n",
    "    \"\"\"Writes a collection of data files by play & character.\"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    for character_name, sound_bites in examples.items():\n",
    "        filename = os.path.join(output_directory, character_name + '.txt')\n",
    "        with open(filename, 'w') as output:\n",
    "            for sound_bite in sound_bites:\n",
    "                output.write(sound_bite + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_vec(c, n_vocab=90):\n",
    "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
    "    return ord(c) % n_vocab\n",
    "\n",
    "def word_to_indices(word, n_vocab=90):\n",
    "    \"\"\"\n",
    "    Converts a word or list of words into a list of indices.\n",
    "    Each character is mapped to an index based on the vocabulary size.\n",
    "    \"\"\"\n",
    "    if isinstance(word, list):  # If input is a list of words\n",
    "        res = []\n",
    "        for stringa in word:\n",
    "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
    "        return res\n",
    "    else:  # If input is a single word\n",
    "        return [ord(c) % n_vocab for c in word]\n",
    "\n",
    "def process_x(raw_x_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw input data into padded sequences of indices.\n",
    "    Ensures all sequences are of uniform length.\n",
    "    \"\"\"\n",
    "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
    "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
    "    return torch.tensor(x_batch, dtype=torch.long)\n",
    "\n",
    "def process_y(raw_y_batch, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Processes raw target data into padded sequences of indices.\n",
    "    Shifts the sequence by one character to the right.\n",
    "    y[1:seq_len + 1] takes the input data, right shift of an\n",
    "    element and uses the next element of the sequence to fill\n",
    "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
    "    added to reach the desired sequence length.\n",
    "    \"\"\"\n",
    "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
    "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
    "    return torch.tensor(y_batch, dtype=torch.long)\n",
    "\n",
    "def create_batches(data, batch_size, seq_len, n_vocab):\n",
    "    \"\"\"\n",
    "    Creates batches of input and target data from dialogues.\n",
    "    Each batch contains sequences of uniform length.\n",
    "    \"\"\"\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    dialogues = list(data.values())\n",
    "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
    "\n",
    "    batch = []\n",
    "    for dialogue in dialogues:\n",
    "        batch.append(dialogue)\n",
    "        if len(batch) == batch_size:\n",
    "            x_batch = process_x(batch, seq_len, n_vocab)\n",
    "            y_batch = process_y(batch, seq_len, n_vocab)\n",
    "            x_batches.append(x_batch)\n",
    "            y_batches.append(y_batch)\n",
    "            batch = []\n",
    "\n",
    "    # Add the last batch if it's not full\n",
    "    if batch:\n",
    "        x_batch = process_x(batch, seq_len, n_vocab)\n",
    "        y_batch = process_y(batch, seq_len, n_vocab)\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    return x_batches, y_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_centralized(model, optimizer, subfolder, epoch, lr, wd, results):\n",
    "            \"\"\"Salva il risultato del modello e rimuove quello precedente.\"\"\"\n",
    "            subfolder_path = os.path.join(OUTPUT_DIR, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "            # File corrente e precedente\n",
    "            filename = f\"model_epoch_{epoch}_params_LR{lr}_WD{wd}.pth\"\n",
    "            filepath = os.path.join(subfolder_path, filename)\n",
    "            filename_json = f\"model_epoch_{epoch}_params_LR{lr}_WD{wd}.json\"\n",
    "            filepath_json = os.path.join(subfolder_path, filename_json)\n",
    "\n",
    "\n",
    "            previous_filename = f\"model_epoch_{epoch -1}_params_LR{lr}_WD{wd}.pth\"\n",
    "            previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
    "            previous_filename_json = f\"model_epoch_{epoch -1}_params_LR{lr}_WD{wd}.json\"\n",
    "            previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
    "\n",
    "            # Rimuove il checkpoint precedente\n",
    "            if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
    "                os.remove(previous_filepath)\n",
    "                os.remove(previous_filepath_json)\n",
    "\n",
    "            # Salva il nuovo checkpoint\n",
    "            if optimizer is not None:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
    "                    'epoch': epoch\n",
    "                }, filepath)\n",
    "            else:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'epoch': epoch\n",
    "                }, filepath)\n",
    "\n",
    "            \n",
    "            with open(filepath_json, 'w') as json_file:\n",
    "                json.dump(results, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_centralized(validation_losses, validation_accuracies, lr, wd):\n",
    "    # Plot centralized validation performance\n",
    "    plt.figure(figsize=(12,10))\n",
    "    # Plot Validation Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(validation_losses, label=f\"lr{lr}-wd{wd}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Loss Across Learning Rates and Weight Decays\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Validation Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(validation_accuracies, label=f\"lr{lr}-wd{wd}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Validation Accuracy Across Learning Rates and Weight Decays\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Test Loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(validation_losses, label=f\"lr{lr}-wd{wd}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Test Loss Across Learning Rates and Weight Decays\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Plot Validation Accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(validation_accuracies, label=f\"lr{lr}-wd{wd}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Test Accuracy Across Learning Rates and Weight Decays\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f\"processed_data/Centralized_lr{lr}_wd{wd}/val_test_loss_accuracy.png\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to handle the Shakespeare dataset in a way suitable for PyTorch.\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, clients=None, seq_length=80, n_vocab=90):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading and preprocessing the data.\n",
    "        Args:\n",
    "        - data_path: Path to the JSON file containing the dataset.\n",
    "        - clients: List of client IDs to load data for (default: all clients).\n",
    "        - seq_length: Sequence length for character-level data.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length  # Sequence length for the model\n",
    "        self.n_vocab = n_vocab  # Vocabulary size\n",
    "\n",
    "        # Create character mappings\n",
    "        self.data = list(text.values())  # Convert the dictionary values to a list\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the input-target pair at the specified index.\n",
    "        \"\"\"\n",
    "        diag = self.data[idx]\n",
    "        x = process_x(diag, self.seq_length, self.n_vocab)\n",
    "        y = process_y(diag, self.seq_length, self.n_vocab)\n",
    "        return x[0], y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character-level LSTM model for Shakespeare data.\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab=90, embedding_dim=8, hidden_dim=256, seq_length=80, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        Args:\n",
    "        - n_vocab: Number of unique characters in the dataset.\n",
    "        - embedding_dim: Size of the character embedding.\n",
    "        - hidden_dim: Number of LSTM hidden units.\n",
    "        - num_layers: Number of LSTM layers.\n",
    "        - seq_length: Length of input sequences.\n",
    "        \"\"\"\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.n_vocab = n_vocab\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Character embedding layer: Maps indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_dim)  # Character embedding layer.\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_first = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM first layer\n",
    "        self.lstm_second = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)  # LSTM second layer.\n",
    "        \n",
    "        # Fully connected layer: Maps LSTM output to vocabulary size.\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)  # Output layer (vocab_size outputs).\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "        - x: Input batch (character indices).\n",
    "        - hidden: Hidden state for LSTM (default: None, initialized internally).\n",
    "        Returns:\n",
    "        - Output logits and the updated hidden state.\n",
    "        \"\"\"\n",
    "        # Embedding layer: Convert indices to embeddings.\n",
    "        x = self.embedding(x)  \n",
    "\n",
    "        # First LSTM\n",
    "        output, _ = self.lstm_first(x)  # Process through first LSTM layer.\n",
    "        # Second LSTM\n",
    "        output, hidden = self.lstm_second(x)  # Process through second LSTM layer.\n",
    "        # Fully connected layer: Generate logits for each character.\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # Note: Softmax is not applied here because CrossEntropyLoss in PyTorch\n",
    "        # combines the softmax operation with the computation of the loss. \n",
    "        # Adding softmax here would be redundant and could introduce numerical instability.\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initializes hidden and cell states for the LSTM.\n",
    "        Args:\n",
    "        - batch_size: Number of sequences in the batch.\n",
    "        Returns:\n",
    "        - A tuple of zero-initialized hidden and cell states.\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the centralized training pipeline.\n",
    "def train_centralized(model, train_data, test_data, val_data, criterion, optimizer, scheduler, epochs, device, lr, wd):\n",
    "    \"\"\"\n",
    "    Train the model on a centralized dataset.\n",
    "    Args:\n",
    "    - model: The LSTM model to train.\n",
    "    - train_loader: DataLoader for training data.\n",
    "    - test_loader: DataLoader for test data.\n",
    "    - criterion: Loss function.\n",
    "    - optimizer: Optimizer (SGD).\n",
    "    - scheduler: Learning rate scheduler.\n",
    "    - epochs: Number of training epochs.\n",
    "    - device: Device to train on (CPU or GPU).\n",
    "    Returns:\n",
    "    - Training losses and accuracies, along with test loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to the device (CPU/GPU).\n",
    "    model.train()  # Set the model to training mode.\n",
    "    epoch_train_losses = []  # Store training loss for each epoch.\n",
    "    epoch_train_accuracies = []  # Store training accuracy for each epoch.\n",
    "    epoch_validation_losses = []  # Store validation loss for each epoch.\n",
    "    epoch_validation_accuracies = []  # Store validation accuracy for each epoch.\n",
    "    epoch_test_losses = []  # Store test loss for each epoch.\n",
    "    epoch_test_accuracies = []  # Store test accuracy for each epoch.\n",
    "\n",
    "    subfolder = f\"Centralized_lr{lr}_wd{wd}\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        progress = tqdm(train_data, desc=f\"Epoch {epoch + 1}/{epochs}\")  # Track progress.\n",
    "\n",
    "        for inputs, targets in progress:\n",
    "            \n",
    "            # Get batch dimensions\n",
    "            batch_size = inputs.size(0)\n",
    "            seq_length = inputs.size(1)\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()  # Clear previous gradients.\n",
    "            # Initialize hidden state\n",
    "            state = model.init_hidden(inputs.size(0), device)\n",
    "            state = tuple(s.to(device) for s in state)\n",
    "            # Forward pass with memory efficiency\n",
    "            outputs, _ = model(inputs, state)\n",
    "            # outputs = outputs.view(-1, model.n_vocab)  # Reshape for loss computation.\n",
    "            # targets = targets.view(-1)  # Reshape for loss computation.\n",
    "            \n",
    "            # Ensure targets have the correct shape\n",
    "            if targets.dim() == 1:  # Targets might need an expansion\n",
    "                targets = targets.unsqueeze(1).expand(-1, inputs.shape[1])\n",
    "\n",
    "            # Reshape outputs and targets to align properly\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))  # Flatten to [batch_size * seq_length, vocab_size]\n",
    "            targets = targets.reshape(-1)  # Flatten to [batch_size * seq_length]\n",
    "\n",
    "            # Validate shapes before loss\n",
    "            assert outputs.size(0) == targets.size(0), f\"Shape mismatch: outputs={outputs.shape}, targets={targets.shape}\"\n",
    "            \n",
    "            loss = criterion(outputs, targets)  # Compute loss.\n",
    "            loss.backward()  # Backpropagation.\n",
    "            optimizer.step()  # Update weights.\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predictions = outputs.max(1)  # Get predictions.\n",
    "            correct_predictions += (predictions == targets).sum().item()  # Count correct predictions.\n",
    "            total_samples += targets.size(0)  # Update sample count.\n",
    "            progress.set_postfix(loss=loss.item())  # Show current loss.\n",
    "\n",
    "        train_accuracy = correct_predictions / total_samples  # Compute accuracy.\n",
    "        avg_loss = total_loss / len(train_data)  # Compute average loss.\n",
    "        epoch_train_losses.append(avg_loss)\n",
    "        epoch_train_accuracies.append(train_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        scheduler.step()  # Update learning rate (scheduler).\n",
    "\n",
    "        # Evaluate on the validation set.\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_data, criterion, device)\n",
    "        epoch_validation_losses.append(val_loss)\n",
    "        epoch_validation_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Evaluate on the test set.\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_data, criterion, device)\n",
    "        epoch_test_losses.append(test_loss)\n",
    "        epoch_test_accuracies.append(test_accuracy)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        results={\n",
    "                'train_losses': epoch_train_losses,\n",
    "                'train_accuracies': epoch_train_accuracies,\n",
    "                'validation_losses': epoch_validation_losses,\n",
    "                'validation_accuracies': epoch_validation_accuracies,\n",
    "                'test_losses': epoch_test_losses,\n",
    "                'test_accuracies': epoch_test_accuracies\n",
    "                    }\n",
    "        \n",
    "        save_results_centralized(model, optimizer, subfolder, epoch, lr, wd, results)\n",
    "        \n",
    "    # Final evaluation on test set\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_data, criterion, device)\n",
    "    print(f\"Final -> Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return epoch_train_losses, epoch_train_accuracies, epoch_validation_losses, epoch_validation_accuracies, epoch_test_losses, epoch_test_accuracies\n",
    "\n",
    "\n",
    "# Evaluate model performance on a dataset.\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    Args:\n",
    "    - model: Trained model.\n",
    "    - data_loader: DataLoader for the evaluation dataset.\n",
    "    - criterion: Loss function.\n",
    "    - device: Device to evaluate on (CPU/GPU).\n",
    "    Returns:\n",
    "    - Average loss and accuracy.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation.\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Initialize hidden state\n",
    "            state = model.init_hidden(inputs.size(0), device) \n",
    "            state = (state[0].to(device), state[1].to(device)) \n",
    "            outputs, _ = model(inputs, state)\n",
    "            # Ensure targets have the correct shape\n",
    "            if targets.dim() == 1:  # Targets might need an expansion\n",
    "                targets = targets.unsqueeze(1).expand(-1, inputs.shape[1])\n",
    "\n",
    "            # Reshape outputs and targets to align properly\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))  # Flatten to [batch_size * seq_length, vocab_size]\n",
    "            targets = targets.reshape(-1)  # Flatten to [batch_size * seq_length]\n",
    "\n",
    "            loss = criterion(outputs, targets)  # Compute loss.\n",
    "            total_loss += loss.item()\n",
    "            _, predictions = outputs.max(1)\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)  # Compute average loss.\n",
    "    accuracy = (correct_predictions / total_samples ) * 100  # Compute accuracy.\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 133\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Best parameters:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# {'hyperparameters': 'LR=0.1 WD=0.0001', 'val_accuracy': 64.10944206008584, 'val_loss': 1.326635092496872, \u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#  'test_loss': 1.5106483635149504, 'test_accuracy': 59.98389175257732}  \u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m users \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     29\u001b[0m user_data \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(seq) \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m users \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m user_data[user][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     32\u001b[0m chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(all_texts))\n\u001b[0;32m     33\u001b[0m char_to_idx \u001b[38;5;241m=\u001b[39m {ch: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Dataset and training configurations\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "    epochs = 20  # Number of epochs for centralized training -> # TODO search hyperparameters for different epochs from 20 to 200\n",
    "    seq_length = 80  # Sequence length for LSTM inputs\n",
    "    batch_size = 64 # batch size for centralized\n",
    "    n_vocab = 90 # Character number in vobulary (ASCII)    \n",
    "    learning_rate = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "    embedding_size = 8\n",
    "    hidden_dim = 256\n",
    "    train_split = 0.8 # In LEAF Dataset the common split used is 80/20\n",
    "    momentum = 0.9 \n",
    "    weight_decay = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "    # Load data\n",
    "    base_path = os.path.join('leaf', 'data', 'shakespeare', 'data')\n",
    "    train_path = os.path.join(base_path, 'train', 'all_data_iid_1_0_keep_0_train_8.json')\n",
    "    test_path = os.path.join(base_path, 'test', 'all_data_iid_1_0_keep_0_test_8.json')\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_dataset = json.load(f)\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "\n",
    "    num_clients = len(train_dataset['users'])\n",
    "    print(\"Number of clients:\", num_clients) \n",
    "    users = train_dataset['users']\n",
    "    user_data = train_dataset['user_data']\n",
    "\n",
    "    all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
    "    chars = sorted(set(all_texts))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "\n",
    "    # Padding character\n",
    "    char_to_idx[''] = len(char_to_idx)\n",
    "\n",
    "    # Function to convert character values into indices\n",
    "    def char_to_tensor(characters):\n",
    "        indices = [char_to_idx.get(char, char_to_idx['']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    # Prepare the training data_loader\n",
    "    input_tensors = []\n",
    "    target_tensors = []\n",
    "    for user in train_dataset['users']:\n",
    "        for entry, target in zip(train_dataset['user_data'][user]['x'], train_dataset['user_data'][user]['y']):\n",
    "            input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
    "            target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
    "\n",
    "    # Padding and DataLoader creation\n",
    "    padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[''])\n",
    "    targets = torch.cat(target_tensors)\n",
    "    dataset = TensorDataset(padded_inputs, targets)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Prepare the validation data_loader\n",
    "    train_size = int(0.9 * len(dataset))  # 90% of data for training\n",
    "    val_size = len(dataset) - train_size  # 10% of data for validation\n",
    "    train_data, validation_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Prepare the testing data_loader\n",
    "    input_tensors = []\n",
    "    target_tensors = []\n",
    "    for user in test_dataset['users']:\n",
    "        for entry, target in zip(test_dataset['user_data'][user]['x'], test_dataset['user_data'][user]['y']):\n",
    "            input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
    "            target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
    "\n",
    "    # Padding e creazione di DataLoader\n",
    "    padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[''])\n",
    "    targets = torch.cat(target_tensors)\n",
    "    dataset = TensorDataset(padded_inputs, targets)\n",
    "    \n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "    # ====================\n",
    "    # Start Centralized Training\n",
    "    # ====================\n",
    "    print(\"Starting centralized training...\")\n",
    "\n",
    "    # Saving best result\n",
    "    best_result = {\n",
    "        \"hyperparameters\": None,\n",
    "        \"val_accuracy\": 0.0,\n",
    "        \"val_loss\": float('inf'),\n",
    "        \"test_loss\": float('inf'),\n",
    "        \"test_accuracy\": 0.0\n",
    "    }\n",
    "    test_tot_losses = {}\n",
    "    test_tot_accuracies = {}\n",
    "    \n",
    "    for lr in learning_rate:\n",
    "        for wd in weight_decay:\n",
    "            print(f\"Learning Rate = {lr} and Weight Decay = {wd}\")\n",
    "\n",
    "            model = CharLSTM(n_vocab, embedding_size, hidden_dim, seq_length, num_layers=2)  # Initialize LSTM model\n",
    "            criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "            optimizer = optim.SGD(model.parameters(), lr, momentum, 0, wd)  # Optimizer\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)  # Learning rate scheduler\n",
    "\n",
    "\n",
    "            # Train and evaluate centralized model\n",
    "            train_losses, train_accuracies, validation_losses, validation_accuracies, test_losses, test_accuracies = train_centralized(\n",
    "                model, train_loader, test_loader, val_loader, criterion, optimizer, scheduler, epochs, device, lr, wd\n",
    "            )\n",
    "            test_tot_losses[f\"Learning Rate = {lr} and Weight Decay = {wd}\"] = test_losses\n",
    "            test_tot_accuracies[f\"Learning Rate = {lr} and Weight Decay = {wd}\"] = test_accuracies\n",
    "\n",
    "            if validation_losses[-1] < best_result[\"val_loss\"]:\n",
    "                best_result[\"hyperparameters\"] = f\"LR={lr} WD={wd}\"\n",
    "                best_result[\"val_accuracy\"] = validation_accuracies[-1]\n",
    "                best_result[\"val_loss\"] = validation_losses[-1]\n",
    "                best_result[\"test_loss\"] = test_losses[-1]\n",
    "                best_result[\"test_accuracy\"] = test_accuracies[-1]\n",
    "                print(f\"Update best result -> Val Accuracy: {validation_accuracies[-1]:.4f}, Val Loss: {validation_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "            plot_results_centralized(validation_losses, validation_accuracies, lr, wd)\n",
    "            \n",
    "    # Print best parameters found\n",
    "    print(f\"Best parameters:\\n{best_result} \")\n",
    "\n",
    "# Best parameters:\n",
    "# {'hyperparameters': 'LR=0.1 WD=0.0001', 'val_accuracy': 64.10944206008584, 'val_loss': 1.326635092496872, \n",
    "#  'test_loss': 1.5106483635149504, 'test_accuracy': 59.98389175257732}  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-project-NFYoYubB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
