{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Project 5 - Federated Learning - Track B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LRScheduler, StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.datasets.cifar import CIFAR100\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def emit(self, record) -> None:\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(\"\\r\\033[K\" + msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    COLORS = {\n",
    "        \"DEBUG\": \"\\033[1;34m\",\n",
    "        \"INFO\": \"\\033[1;32m\",\n",
    "        \"WARNING\": \"\\033[1;33m\",\n",
    "        \"ERROR\": \"\\033[1;31m\",\n",
    "        \"CRITICAL\": \"\\033[1;35m\",\n",
    "        \"RESET\": \"\\033[0m\",\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        levelname = record.levelname\n",
    "        if levelname in self.COLORS:\n",
    "            record.levelname = (\n",
    "                f\"{self.COLORS[levelname]}{levelname}{self.COLORS['RESET']}\"\n",
    "            )\n",
    "        return super().format(record)\n",
    "\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()\n",
    "    root_logger.setLevel(level)\n",
    "\n",
    "    tqdm_handler = TqdmLoggingHandler()\n",
    "    formatter = ColoredFormatter(\n",
    "        fmt=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    tqdm_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(tqdm_handler)\n",
    "\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentSettings:\n",
    "    \"\"\"Settings for heterogeneity experiments.\"\"\"\n",
    "\n",
    "    # Core parameters\n",
    "    K: int = 100  # Number of clients\n",
    "    C: float = 0.1  # Client fraction\n",
    "\n",
    "    # Non-IID configurations\n",
    "    Nc: Tuple[Optional[int], ...] = (None, 1, 5, 10, 50)  # None = IID\n",
    "\n",
    "    # Local steps configurations (J) with scaled rounds\n",
    "    J_configs: Dict[int, int] = field(\n",
    "        default_factory=lambda: {\n",
    "            4: 2000,  # Base configuration\n",
    "            8: 1000,  # Halved rounds\n",
    "            16: 500,  # Quarter rounds\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def get_rounds(self, local_epochs: int) -> int:\n",
    "        \"\"\"Scale rounds based on local epochs.\"\"\"\n",
    "        base_j = min(self.J_configs.keys())\n",
    "        base_rounds = self.J_configs[base_j]\n",
    "        return int(base_rounds * (base_j / local_epochs))\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BaseConfig:\n",
    "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CPU_COUNT: int = os.cpu_count() or 1\n",
    "    NUM_WORKERS: int = min(0, CPU_COUNT)\n",
    "    PERSISTENT_WORKERS: bool = False\n",
    "    PIN_MEMORY: bool = False\n",
    "    PREFETCH_FACTOR: Optional[int] = None\n",
    "\n",
    "    # Memory settings\n",
    "    VIRTUAL_MEMORY_SIZE_MB: int = 16 * 1024 * 1024  # 16GB\n",
    "\n",
    "    # Random seed\n",
    "    SEED: int = 42\n",
    "\n",
    "    # Paths\n",
    "    ROOT_DIR: Path = Path.cwd()\n",
    "    CONFIGS_DIR: Path = ROOT_DIR / \"configs\"\n",
    "    DATA_DIR: Path = ROOT_DIR / \"data\"\n",
    "    MODELS_DIR: Path = ROOT_DIR / \"models\"\n",
    "    RESULTS_DIR: Path = ROOT_DIR / \"results\"\n",
    "    RUNS_DIR: Path = ROOT_DIR / \"runs\"\n",
    "    OLD_RUNS_DIR: Path = RUNS_DIR / \"old_runs\"\n",
    "\n",
    "    # Training Parameters\n",
    "    BATCH_SIZE: int = 64\n",
    "    LEARNING_RATE: float = 0.01\n",
    "    NUM_EPOCHS: int = 200\n",
    "    MOMENTUM: float = 0.9\n",
    "    WEIGHT_DECAY: float = 4e-4\n",
    "    NUM_CLASSES: int = 100\n",
    "\n",
    "    def serialize(self) -> dict:\n",
    "        \"\"\"Serialize essential config parameters.\"\"\"\n",
    "        return {\n",
    "            \"batch_size\": self.BATCH_SIZE,\n",
    "            \"learning_rate\": self.LEARNING_RATE,\n",
    "            \"num_epochs\": self.NUM_EPOCHS,\n",
    "            \"momentum\": self.MOMENTUM,\n",
    "            \"weight_decay\": self.WEIGHT_DECAY,\n",
    "            \"num_classes\": self.NUM_CLASSES,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize(cls, data: dict) -> \"BaseConfig\":\n",
    "        \"\"\"Create config from serialized data.\"\"\"\n",
    "        return cls(\n",
    "            BATCH_SIZE=data[\"batch_size\"],\n",
    "            LEARNING_RATE=data[\"learning_rate\"],\n",
    "            NUM_EPOCHS=data[\"num_epochs\"],\n",
    "            MOMENTUM=data[\"momentum\"],\n",
    "            WEIGHT_DECAY=data[\"weight_decay\"],\n",
    "            NUM_CLASSES=data[\"num_classes\"],\n",
    "        )\n",
    "\n",
    "    def matches(self, other: Union[dict, \"BaseConfig\"]) -> bool:\n",
    "        \"\"\"Check if config matches current config.\"\"\"\n",
    "        if isinstance(other, dict):\n",
    "            return (\n",
    "                other.get(\"batch_size\") == self.BATCH_SIZE\n",
    "                and other.get(\"num_classes\") == self.NUM_CLASSES\n",
    "                and other.get(\"learning_rate\") == self.LEARNING_RATE\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                other.BATCH_SIZE == self.BATCH_SIZE\n",
    "                and other.NUM_CLASSES == self.NUM_CLASSES\n",
    "                and other.LEARNING_RATE == self.LEARNING_RATE\n",
    "            )\n",
    "\n",
    "\n",
    "# Create directories\n",
    "config = BaseConfig()\n",
    "for dir_path in [\n",
    "    config.DATA_DIR,\n",
    "    config.MODELS_DIR,\n",
    "    config.RESULTS_DIR,\n",
    "    config.CONFIGS_DIR,\n",
    "    config.RUNS_DIR,\n",
    "    config.OLD_RUNS_DIR,\n",
    "]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FederatedConfig(BaseConfig):\n",
    "    \"\"\"Federated Learning specific configuration.\"\"\"\n",
    "\n",
    "    TWO_PHASE: bool = False\n",
    "    NUM_CLIENTS: int = 100  # K\n",
    "    PARTICIPATION_RATE: float = 0.1  # C\n",
    "    LOCAL_EPOCHS: int = 4  # J\n",
    "    NUM_ROUNDS: int = 2000\n",
    "    CLASSES_PER_CLIENT: Optional[int] = None  # None for IID\n",
    "    PARTICIPATION_MODE: str = \"uniform\"\n",
    "    DIRICHLET_ALPHA: Optional[float] = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        base_params = {\n",
    "            k: v\n",
    "            for k, v in kwargs.items()\n",
    "            if k\n",
    "            in [\n",
    "                \"BATCH_SIZE\",\n",
    "                \"LEARNING_RATE\",\n",
    "                \"NUM_EPOCHS\",\n",
    "                \"MOMENTUM\",\n",
    "                \"WEIGHT_DECAY\",\n",
    "                \"NUM_CLASSES\",\n",
    "            ]\n",
    "        }\n",
    "        super().__init__(*args, **base_params)\n",
    "\n",
    "    def get_epochs_for_training(self) -> int:\n",
    "        \"\"\"Get actual number of epochs to use in training.\"\"\"\n",
    "        return max(1, self.LOCAL_EPOCHS // 2) if self.TWO_PHASE else self.LOCAL_EPOCHS\n",
    "\n",
    "    def matches(self, other: Union[dict, \"FederatedConfig\"]) -> bool:  # type: ignore\n",
    "        \"\"\"Check if config matches with detailed logging.\"\"\"\n",
    "\n",
    "        def log_mismatch(field: str, val1: Any, val2: Any) -> None:\n",
    "            logging.warning(\n",
    "                f\"Config mismatch in {field}: {val1} != {val2} (saved != current)\"\n",
    "            )\n",
    "\n",
    "        if isinstance(other, dict):\n",
    "            # Base config comparison\n",
    "            if not super().matches(other):\n",
    "                logging.warning(\"Base config mismatch\")\n",
    "                return False\n",
    "\n",
    "            # Compare federated parameters\n",
    "            comparisons = [\n",
    "                (\"num_clients\", other.get(\"num_clients\"), self.NUM_CLIENTS),\n",
    "                (\n",
    "                    \"participation_rate\",\n",
    "                    other.get(\"participation_rate\"),\n",
    "                    self.PARTICIPATION_RATE,\n",
    "                ),\n",
    "                (\"local_epochs\", other.get(\"local_epochs\"), self.LOCAL_EPOCHS),\n",
    "                (\n",
    "                    \"classes_per_client\",\n",
    "                    other.get(\"classes_per_client\"),\n",
    "                    self.CLASSES_PER_CLIENT,\n",
    "                ),\n",
    "                (\n",
    "                    \"participation_mode\",\n",
    "                    other.get(\"participation_mode\"),\n",
    "                    self.PARTICIPATION_MODE,\n",
    "                ),\n",
    "                (\"dirichlet_alpha\", other.get(\"dirichlet_alpha\"), self.DIRICHLET_ALPHA),\n",
    "                (\"two_phase\", other.get(\"two_phase\", False), self.TWO_PHASE),\n",
    "            ]\n",
    "\n",
    "            for field, saved_val, current_val in comparisons:\n",
    "                if saved_val != current_val:\n",
    "                    log_mismatch(field, saved_val, current_val)\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            # Base config comparison\n",
    "            if not super().matches(other):\n",
    "                logging.debug(\"Base config mismatch\")\n",
    "                return False\n",
    "\n",
    "            # Compare federated parameters\n",
    "            comparisons = [\n",
    "                (\"num_clients\", other.NUM_CLIENTS, self.NUM_CLIENTS),\n",
    "                (\n",
    "                    \"participation_rate\",\n",
    "                    other.PARTICIPATION_RATE,\n",
    "                    self.PARTICIPATION_RATE,\n",
    "                ),\n",
    "                (\"local_epochs\", other.LOCAL_EPOCHS, self.LOCAL_EPOCHS),\n",
    "                (\n",
    "                    \"classes_per_client\",\n",
    "                    other.CLASSES_PER_CLIENT,\n",
    "                    self.CLASSES_PER_CLIENT,\n",
    "                ),\n",
    "                (\n",
    "                    \"participation_mode\",\n",
    "                    other.PARTICIPATION_MODE,\n",
    "                    self.PARTICIPATION_MODE,\n",
    "                ),\n",
    "                (\"dirichlet_alpha\", other.DIRICHLET_ALPHA, self.DIRICHLET_ALPHA),\n",
    "                (\"two_phase\", other.TWO_PHASE, self.TWO_PHASE),\n",
    "            ]\n",
    "\n",
    "            for field, saved_val, current_val in comparisons:\n",
    "                if saved_val != current_val:\n",
    "                    log_mismatch(field, saved_val, current_val)\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "    def serialize(self) -> dict:\n",
    "        \"\"\"Serialize with validation.\"\"\"\n",
    "        data = super().serialize()\n",
    "        data.update(\n",
    "            {\n",
    "                \"num_clients\": self.NUM_CLIENTS,\n",
    "                \"participation_rate\": self.PARTICIPATION_RATE,\n",
    "                \"local_epochs\": self.LOCAL_EPOCHS,\n",
    "                \"num_rounds\": self.NUM_ROUNDS,\n",
    "                \"classes_per_client\": self.CLASSES_PER_CLIENT,\n",
    "                \"participation_mode\": self.PARTICIPATION_MODE,\n",
    "                \"dirichlet_alpha\": self.DIRICHLET_ALPHA,\n",
    "                \"two_phase\": self.TWO_PHASE,\n",
    "            }\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize(cls, data: dict) -> \"FederatedConfig\":\n",
    "        \"\"\"Deserialize with validation.\"\"\"\n",
    "        # Ensure all required fields are present\n",
    "        required_fields = {\n",
    "            \"num_clients\",\n",
    "            \"participation_rate\",\n",
    "            \"local_epochs\",\n",
    "            \"num_rounds\",\n",
    "            \"classes_per_client\",\n",
    "            \"participation_mode\",\n",
    "            \"two_phase\",\n",
    "        }\n",
    "\n",
    "        missing = required_fields - set(data.keys())\n",
    "        if missing:\n",
    "            logging.warning(f\"Missing required fields in config: {missing}\")\n",
    "            # Set defaults for backward compatibility\n",
    "            for field in missing:\n",
    "                data[field] = getattr(cls, field.upper())\n",
    "\n",
    "        return cls(\n",
    "            BATCH_SIZE=data.get(\"batch_size\", cls.BATCH_SIZE),\n",
    "            LEARNING_RATE=data.get(\"learning_rate\", cls.LEARNING_RATE),\n",
    "            NUM_EPOCHS=data.get(\"num_epochs\", cls.NUM_EPOCHS),\n",
    "            MOMENTUM=data.get(\"momentum\", cls.MOMENTUM),\n",
    "            WEIGHT_DECAY=data.get(\"weight_decay\", cls.WEIGHT_DECAY),\n",
    "            NUM_CLASSES=data.get(\"num_classes\", cls.NUM_CLASSES),\n",
    "            NUM_CLIENTS=data.get(\"num_clients\", cls.NUM_CLIENTS),\n",
    "            PARTICIPATION_RATE=data.get(\"participation_rate\", cls.PARTICIPATION_RATE),\n",
    "            LOCAL_EPOCHS=data.get(\"local_epochs\", cls.LOCAL_EPOCHS),\n",
    "            NUM_ROUNDS=data.get(\"num_rounds\", cls.NUM_ROUNDS),\n",
    "            CLASSES_PER_CLIENT=data.get(\"classes_per_client\", cls.CLASSES_PER_CLIENT),\n",
    "            PARTICIPATION_MODE=data.get(\"participation_mode\", cls.PARTICIPATION_MODE),\n",
    "            DIRICHLET_ALPHA=data.get(\"dirichlet_alpha\", cls.DIRICHLET_ALPHA),\n",
    "            TWO_PHASE=data.get(\"two_phase\", cls.TWO_PHASE),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.connected = nn.Sequential(\n",
    "            nn.Linear(5 * 5 * 64, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(192, config.NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.connected(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Logger\n",
    "\n",
    "This class is used to save the model training in a way to be analyzed using tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsManager:\n",
    "    \"\"\"Enhanced metrics manager for experiment tracking, visualization and comparison.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: BaseConfig,\n",
    "        model_name: str,\n",
    "        training_type: Literal[\"centralized\", \"federated\"],\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.config = config\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        # Setup experiment directories\n",
    "        self.experiment_group = training_type\n",
    "        if experiment_name:\n",
    "            self.experiment_group = f\"{training_type}_{experiment_name.split('_')[0]}\"\n",
    "\n",
    "        self.experiment_dir = config.RUNS_DIR / self.experiment_group\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Setup run name and paths\n",
    "        run_suffix = f\"{experiment_name}_{timestamp}\" if experiment_name else timestamp\n",
    "        self.run_name = f\"{model_name}_{run_suffix}\"\n",
    "        self.run_dir = self.experiment_dir / self.run_name\n",
    "\n",
    "        # Setup metrics storage\n",
    "        self.metrics_dir = self.run_dir / \"metrics\"\n",
    "        self.metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.train_file = self.metrics_dir / \"train_metrics.csv\"\n",
    "        self.val_file = self.metrics_dir / \"val_metrics.csv\"\n",
    "        self.test_file = self.metrics_dir / \"test_metrics.csv\"\n",
    "\n",
    "        # Initialize CSV files\n",
    "        for file in [self.train_file, self.val_file, self.test_file]:\n",
    "            if not file.exists():\n",
    "                with open(file, \"w\", buffering=1) as f:\n",
    "                    f.write(\"step,loss,accuracy\\n\")\n",
    "                    f.flush()\n",
    "\n",
    "        # Setup TensorBoard\n",
    "        self.writer = SummaryWriter(self.run_dir)\n",
    "\n",
    "        # In-memory metrics storage\n",
    "        self.metrics = {\n",
    "            \"train\": {\"loss\": [], \"accuracy\": [], \"steps\": []},\n",
    "            \"validation\": {\"loss\": [], \"accuracy\": [], \"steps\": []},\n",
    "            \"test\": {\"loss\": [], \"accuracy\": [], \"steps\": []},\n",
    "        }\n",
    "\n",
    "        # Store configuration\n",
    "        self.save_config()\n",
    "\n",
    "    def save_config(self) -> None:\n",
    "        \"\"\"Save experiment configuration.\"\"\"\n",
    "        config_dict = {\n",
    "            \"experiment_group\": self.experiment_group,\n",
    "            \"run_name\": self.run_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"config\": self.config.serialize(),\n",
    "        }\n",
    "\n",
    "        with open(self.run_dir / \"config.json\", \"w\") as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    def save_metrics(self) -> None:\n",
    "        \"\"\"Safely save all metrics to disk.\"\"\"\n",
    "        try:\n",
    "            # Ensure TensorBoard writes are flushed\n",
    "            self.writer.flush()\n",
    "\n",
    "            # Function to safely write metrics to CSV\n",
    "            def write_metrics_to_csv(metrics_data: dict, file_path: Path) -> None:\n",
    "                if not metrics_data[\"steps\"]:\n",
    "                    return\n",
    "\n",
    "                df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"step\": metrics_data[\"steps\"],\n",
    "                        \"loss\": metrics_data[\"loss\"],\n",
    "                        \"accuracy\": metrics_data[\"accuracy\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Write with proper index handling\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "            # Save each split's metrics\n",
    "            for split, file_path in [\n",
    "                (\"train\", self.train_file),\n",
    "                (\"validation\", self.val_file),\n",
    "                (\"test\", self.test_file),\n",
    "            ]:\n",
    "                if self.metrics[split][\"steps\"]:  # Only save if we have data\n",
    "                    write_metrics_to_csv(self.metrics[split], file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving metrics: {e}\")\n",
    "            raise\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        split: Literal[\"train\", \"validation\", \"test\"],\n",
    "        loss: float,\n",
    "        accuracy: float,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Log metrics for specified split.\"\"\"\n",
    "        # TensorBoard logging\n",
    "        if not isinstance(loss, (int, float)) or not isinstance(accuracy, (int, float)):\n",
    "            logging.error(f\"Invalid metrics: loss={loss}, accuracy={accuracy}\")\n",
    "            return\n",
    "        if math.isnan(loss) or math.isnan(accuracy):\n",
    "            logging.error(f\"NaN detected: loss={loss}, accuracy={accuracy}\")\n",
    "            return\n",
    "\n",
    "        self.writer.add_scalars(\"metrics/loss\", {split: loss}, step)\n",
    "        self.writer.add_scalars(\"metrics/accuracy\", {split: accuracy}, step)\n",
    "\n",
    "        # Store in memory\n",
    "        self.metrics[split][\"loss\"].append(loss)\n",
    "        self.metrics[split][\"accuracy\"].append(accuracy)\n",
    "        self.metrics[split][\"steps\"].append(step)\n",
    "\n",
    "        # Save metrics to disk every 10 steps\n",
    "        if len(self.metrics[split][\"steps\"]) % 10 == 0:\n",
    "            self.save_metrics()\n",
    "\n",
    "    def log_fl_metrics(\n",
    "        self,\n",
    "        round_idx: int,\n",
    "        metrics: Dict[str, float],\n",
    "        client_stats: Optional[Dict[str, int | float]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Log federated learning specific metrics.\"\"\"\n",
    "        val_loss = metrics.get(\"val_loss\")\n",
    "        val_accuracy = metrics.get(\"val_accuracy\")\n",
    "        test_loss = metrics.get(\"test_loss\")\n",
    "        test_accuracy = metrics.get(\"test_accuracy\")\n",
    "\n",
    "        if val_loss is not None and val_accuracy is not None:\n",
    "            self.log_metrics(\"validation\", val_loss, val_accuracy, round_idx)\n",
    "\n",
    "        if test_loss is not None and test_accuracy is not None:\n",
    "            self.log_metrics(\"test\", test_loss, test_accuracy, round_idx)\n",
    "\n",
    "        if client_stats:\n",
    "            self.writer.add_scalars(\"federated/client_stats\", client_stats, round_idx)\n",
    "\n",
    "    def plot_learning_curves(self) -> None:\n",
    "        \"\"\"Generate learning curves plot.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        for split in [\"train\", \"validation\", \"test\"]:\n",
    "            if len(self.metrics[split][\"steps\"]) > 0:\n",
    "                ax1.plot(\n",
    "                    self.metrics[split][\"steps\"],\n",
    "                    self.metrics[split][\"loss\"],\n",
    "                    label=f\"{split} loss\",\n",
    "                )\n",
    "                ax2.plot(\n",
    "                    self.metrics[split][\"steps\"],\n",
    "                    self.metrics[split][\"accuracy\"],\n",
    "                    label=f\"{split} accuracy\",\n",
    "                )\n",
    "\n",
    "        ax1.set_title(\"Loss Curves\")\n",
    "        ax1.set_xlabel(\"Steps\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.set_title(\"Accuracy Curves\")\n",
    "        ax2.set_xlabel(\"Steps\")\n",
    "        ax2.set_ylabel(\"Accuracy (%)\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.metrics_dir / \"learning_curves.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def save_summary(self) -> None:\n",
    "        \"\"\"Save experiment summary statistics.\"\"\"\n",
    "        summary = {\n",
    "            \"experiment_group\": self.experiment_group,\n",
    "            \"run_name\": self.run_name,\n",
    "            \"final_metrics\": {},\n",
    "        }\n",
    "\n",
    "        for split in [\"train\", \"validation\", \"test\"]:\n",
    "            if len(self.metrics[split][\"loss\"]) > 0:\n",
    "                summary[\"final_metrics\"][split] = {\n",
    "                    \"final_loss\": self.metrics[split][\"loss\"][-1],\n",
    "                    \"final_accuracy\": self.metrics[split][\"accuracy\"][-1],\n",
    "                    \"best_accuracy\": max(self.metrics[split][\"accuracy\"]),\n",
    "                    \"best_loss\": min(self.metrics[split][\"loss\"]),\n",
    "                }\n",
    "\n",
    "        with open(self.metrics_dir / \"summary.json\", \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(config_path: Path) -> Tuple[str, str, BaseConfig]:\n",
    "        \"\"\"Load experiment configuration.\"\"\"\n",
    "        with open(config_path) as f:\n",
    "            config_dict = json.load(f)\n",
    "\n",
    "        # Determine config type and deserialize\n",
    "        config_data = config_dict[\"config\"]\n",
    "        if \"num_clients\" in config_data:\n",
    "            config = FederatedConfig.deserialize(config_data)\n",
    "        else:\n",
    "            config = BaseConfig.deserialize(config_data)\n",
    "\n",
    "        return config_dict[\"experiment_group\"], config_dict[\"run_name\"], config\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_runs(base_dir: Path, experiment_group: str) -> None:\n",
    "        \"\"\"Compare multiple runs within an experiment group.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import pandas as pd\n",
    "\n",
    "        exp_dir = base_dir / experiment_group\n",
    "        if not exp_dir.exists():\n",
    "            raise ValueError(f\"No experiments found for group {experiment_group}\")\n",
    "\n",
    "        # Collect all run data\n",
    "        summaries = []\n",
    "        for run_dir in exp_dir.glob(\"*\"):\n",
    "            if not run_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            config_file = run_dir / \"config.json\"\n",
    "            summary_file = run_dir / \"metrics\" / \"summary.json\"\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file) as f:\n",
    "                    summary = json.load(f)\n",
    "                    if summaries:\n",
    "                        # get only the config\n",
    "                        summary[\"config\"] = MetricsManager.load_config(config_file)[2]\n",
    "\n",
    "                    summaries.append(summary)\n",
    "\n",
    "        if not summaries:\n",
    "            raise ValueError(\"No run data found\")\n",
    "\n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for summary in summaries:\n",
    "            row = {\"run\": summary[\"run_name\"]}\n",
    "            for split in [\"train\", \"validation\", \"test\"]:\n",
    "                if split in summary[\"final_metrics\"]:\n",
    "                    metrics = summary[\"final_metrics\"][split]\n",
    "                    row.update(\n",
    "                        {\n",
    "                            f\"{split}_final_loss\": metrics[\"final_loss\"],\n",
    "                            f\"{split}_final_acc\": metrics[\"final_accuracy\"],\n",
    "                            f\"{split}_best_acc\": metrics[\"best_accuracy\"],\n",
    "                        }\n",
    "                    )\n",
    "            comparison_data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        # Save comparison results\n",
    "        results_dir = exp_dir / \"comparisons\"\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        df.to_csv(results_dir / \"comparison.csv\", index=False)\n",
    "        df.to_latex(\n",
    "            results_dir / \"comparison.tex\",\n",
    "            float_format=\"%.2f\",\n",
    "            index=False,\n",
    "            caption=f\"Comparison of {experiment_group} experiments\",\n",
    "            label=f\"tab:{experiment_group}_comparison\",\n",
    "        )\n",
    "\n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(df[\"run\"], df[\"test_best_acc\"])\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.title(f\"{experiment_group} - Test Accuracy Comparison\")\n",
    "        plt.ylabel(\"Best Test Accuracy (%)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / \"accuracy_comparison.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close writer and save final artifacts.\"\"\"\n",
    "        try:\n",
    "            # Save final metrics\n",
    "            self.save_metrics()\n",
    "            # Generate plots and summaries\n",
    "            self.plot_learning_curves()\n",
    "            self.save_summary()\n",
    "        finally:\n",
    "            self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Manager\n",
    "\n",
    "Here the CIFAR100 is downloaded and the train, validation and test split are constructed to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar100DatasetManager:\n",
    "    config: BaseConfig\n",
    "    validation_split: float\n",
    "    train_transform: transforms.Compose\n",
    "    test_transform: transforms.Compose\n",
    "    train_loader: DataLoader[CIFAR100]\n",
    "    val_loader: DataLoader[CIFAR100]\n",
    "    test_loader: DataLoader[CIFAR100]\n",
    "\n",
    "    def __init__(self, config: BaseConfig, validation_split: float = 0.1) -> None:\n",
    "        self.config = config\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.train_loader, self.val_loader, self.test_loader = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "    ) -> Tuple[DataLoader[CIFAR100], DataLoader[CIFAR100], DataLoader[CIFAR100]]:\n",
    "        full_trainset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "\n",
    "        train_size: int = int((1 - self.validation_split) * len(full_trainset))\n",
    "        val_size: int = len(full_trainset) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_trainset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.config.SEED),\n",
    "        )\n",
    "\n",
    "        test_dataset: CIFAR100 = CIFAR100(\n",
    "            root=self.config.DATA_DIR,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.test_transform,\n",
    "        )\n",
    "\n",
    "        loader_kwargs = {\"num_workers\": self.config.NUM_WORKERS, \"pin_memory\": True}\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        test_loader: DataLoader[CIFAR100] = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.train_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.val_loader.dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset[CIFAR100]:\n",
    "        return self.test_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Trainer\n",
    "\n",
    "This class is responsible to train and evaluate the model (here only considered for typing the LeNet model) in the traditional sense.\n",
    "Local training with normal train and evaluate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedTrainer:\n",
    "    model: LeNet\n",
    "    config: BaseConfig\n",
    "    device: torch.device\n",
    "    metrics: MetricsManager\n",
    "\n",
    "    def __init__(\n",
    "        self, model: LeNet, config: BaseConfig, experiment_name: str = \"baseline\"\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        self.device_type = str(config.DEVICE)\n",
    "        self.model = model.to(config.DEVICE)\n",
    "        self.metrics = MetricsManager(\n",
    "            config=config,\n",
    "            model_name=model.__class__.__name__.lower(),\n",
    "            training_type=\"centralized\",\n",
    "            experiment_name=experiment_name,\n",
    "        )\n",
    "        self.checkpoint_dir = config.MODELS_DIR / \"centralized\"\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.checkpoint_name = (\n",
    "            f\"centralized\"\n",
    "            f\"_lr{config.LEARNING_RATE:.3f}\"\n",
    "            f\"_wd{config.WEIGHT_DECAY:.4f}\"\n",
    "            f\"_m{config.MOMENTUM:.1f}\"\n",
    "            f\"_{experiment_name}.pt\"\n",
    "        )\n",
    "        self.checkpoint_path = self.checkpoint_dir / self.checkpoint_name\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self, epoch: int, best_val_loss: float, best_val_acc: float\n",
    "    ) -> None:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"config\": self.config.serialize(),\n",
    "        }\n",
    "        torch.save(checkpoint, self.checkpoint_path)\n",
    "        logging.info(f\"Checkpoint saved: {self.checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self) -> Tuple[int, float, float]:\n",
    "        if not self.checkpoint_path.exists():\n",
    "            return 0, float(\"inf\"), 0.0\n",
    "\n",
    "        checkpoint = torch.load(self.checkpoint_path)\n",
    "\n",
    "        # Validate using matches method\n",
    "        if not self.config.matches(checkpoint[\"config\"]):\n",
    "            logging.warning(\"Config mismatch in checkpoint, starting fresh training\")\n",
    "            return 0, float(\"inf\"), 0.0\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        logging.info(f\"Resumed from checkpoint: {self.checkpoint_path}\")\n",
    "        return (\n",
    "            checkpoint[\"epoch\"],\n",
    "            checkpoint[\"best_val_loss\"],\n",
    "            checkpoint[\"best_val_acc\"],\n",
    "        )\n",
    "\n",
    "    def evaluate_model(\n",
    "        self, model: LeNet, data_loader: DataLoader[CIFAR100]\n",
    "    ) -> Tuple[float, float]:\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        avg_loss: float = total_loss / total\n",
    "        accuracy: float = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        max_epochs: Optional[int] = None,\n",
    "        max_patience: int = 10,\n",
    "        scheduler_fn: Optional[LRScheduler] = None,\n",
    "        manual_scheduler: bool = False,\n",
    "    ) -> float:\n",
    "        start_epoch, best_val_loss, best_val_acc = self.load_checkpoint()\n",
    "\n",
    "        epoch = start_epoch\n",
    "        epochs: int = max_epochs or self.config.NUM_EPOCHS\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        scheduler = (\n",
    "            scheduler_fn\n",
    "            if manual_scheduler\n",
    "            else CosineAnnealingLR(optimizer=optimizer, T_max=epochs)\n",
    "        )\n",
    "\n",
    "        best_model_state = None\n",
    "        patience = max_patience\n",
    "        patience_counter = 0\n",
    "        train_acc = 0.0\n",
    "        avg_train_loss = 0.0\n",
    "\n",
    "        epoch_pbar = tqdm(\n",
    "            range(start_epoch, epochs),\n",
    "            initial=start_epoch,\n",
    "            total=epochs,\n",
    "            desc=\"Training\",\n",
    "            unit=\"epoch\",\n",
    "            leave=True,\n",
    "            colour=\"green\",\n",
    "            position=0,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            for epoch in epoch_pbar:\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                data_iterator = iter(train_loader)\n",
    "                try:\n",
    "                    next_batch = next(data_iterator)\n",
    "                    next_batch = [\n",
    "                        t.to(self.device, non_blocking=True) for t in next_batch\n",
    "                    ]\n",
    "                except StopIteration:\n",
    "                    logging.error(\"Empty training data loader\")\n",
    "                    return best_val_acc\n",
    "\n",
    "                total_steps = len(train_loader)\n",
    "\n",
    "                for step in range(total_steps):\n",
    "                    # Use current batch and prefetch next\n",
    "                    current_batch = next_batch\n",
    "                    try:\n",
    "                        next_batch = next(data_iterator)\n",
    "                        next_batch = [\n",
    "                            t.to(self.device, non_blocking=True) for t in next_batch\n",
    "                        ]\n",
    "                    except StopIteration:\n",
    "                        data_iterator = iter(train_loader)\n",
    "                        next_batch = next(data_iterator)\n",
    "                        next_batch = [\n",
    "                            t.to(self.device, non_blocking=True) for t in next_batch\n",
    "                        ]\n",
    "\n",
    "                    inputs, targets = current_batch\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                    with torch.amp.autocast_mode.autocast(device_type=self.device_type):\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        total_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        total += targets.size(0)\n",
    "                        correct += predicted.eq(targets).sum().item()\n",
    "                        train_acc = 100.0 * correct / total\n",
    "                        avg_train_loss = total_loss / (step + 1)\n",
    "\n",
    "                        global_step = epoch * total_steps + step\n",
    "                        self.metrics.log_metrics(\n",
    "                            split=\"train\",\n",
    "                            loss=avg_train_loss,\n",
    "                            accuracy=train_acc,\n",
    "                            step=global_step,\n",
    "                        )\n",
    "\n",
    "                    del inputs, targets, outputs, loss\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Validation phase\n",
    "                val_loss, val_acc = self.evaluate_model(self.model, val_loader)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                self.metrics.log_metrics(\n",
    "                    split=\"validation\",\n",
    "                    loss=val_loss,\n",
    "                    accuracy=val_acc,\n",
    "                    step=epoch,\n",
    "                )\n",
    "\n",
    "                epoch_pbar.set_postfix(\n",
    "                    {\n",
    "                        \"ep\": f\"{epoch+1}/{max_epochs or self.config.NUM_EPOCHS}\",\n",
    "                        \"tr_loss\": f\"{avg_train_loss:.3f}\",\n",
    "                        \"tr_acc\": f\"{train_acc:.1f}%\",\n",
    "                        \"val_loss\": f\"{val_loss:.3f}\",\n",
    "                        \"val_acc\": f\"{val_acc:.1f}%\",\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = self.model.state_dict().copy()\n",
    "                    self.save_checkpoint(epoch, best_val_loss, best_val_acc)\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                logging.info(\"Training completed!\")\n",
    "            # Final evaluation\n",
    "            if best_model_state is not None:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "            test_loss, test_acc = self.evaluate_model(self.model, test_loader)\n",
    "            self.metrics.log_metrics(\n",
    "                split=\"test\",\n",
    "                loss=test_loss,\n",
    "                accuracy=test_acc,\n",
    "                step=epoch,\n",
    "            )\n",
    "            logging.info(\n",
    "                f\"Final Test Results - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "            return best_val_acc\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning with Two-Phase Training\n",
    "\n",
    "### Overview\n",
    "The implementation extends standard Federated Averaging (FedAvg) by introducing an innovative two-phase training approach. This modification aims to enhance model performance through intermediate model shuffling between clients.\n",
    "\n",
    "### Two-Phase Innovation\n",
    "The key innovation lies in splitting each federation round into two distinct training phases:\n",
    "\n",
    "1. **Phase 1 (Initial Training)**\n",
    "   - Selected clients receive global model\n",
    "   - Each client trains for J/2 epochs (set automatically in the config for experimenting with both standard and two-phase approach)\n",
    "   - Training occurs on local datasets\n",
    "   - Models capture initial client-specific features\n",
    "\n",
    "2. **Intermediate Shuffling**\n",
    "   - Client models are randomly redistributed\n",
    "   - Each client receives a different model\n",
    "   - Ensures knowledge sharing across clients\n",
    "\n",
    "3. **Phase 2 (Extended Training)**\n",
    "   - Clients train received models for J/2 epochs\n",
    "   - Training continues on local datasets\n",
    "   - Models benefit from different data distributions\n",
    "\n",
    "4. **Final Aggregation**\n",
    "   - Twice-trained models are collected\n",
    "   - FedAvg applied with dataset size weighting\n",
    "   - Results update global model\n",
    "\n",
    "### Implementation Benefits\n",
    "\n",
    "1. **Enhanced Knowledge Sharing**\n",
    "   - Models exposed to multiple data distributions\n",
    "   - Better feature generalization\n",
    "   - Reduced impact of client data heterogeneity\n",
    "\n",
    "2. **Improved Non-IID Handling**\n",
    "   - Significant gains in non-IID scenarios (+3.08% with 50 classes)\n",
    "   - Consistent performance in moderate non-IID settings\n",
    "   - Better adaptation to data heterogeneity\n",
    "\n",
    "3. **Flexible Configuration**\n",
    "   - Adjustable local epochs (J parameter)\n",
    "   - Compatible with various client selection strategies\n",
    "   - Maintains FedAvg's core benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LeNet,\n",
    "        train_dataset: Dataset[CIFAR100],\n",
    "        val_loader: DataLoader[CIFAR100],\n",
    "        test_loader: DataLoader[CIFAR100],\n",
    "        config: FederatedConfig,\n",
    "        experiment_name: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.global_model = model.to(config.DEVICE)\n",
    "        self.device = config.DEVICE\n",
    "        self.device_type = str(config.DEVICE)\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        # Pre-create all client shards and dataloaders\n",
    "        self.num_clients = config.NUM_CLIENTS\n",
    "        self.client_loaders = self._setup_client_data(train_dataset)\n",
    "\n",
    "        # Setup client selection\n",
    "        if config.PARTICIPATION_MODE == \"skewed\":\n",
    "            if config.DIRICHLET_ALPHA is None:\n",
    "                raise ValueError(\"dirichlet_alpha required for skewed mode\")\n",
    "            self.selection_probs = np.random.dirichlet(\n",
    "                [config.DIRICHLET_ALPHA] * config.NUM_CLIENTS\n",
    "            )\n",
    "        else:\n",
    "            self.selection_probs = np.ones(config.NUM_CLIENTS) / config.NUM_CLIENTS\n",
    "\n",
    "        experiment_group = \"federated\"\n",
    "\n",
    "        self.metrics = MetricsManager(\n",
    "            config=config,\n",
    "            model_name=model.__class__.__name__.lower(),\n",
    "            training_type=experiment_group,\n",
    "            experiment_name=experiment_name,\n",
    "        )\n",
    "\n",
    "        # Pre-allocate client models\n",
    "        self.client_models = [copy.deepcopy(model) for _ in range(config.NUM_CLIENTS)]\n",
    "        self.checkpoint_dir = config.MODELS_DIR / \"federated\"\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.checkpoint_name = (\n",
    "            f\"{'iid' if config.CLASSES_PER_CLIENT is None else f'noniid_{config.CLASSES_PER_CLIENT}cls'}\"\n",
    "            f\"_{config.PARTICIPATION_MODE}\"\n",
    "            f\"_C{config.NUM_CLIENTS}\"\n",
    "            f\"_P{config.PARTICIPATION_RATE:.2f}\"\n",
    "            f\"_E{config.LOCAL_EPOCHS}\"\n",
    "            f\"_R{config.NUM_ROUNDS}\"\n",
    "            f\"_{'two_phase' if config.TWO_PHASE else 'standard'}\"\n",
    "            f\"_lr{config.LEARNING_RATE:.3f}\"\n",
    "            f\"_wd{config.WEIGHT_DECAY:.4f}\"\n",
    "            f\"_m{config.MOMENTUM:.1f}\"\n",
    "        )\n",
    "        if config.DIRICHLET_ALPHA is not None:\n",
    "            self.checkpoint_name += f\"_alpha{config.DIRICHLET_ALPHA:.1f}\"\n",
    "\n",
    "        if experiment_name:\n",
    "            self.checkpoint_name += f\"_{experiment_name}\"\n",
    "\n",
    "        self.checkpoint_name += \".pt\"\n",
    "        self.checkpoint_path = self.checkpoint_dir / self.checkpoint_name\n",
    "\n",
    "    def _setup_client_data(\n",
    "        self, dataset: Dataset[CIFAR100]\n",
    "    ) -> List[DataLoader[CIFAR100]]:\n",
    "        shards: List[Subset[CIFAR100]] = (\n",
    "            self._create_iid_shards(dataset)\n",
    "            if self.config.CLASSES_PER_CLIENT is None\n",
    "            else self._create_noniid_shards(dataset)\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            DataLoader(\n",
    "                shard,\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                num_workers=self.config.NUM_WORKERS,\n",
    "                pin_memory=self.config.PIN_MEMORY,\n",
    "                persistent_workers=self.config.PERSISTENT_WORKERS,\n",
    "                prefetch_factor=self.config.PREFETCH_FACTOR,\n",
    "                drop_last=True,\n",
    "            )\n",
    "            for shard in shards\n",
    "        ]\n",
    "\n",
    "    def _create_iid_shards(self, dataset: Dataset[CIFAR100]) -> List[Subset[CIFAR100]]:\n",
    "        \"\"\"Create IID data shards.\"\"\"\n",
    "        if len(dataset) == 0:\n",
    "            raise ValueError(\"Empty dataset\")\n",
    "\n",
    "        indices = np.random.permutation(len(dataset))\n",
    "        shard_size = len(dataset) // self.num_clients\n",
    "\n",
    "        return [\n",
    "            Subset(dataset, indices[i : i + shard_size])\n",
    "            for i in range(0, len(indices), shard_size)\n",
    "        ]\n",
    "\n",
    "    def _create_noniid_shards(\n",
    "        self, dataset: Dataset[CIFAR100]\n",
    "    ) -> List[Subset[CIFAR100]]:\n",
    "        \"\"\"Create non-IID data shards using class distribution.\"\"\"\n",
    "        # Handle both Dataset and Subset cases\n",
    "        if isinstance(dataset, Subset):\n",
    "            # If dataset is a Subset, get targets from the original dataset\n",
    "            targets = np.array(\n",
    "                [dataset.dataset.targets[idx] for idx in dataset.indices]\n",
    "            )\n",
    "            original_indices = np.array(dataset.indices)\n",
    "        else:\n",
    "            # If dataset is the original dataset\n",
    "            targets = np.array(dataset.targets)\n",
    "            original_indices = np.arange(len(dataset))\n",
    "\n",
    "        # Group indices by class\n",
    "        class_indices = {\n",
    "            label: np.where(targets == label)[0]\n",
    "            for label in range(self.config.NUM_CLASSES)\n",
    "        }\n",
    "\n",
    "        # Convert relative indices back to original dataset indices\n",
    "        class_indices = {\n",
    "            label: original_indices[indices.astype(int)]\n",
    "            for label, indices in class_indices.items()\n",
    "        }\n",
    "\n",
    "        client_indices = []\n",
    "        num_classes = self.config.CLASSES_PER_CLIENT or self.config.NUM_CLASSES\n",
    "\n",
    "        for _ in range(self.num_clients):\n",
    "            indices = []\n",
    "            # Select random classes for this client\n",
    "            selected_classes = np.random.choice(\n",
    "                list(class_indices.keys()),\n",
    "                size=min(num_classes, len(class_indices)),\n",
    "                replace=False,\n",
    "            )\n",
    "\n",
    "            # Add samples from each selected class\n",
    "            for class_label in selected_classes:\n",
    "                class_samples = np.random.choice(\n",
    "                    class_indices[class_label],\n",
    "                    size=len(class_indices[class_label]) // self.num_clients,\n",
    "                    replace=False,\n",
    "                )\n",
    "                indices.extend(class_samples)\n",
    "\n",
    "            client_indices.append(\n",
    "                Subset(\n",
    "                    dataset.dataset if isinstance(dataset, Subset) else dataset, indices\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return client_indices\n",
    "\n",
    "    def _evaluate(self, loader: DataLoader[CIFAR100]) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate model on given data loader.\n",
    "        # Returns\n",
    "            Tuple of (loss, accuracy)\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with (\n",
    "            torch.no_grad(),\n",
    "            torch.amp.autocast_mode.autocast(device_type=self.device_type),\n",
    "        ):\n",
    "            for inputs, targets in loader:\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                targets = targets.to(self.device, non_blocking=True)\n",
    "\n",
    "                outputs = self.global_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "    def _aggregate_models(self, selected_clients: List[int] | npt.NDArray) -> None:\n",
    "        \"\"\"Aggregate models using weighted average based on dataset sizes.\"\"\"\n",
    "        with (\n",
    "            torch.no_grad(),\n",
    "            torch.amp.autocast_mode.autocast(device_type=self.device_type),\n",
    "        ):\n",
    "            # Calculate total samples across selected clients\n",
    "            total_samples = sum(\n",
    "                len(self.client_loaders[idx].dataset) for idx in selected_clients\n",
    "            )\n",
    "\n",
    "            # Initialize aggregated parameters\n",
    "            for k, v in self.global_model.state_dict().items():\n",
    "                weighted_sum = torch.zeros_like(v)\n",
    "                for idx in selected_clients:\n",
    "                    # Get client's weight based on dataset size\n",
    "                    client_weight = (\n",
    "                        len(self.client_loaders[idx].dataset) / total_samples\n",
    "                    )\n",
    "                    client_params = (\n",
    "                        self.client_models[idx].state_dict()[k].to(self.device)\n",
    "                    )\n",
    "                    weighted_sum.add_(client_params * client_weight)\n",
    "\n",
    "                # Update global model\n",
    "                v.copy_(weighted_sum)\n",
    "\n",
    "    def save_checkpoint(self, round_idx: int, best_val_loss: float) -> None:\n",
    "        checkpoint = {\n",
    "            \"round\": round_idx,\n",
    "            \"model_state_dict\": self.global_model.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"config\": self.config.serialize(),\n",
    "        }\n",
    "        torch.save(checkpoint, self.checkpoint_path)\n",
    "        logging.info(f\"Checkpoint saved: {self.checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self) -> Tuple[int, float]:\n",
    "        if not self.checkpoint_path.exists():\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        checkpoint = torch.load(self.checkpoint_path)\n",
    "        config_data = checkpoint[\"config\"]\n",
    "\n",
    "        # Deserialize config for comparison\n",
    "        if isinstance(config_data, dict):\n",
    "            saved_config = FederatedConfig.deserialize(config_data)\n",
    "        else:\n",
    "            saved_config = config_data\n",
    "\n",
    "        # Validate using matches method\n",
    "        if not self.config.matches(saved_config):\n",
    "            logging.warning(\"Config mismatch in checkpoint, starting fresh training\")\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        self.global_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        logging.info(f\"Resumed from checkpoint: {self.checkpoint_path}\")\n",
    "        return checkpoint[\"round\"], checkpoint[\"best_val_loss\"]\n",
    "\n",
    "    def train_client(self, client_idx: int, model: LeNet) -> None:\n",
    "        \"\"\"Train a single client in-place.\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # batch_count = 0\n",
    "\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.config.LEARNING_RATE,\n",
    "            momentum=self.config.MOMENTUM,\n",
    "            weight_decay=self.config.WEIGHT_DECAY,\n",
    "            nesterov=True,\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = torch.amp.grad_scaler.GradScaler(device=self.device_type)\n",
    "\n",
    "        data_iterator = iter(self.client_loaders[client_idx])\n",
    "        try:\n",
    "            next_batch = next(data_iterator)\n",
    "            next_batch = [t.to(self.device, non_blocking=True) for t in next_batch]\n",
    "        except StopIteration:\n",
    "            logging.error(f\"Client {client_idx} has no data to train on\")\n",
    "            return\n",
    "\n",
    "        local_epochs = self.config.get_epochs_for_training()\n",
    "        total_steps = len(self.client_loaders[client_idx])\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            for step in range(total_steps):\n",
    "                current_batch = next_batch\n",
    "                try:\n",
    "                    next_batch = next(data_iterator)\n",
    "                    next_batch = [\n",
    "                        t.to(self.device, non_blocking=True) for t in next_batch\n",
    "                    ]\n",
    "                except StopIteration:\n",
    "                    if epoch + 1 < local_epochs:\n",
    "                        data_iterator = iter(self.client_loaders[client_idx])\n",
    "                        next_batch = next(data_iterator)\n",
    "                        next_batch = [\n",
    "                            t.to(self.device, non_blocking=True) for t in next_batch\n",
    "                        ]\n",
    "\n",
    "                inputs, targets = current_batch\n",
    "\n",
    "                # inputs = inputs.to(self.device, non_blocking=True)\n",
    "                # targets = targets.to(self.device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast_mode.autocast(device_type=self.device_type):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += predicted.eq(targets).sum().item()\n",
    "                    # batch_count += 1\n",
    "\n",
    "                    # Log metrics periodically\n",
    "                    if step % 10 == 0:\n",
    "                        avg_loss = total_loss / (step + 1)\n",
    "                        accuracy = 100.0 * correct / total\n",
    "\n",
    "                        # Global step calculation\n",
    "                        global_step = (\n",
    "                            epoch * len(self.client_loaders[client_idx]) + step\n",
    "                        )\n",
    "\n",
    "                        self.metrics.log_metrics(\n",
    "                            split=\"train\",\n",
    "                            loss=avg_loss,\n",
    "                            accuracy=accuracy,\n",
    "                            step=global_step,\n",
    "                        )\n",
    "\n",
    "                del inputs, targets, outputs, loss\n",
    "                if torch.cuda.is_available() and step % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "    def _shuffle_and_redistribute_models(\n",
    "        self, selected_clients: npt.NDArray\n",
    "    ) -> Dict[int, int]:\n",
    "        \"\"\"Shuffle and redistribute models among selected clients.\n",
    "        Returns mapping of client_id -> received_model_id\"\"\"\n",
    "        shuffled_indices = selected_clients.copy()\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        return {\n",
    "            client: shuffled_indices[i] for i, client in enumerate(selected_clients)\n",
    "        }\n",
    "\n",
    "    def train(self, max_patience: int = 50) -> None:\n",
    "        # Load existing checkpoint if available\n",
    "        start_round, best_val_loss = self.load_checkpoint()\n",
    "        best_model_state = (\n",
    "            self.global_model.state_dict().copy() if start_round > 0 else None\n",
    "        )\n",
    "\n",
    "        patience_counter = 0\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        if start_round > 0:\n",
    "            logging.info(f\"Resuming training from round {start_round}\")\n",
    "\n",
    "        round_idx = start_round\n",
    "        rounds = self.config.NUM_ROUNDS\n",
    "\n",
    "        round_pbar = tqdm(\n",
    "            range(start_round, rounds),\n",
    "            initial=start_round,\n",
    "            total=rounds,\n",
    "            desc=\"Training\",\n",
    "            unit=\"round\",\n",
    "            colour=\"green\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            for round_idx in round_pbar:\n",
    "                # Select clients\n",
    "                num_selected = max(\n",
    "                    1,\n",
    "                    int(self.config.PARTICIPATION_RATE * self.config.NUM_CLIENTS),\n",
    "                )\n",
    "                selected_clients = np.random.choice(\n",
    "                    self.config.NUM_CLIENTS,\n",
    "                    size=num_selected,\n",
    "                    replace=False,\n",
    "                    p=self.selection_probs,\n",
    "                )\n",
    "\n",
    "                # moves models to device and loads global model\n",
    "                for idx in selected_clients:\n",
    "                    self.client_models[idx].to(self.device)\n",
    "                    self.client_models[idx].load_state_dict(\n",
    "                        self.global_model.state_dict()\n",
    "                    )\n",
    "\n",
    "                # Train after models are loaded into device\n",
    "                for idx in selected_clients:\n",
    "                    self.train_client(idx, self.client_models[idx])\n",
    "\n",
    "                # Evaluate after first phase\n",
    "\n",
    "                val_loss_p1, val_acc_p1 = self._evaluate(self.val_loader)\n",
    "                if self.config.TWO_PHASE:\n",
    "                    # Phase 2: Shuffle and retrain\n",
    "                    logging.debug(\"Phase 2: Training with shuffled models\")\n",
    "                    model_assignments = self._shuffle_and_redistribute_models(\n",
    "                        selected_clients\n",
    "                    )\n",
    "\n",
    "                    # Train with shuffled models\n",
    "                    for client_idx, model_idx in model_assignments.items():\n",
    "                        self.client_models[client_idx].load_state_dict(\n",
    "                            self.client_models[model_idx].state_dict()\n",
    "                        )\n",
    "                        self.train_client(client_idx, self.client_models[client_idx])\n",
    "\n",
    "                # Aggregate models\n",
    "                self._aggregate_models(selected_clients)\n",
    "\n",
    "                # Evaluate\n",
    "                val_loss, val_acc = self._evaluate(self.val_loader)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = self.global_model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                    self.save_checkpoint(round_idx, best_val_loss)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                metrics_dict = {\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_accuracy\": val_acc,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"best_val_acc\": best_val_acc,\n",
    "                }\n",
    "\n",
    "                if self.config.TWO_PHASE:\n",
    "                    metrics_dict.update(\n",
    "                        {\n",
    "                            \"val_loss_phase1\": val_loss_p1,\n",
    "                            \"val_accuracy_phase1\": val_acc_p1,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Log metrics\n",
    "                self.metrics.log_fl_metrics(\n",
    "                    round_idx=round_idx,\n",
    "                    metrics=metrics_dict,\n",
    "                    client_stats={\n",
    "                        \"num_selected\": len(selected_clients),\n",
    "                        \"participation_rate\": len(selected_clients)\n",
    "                        / self.config.NUM_CLIENTS,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Update progress bar\n",
    "                postfix_dict = {\n",
    "                    \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                    \"val_acc\": f\"{val_acc:.2f}%\",\n",
    "                    \"best\": f\"{best_val_acc:.2f}%\",\n",
    "                }\n",
    "                if self.config.TWO_PHASE:\n",
    "                    postfix_dict.update({\"p1_acc\": f\"{val_acc_p1:.2f}%\"})\n",
    "                round_pbar.set_postfix(postfix_dict, refresh=True)\n",
    "\n",
    "                if patience_counter >= max_patience:\n",
    "                    logging.info(f\"Early stopping at round {round_idx}\")\n",
    "                    break\n",
    "\n",
    "            # Final evaluation\n",
    "            if best_model_state:\n",
    "                self.global_model.load_state_dict(best_model_state)\n",
    "            test_loss, test_acc = self._evaluate(self.test_loader)\n",
    "            self.metrics.log_metrics(\n",
    "                split=\"test\",\n",
    "                loss=test_loss,\n",
    "                accuracy=test_acc,\n",
    "                step=round_idx + 1,\n",
    "            )\n",
    "\n",
    "            self.save_checkpoint(round_idx + 1, best_val_loss)\n",
    "\n",
    "        finally:\n",
    "            self.metrics.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_plots(base_dir: Path) -> None:\n",
    "    \"\"\"Create comprehensive comparison plots for all experiments.\"\"\"\n",
    "\n",
    "    plots_dir = base_dir / \"comparison_plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Set style\n",
    "    plt.style.use(\"default\")  # Use matplotlib default style\n",
    "    # Set color cycle\n",
    "    plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\n",
    "        color=[\n",
    "            \"#1f77b4\",\n",
    "            \"#ff7f0e\",\n",
    "            \"#2ca02c\",\n",
    "            \"#d62728\",\n",
    "            \"#9467bd\",\n",
    "            \"#8c564b\",\n",
    "            \"#e377c2\",\n",
    "            \"#7f7f7f\",\n",
    "        ]\n",
    "    )\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    def load_metrics(run_dir: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load metrics from a run directory.\"\"\"\n",
    "        metrics_dir = run_dir / \"metrics\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Load validation metrics\n",
    "        val_file = metrics_dir / \"val_metrics.csv\"\n",
    "        if val_file.exists():\n",
    "            metrics[\"validation\"] = pd.read_csv(val_file)\n",
    "\n",
    "        # Load test metrics\n",
    "        test_file = metrics_dir / \"test_metrics.csv\"\n",
    "        if test_file.exists():\n",
    "            metrics[\"test\"] = pd.read_csv(test_file)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def plot_baseline_comparison():\n",
    "        \"\"\"Compare centralized vs federated baselines.\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Plot validation accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "\n",
    "        variants = {\n",
    "            \"Centralized\": base_dir / \"centralized\" / \"lenet_baseline\",\n",
    "            \"FedAvg Standard\": base_dir / \"federated\" / \"lenet_baseline_standard\",\n",
    "            \"FedAvg Two-Phase\": base_dir / \"federated\" / \"lenet_baseline_two_phase\",\n",
    "        }\n",
    "\n",
    "        for label, path in variants.items():\n",
    "            if path.exists():\n",
    "                metrics = load_metrics(path)\n",
    "                if \"validation\" in metrics:\n",
    "                    plt.plot(\n",
    "                        metrics[\"validation\"][\"step\"],\n",
    "                        metrics[\"validation\"][\"accuracy\"],\n",
    "                        label=label,\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "\n",
    "        plt.title(\"Validation Accuracy Comparison\")\n",
    "        plt.xlabel(\"Epochs/Rounds\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot validation loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "\n",
    "        variants = {\n",
    "            \"Centralized\": base_dir / \"centralized\" / \"lenet_baseline\",\n",
    "            \"FedAvg Standard\": base_dir / \"federated\" / \"lenet_baseline_standard\",\n",
    "            \"FedAvg Two-Phase\": base_dir / \"federated\" / \"lenet_baseline_two_phase\",\n",
    "        }\n",
    "\n",
    "        for label, path in variants.items():\n",
    "            if path.exists():\n",
    "                metrics = load_metrics(path)\n",
    "                if \"validation\" in metrics:\n",
    "                    plt.plot(\n",
    "                        metrics[\"validation\"][\"step\"],\n",
    "                        metrics[\"validation\"][\"loss\"],\n",
    "                        label=label,\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "\n",
    "        plt.title(\"Validation Loss: Centralized vs Federated\")\n",
    "        plt.xlabel(\"Epochs/Rounds\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_dir / \"baseline_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_participation_comparison():\n",
    "        \"\"\"Compare different participation strategies.\"\"\"\n",
    "        participation_dir = base_dir / \"federated_participation\"\n",
    "        if not participation_dir.exists():\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for run_dir in participation_dir.glob(\"lenet_participation_*\"):\n",
    "            variant = run_dir.name.split(\"_\")[-1]\n",
    "            metrics = load_metrics(run_dir)\n",
    "\n",
    "            if \"validation\" in metrics:\n",
    "                plt.plot(\n",
    "                    metrics[\"validation\"][\"step\"],\n",
    "                    metrics[\"validation\"][\"accuracy\"],\n",
    "                    label=f\"Participation: {variant}\",\n",
    "                    linewidth=2,\n",
    "                )\n",
    "\n",
    "        plt.title(\"Impact of Participation Strategies\")\n",
    "        plt.xlabel(\"Rounds\")\n",
    "        plt.ylabel(\"Validation Accuracy (%)\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            plots_dir / \"participation_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    def plot_heterogeneity_comparison():\n",
    "        \"\"\"Compare different heterogeneity settings.\"\"\"\n",
    "        heterogeneity_dir = base_dir / \"federated_heterogeneity\"\n",
    "        if not heterogeneity_dir.exists():\n",
    "            return\n",
    "\n",
    "        # Separate plots for different J values\n",
    "        j_values = set()\n",
    "        for run_dir in heterogeneity_dir.glob(\"lenet_heterogeneity_*\"):\n",
    "            # More robust parsing of J value\n",
    "            parts = run_dir.name.split(\"_\")\n",
    "            for part in parts:\n",
    "                if part.startswith(\"J\") and part[1:].isdigit():\n",
    "                    j = int(part[1:])\n",
    "                    j_values.add(j)\n",
    "                    break\n",
    "\n",
    "        for j in sorted(j_values):\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Group runs by training mode\n",
    "            for mode in [\"standard\", \"two_phase\"]:\n",
    "                # Match pattern for both standard and two-phase runs\n",
    "                pattern = f\"*J{j}*{mode}*\"\n",
    "\n",
    "                for run_dir in heterogeneity_dir.glob(pattern):\n",
    "                    # Parse run configuration\n",
    "                    name_parts = run_dir.name.split(\"_\")\n",
    "                    if \"iid\" in name_parts:\n",
    "                        label = f\"IID ({mode})\"\n",
    "                    else:\n",
    "                        # Find the part containing \"cls\"\n",
    "                        for part in name_parts:\n",
    "                            if \"cls\" in part:\n",
    "                                cls = part.replace(\"cls\", \"\")\n",
    "                                label = f\"{cls} classes/client ({mode})\"\n",
    "                                break\n",
    "                        else:\n",
    "                            continue  # Skip if we can't parse the configuration\n",
    "\n",
    "                    metrics = load_metrics(run_dir)\n",
    "                    if \"validation\" in metrics:\n",
    "                        plt.plot(\n",
    "                            metrics[\"validation\"][\"step\"],\n",
    "                            metrics[\"validation\"][\"accuracy\"],\n",
    "                            label=label,\n",
    "                            linewidth=2,\n",
    "                        )\n",
    "\n",
    "            plt.title(f\"Impact of Data Heterogeneity (J={j})\")\n",
    "            plt.xlabel(\"Rounds\")\n",
    "            plt.ylabel(\"Validation Accuracy (%)\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                plots_dir / f\"heterogeneity_J{j}_comparison.png\",\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "    def plot_training_mode_comparison():\n",
    "        \"\"\"Compare standard vs two-phase training across experiments.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        categories = [\"Baseline\", \"Participation\", \"Heterogeneity\"]\n",
    "        standard_accs = []\n",
    "        two_phase_accs = []\n",
    "\n",
    "        for category in categories:\n",
    "            if category == \"Baseline\":\n",
    "                path = base_dir / \"federated\"\n",
    "                standard = \"lenet_baseline_standard\"\n",
    "                two_phase = \"lenet_baseline_two_phase\"\n",
    "            else:\n",
    "                path = base_dir / f\"federated_{category.lower()}\"\n",
    "                standard = \"*_standard\"\n",
    "                two_phase = \"*_two_phase\"\n",
    "\n",
    "            # Get best accuracies for both modes\n",
    "            best_standard = 0\n",
    "            best_two_phase = 0\n",
    "\n",
    "            for mode, pattern in [(\"standard\", standard), (\"two_phase\", two_phase)]:\n",
    "                for run_dir in path.glob(pattern):\n",
    "                    metrics = load_metrics(run_dir)\n",
    "                    if \"validation\" in metrics:\n",
    "                        acc = metrics[\"validation\"][\"accuracy\"].max()\n",
    "                        if mode == \"standard\":\n",
    "                            best_standard = max(best_standard, acc)\n",
    "                        else:\n",
    "                            best_two_phase = max(best_two_phase, acc)\n",
    "\n",
    "            standard_accs.append(best_standard)\n",
    "            two_phase_accs.append(best_two_phase)\n",
    "\n",
    "        # Plot comparison\n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.bar(x - width / 2, standard_accs, width, label=\"Standard FedAvg\")\n",
    "        plt.bar(x + width / 2, two_phase_accs, width, label=\"Two-Phase FedAvg\")\n",
    "\n",
    "        plt.xlabel(\"Experiment Category\")\n",
    "        plt.ylabel(\"Best Validation Accuracy (%)\")\n",
    "        plt.title(\"Standard vs Two-Phase Training Comparison\")\n",
    "        plt.xticks(x, categories)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis=\"y\")\n",
    "\n",
    "        # Add value labels\n",
    "        for i, v in enumerate(standard_accs):\n",
    "            plt.text(i - width / 2, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "        for i, v in enumerate(two_phase_accs):\n",
    "            plt.text(i + width / 2, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            plots_dir / \"training_mode_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    # Generate all plots\n",
    "    plot_baseline_comparison()\n",
    "    plot_participation_comparison()\n",
    "    plot_heterogeneity_comparison()\n",
    "    plot_training_mode_comparison()\n",
    "\n",
    "    # Create a final summary plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Best results from each category\n",
    "    categories = {\n",
    "        \"Centralized\": base_dir / \"centralized\",\n",
    "        \"FedAvg Baseline\": base_dir / \"federated\",\n",
    "        \"Participation\": base_dir / \"federated_participation\",\n",
    "        \"Heterogeneity\": base_dir / \"federated_heterogeneity\",\n",
    "    }\n",
    "\n",
    "    best_accuracies = {}\n",
    "\n",
    "    for category, directory in categories.items():\n",
    "        best_acc = 0\n",
    "        if directory.exists():\n",
    "            for run_dir in directory.glob(\"**/summary.json\"):\n",
    "                with open(run_dir) as f:\n",
    "                    summary = json.load(f)\n",
    "                    if (\n",
    "                        \"final_metrics\" in summary\n",
    "                        and \"test\" in summary[\"final_metrics\"]\n",
    "                    ):\n",
    "                        acc = summary[\"final_metrics\"][\"test\"][\"best_accuracy\"]\n",
    "                        best_acc = max(best_acc, acc)\n",
    "        best_accuracies[category] = best_acc\n",
    "\n",
    "    # Plot best results\n",
    "    plt.bar(best_accuracies.keys(), best_accuracies.values())\n",
    "    plt.title(\"Best Test Accuracy Across Different Approaches\")\n",
    "    plt.ylabel(\"Test Accuracy (%)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, axis=\"y\")\n",
    "\n",
    "    for i, (category, acc) in enumerate(best_accuracies.items()):\n",
    "        plt.text(i, acc, f\"{acc:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / \"final_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compare_experiments(base_dir: Path) -> None:\n",
    "    \"\"\"Compare results across different experimental settings.\"\"\"\n",
    "    # Create comparison plots\n",
    "    create_comparison_plots(base_dir)\n",
    "\n",
    "    # Compare centralized vs federated baseline\n",
    "    MetricsManager.compare_runs(base_dir, \"centralized_baseline\")\n",
    "    MetricsManager.compare_runs(base_dir, \"federated_baseline\")\n",
    "\n",
    "    # Compare different participation schemes\n",
    "    MetricsManager.compare_runs(base_dir, \"federated_participation\")\n",
    "\n",
    "    # Compare different local steps\n",
    "    MetricsManager.compare_runs(base_dir, \"federated_heterogeneity\")\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    import gc\n",
    "\n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
    "        torch.xpu.empty_cache()\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect(generation=2)\n",
    "\n",
    "    if os.name == \"nt\":  # Windows\n",
    "        import ctypes\n",
    "\n",
    "        ctypes.windll.kernel32.SetProcessWorkingSetSize(-1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner\n",
    "\n",
    "This class, when the `run_all()` is called, will run all experiments keeping tracks of what was already done to recover from a potential runtime failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"Manages and runs all experiments sequentially with checkpointing.\"\"\"\n",
    "\n",
    "    def __init__(self, base_config: BaseConfig):\n",
    "        self.config = base_config\n",
    "        self.data = Cifar100DatasetManager(base_config)\n",
    "        self.experiments_dir = base_config.RUNS_DIR / \"experiments_status\"\n",
    "        self.experiments_dir.mkdir(exist_ok=True)\n",
    "        self.status_file = self.experiments_dir / \"completion_status.json\"\n",
    "        self.status = self._load_status()\n",
    "\n",
    "    def _load_status(self) -> dict:\n",
    "        \"\"\"Load or initialize experiment status.\"\"\"\n",
    "        if self.status_file.exists():\n",
    "            with open(self.status_file) as f:\n",
    "                return json.load(f)\n",
    "        return {\n",
    "            \"centralized_hyperparams\": {},\n",
    "            \"centralized_baseline\": False,\n",
    "            \"federated_baseline\": {\"standard\": False, \"two_phase\": False},\n",
    "            \"participation_studies\": {},\n",
    "            \"participation_studies_two_phase\": {},\n",
    "            \"heterogeneity_study\": {},\n",
    "            \"heterogeneity_study_two_phase\": {},\n",
    "        }\n",
    "\n",
    "    def _save_status(self) -> None:\n",
    "        \"\"\"Save current experiment status.\"\"\"\n",
    "        with open(self.status_file, \"w\") as f:\n",
    "            json.dump(self.status, f, indent=2)\n",
    "\n",
    "    def mark_completed(\n",
    "        self,\n",
    "        experiment: str,\n",
    "        variant: Optional[str] = None,\n",
    "        metrics: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Mark an experiment as completed with optional metrics.\"\"\"\n",
    "        if variant:\n",
    "            if experiment not in self.status:\n",
    "                self.status[experiment] = {}\n",
    "            self.status[experiment][variant] = {\"completed\": True, **(metrics or {})}\n",
    "        else:\n",
    "            self.status[experiment] = True\n",
    "        self._save_status()\n",
    "\n",
    "    def is_completed(self, experiment: str, variant: Optional[str] = None) -> bool:\n",
    "        \"\"\"Check if an experiment is completed.\"\"\"\n",
    "        if variant:\n",
    "            return self.status.get(experiment, {}).get(variant, False)\n",
    "        return self.status.get(experiment, False)\n",
    "\n",
    "    def run_centralized_baseline(self) -> None:\n",
    "        \"\"\"Run centralized training with hyperparameter search.\"\"\"\n",
    "        # If baseline is already completed, skip everything\n",
    "        if self.is_completed(\"centralized_baseline\"):\n",
    "            logging.info(\"Centralized baseline already completed, skipping...\")\n",
    "            return\n",
    "\n",
    "        # Grid search parameters\n",
    "        grid_search_epochs = 50\n",
    "        lr_values = [0.1, 0.01, 0.001]\n",
    "        schedulers = [\n",
    "            (\"cosine\", lambda opt: CosineAnnealingLR(opt, T_max=grid_search_epochs)),\n",
    "            (\"step\", lambda opt: StepLR(opt, step_size=30, gamma=0.1)),\n",
    "        ]\n",
    "\n",
    "        # Check if we need to do grid search\n",
    "        need_grid_search = False\n",
    "        for lr in lr_values:\n",
    "            for scheduler_name, _ in schedulers:\n",
    "                variant = f\"lr{lr}_{scheduler_name}\"\n",
    "                if not self.is_completed(\"centralized_hyperparams\", variant):\n",
    "                    need_grid_search = True\n",
    "                    break\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_config = None\n",
    "\n",
    "        if need_grid_search:\n",
    "            logging.info(\"Starting hyperparameter grid search...\")\n",
    "            # Grid search\n",
    "            for lr in lr_values:\n",
    "                for scheduler_name, scheduler_fn in schedulers:\n",
    "                    variant = f\"lr{lr}_{scheduler_name}\"\n",
    "\n",
    "                    if self.is_completed(\"centralized_hyperparams\", variant):\n",
    "                        logging.info(f\"Skipping completed hyperparams: {variant}\")\n",
    "                        # Load saved accuracy\n",
    "                        saved_acc = self.status[\"centralized_hyperparams\"][variant].get(\n",
    "                            \"val_acc\", 0\n",
    "                        )\n",
    "                        if saved_acc > best_val_acc:\n",
    "                            best_val_acc = saved_acc\n",
    "                            best_config = (lr, scheduler_name)\n",
    "                        continue\n",
    "\n",
    "                    logging.info(\n",
    "                        f\"Testing hyperparams: lr={lr}, scheduler={scheduler_name}\"\n",
    "                    )\n",
    "\n",
    "                    config = dataclasses.replace(self.config, LEARNING_RATE=lr)\n",
    "                    model = LeNet(config)\n",
    "                    trainer = CentralizedTrainer(\n",
    "                        model=model,\n",
    "                        config=config,\n",
    "                        experiment_name=f\"grid_search_{variant}\",\n",
    "                    )\n",
    "\n",
    "                    val_acc = trainer.train(\n",
    "                        train_loader=self.data.train_loader,\n",
    "                        val_loader=self.data.val_loader,\n",
    "                        test_loader=self.data.test_loader,\n",
    "                        max_epochs=grid_search_epochs,\n",
    "                        max_patience=10,\n",
    "                        scheduler_fn=scheduler_fn,\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    self.mark_completed(\n",
    "                        \"centralized_hyperparams\", variant, {\"val_acc\": val_acc}\n",
    "                    )\n",
    "\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_config = (lr, scheduler_name)\n",
    "\n",
    "                    cleanup_memory()\n",
    "\n",
    "        else:\n",
    "            # Load best config from saved results\n",
    "            logging.info(\"Loading best config from completed hyperparameter search...\")\n",
    "            for lr in lr_values:\n",
    "                for scheduler_name, _ in schedulers:\n",
    "                    variant = f\"lr{lr}_{scheduler_name}\"\n",
    "                    if variant in self.status[\"centralized_hyperparams\"]:\n",
    "                        val_acc = self.status[\"centralized_hyperparams\"][variant].get(\n",
    "                            \"val_acc\", 0\n",
    "                        )\n",
    "                        if val_acc > best_val_acc:\n",
    "                            best_val_acc = val_acc\n",
    "                            best_config = (lr, scheduler_name)\n",
    "\n",
    "        # Train final model with best hyperparameters\n",
    "        if not self.is_completed(\"centralized_baseline\") and best_config:\n",
    "            logging.info(f\"Training final model with best config: {best_config}\")\n",
    "            lr, scheduler_name = best_config\n",
    "            variant = f\"lr{lr}_{scheduler_name}\"\n",
    "\n",
    "            config = dataclasses.replace(self.config, LEARNING_RATE=lr)\n",
    "            model = LeNet(config)\n",
    "            trainer = CentralizedTrainer(\n",
    "                model=model, config=config, experiment_name=f\"baseline_{variant}\"\n",
    "            )\n",
    "\n",
    "            scheduler_fn = next(s[1] for s in schedulers if s[0] == scheduler_name)\n",
    "            trainer.train(\n",
    "                train_loader=self.data.train_loader,\n",
    "                val_loader=self.data.val_loader,\n",
    "                test_loader=self.data.test_loader,\n",
    "                scheduler_fn=scheduler_fn,\n",
    "            )\n",
    "\n",
    "            self.mark_completed(\"centralized_baseline\")\n",
    "            cleanup_memory()\n",
    "\n",
    "    def run_federated_baseline(self) -> None:\n",
    "        \"\"\"Run federated learning baseline with both training modes.\"\"\"\n",
    "        for mode in [\"standard\", \"two_phase\"]:\n",
    "            if self.is_completed(\"federated_baseline\", mode):\n",
    "                logging.info(f\"Federated baseline ({mode}) already completed\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Running federated baseline (IID) with {mode} training\")\n",
    "            config = FederatedConfig(TWO_PHASE=(mode == \"two_phase\"))\n",
    "            model = LeNet(config)\n",
    "            trainer = FederatedTrainer(\n",
    "                model=model,\n",
    "                train_dataset=self.data.train_dataset,\n",
    "                val_loader=self.data.val_loader,\n",
    "                test_loader=self.data.test_loader,\n",
    "                config=config,\n",
    "                experiment_name=f\"baseline_{mode}\",\n",
    "            )\n",
    "            trainer.train()  # type: ignore\n",
    "            self.mark_completed(\"federated_baseline\", mode)\n",
    "            cleanup_memory()\n",
    "\n",
    "    def run_participation_studies(self) -> None:\n",
    "        \"\"\"Run participation scheme experiments with both training modes.\"\"\"\n",
    "        gamma_values = [0.1, 0.5, 1.0]\n",
    "\n",
    "        for training_mode in [\"standard\", \"two_phase\"]:\n",
    "            status_key = f\"participation_studies{'_two_phase' if training_mode == 'two_phase' else ''}\"\n",
    "\n",
    "            for mode in [\"uniform\", \"skewed\"]:\n",
    "                for gamma in gamma_values:\n",
    "                    variant = f\"{mode}_gamma{gamma}\" if mode == \"skewed\" else \"uniform\"\n",
    "\n",
    "                    if self.is_completed(status_key, variant):\n",
    "                        logging.info(\n",
    "                            f\"Participation study {variant} ({training_mode}) already completed\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    logging.info(\n",
    "                        f\"Running participation study: {variant} with {training_mode} training\"\n",
    "                    )\n",
    "                    config = FederatedConfig(\n",
    "                        NUM_CLIENTS=100,\n",
    "                        PARTICIPATION_RATE=0.1,\n",
    "                        LOCAL_EPOCHS=4,\n",
    "                        PARTICIPATION_MODE=mode,\n",
    "                        DIRICHLET_ALPHA=gamma if mode == \"skewed\" else None,\n",
    "                        TWO_PHASE=(training_mode == \"two_phase\"),\n",
    "                    )\n",
    "                    model = LeNet(config)\n",
    "                    trainer = FederatedTrainer(\n",
    "                        model=model,\n",
    "                        train_dataset=self.data.train_dataset,\n",
    "                        val_loader=self.data.val_loader,\n",
    "                        test_loader=self.data.test_loader,\n",
    "                        config=config,\n",
    "                        experiment_name=f\"participation_{variant}_{training_mode}\",\n",
    "                    )\n",
    "                    trainer.train()\n",
    "                    self.mark_completed(status_key, variant)\n",
    "                    cleanup_memory()\n",
    "\n",
    "    def run_heterogeneity_study(self, settings: ExperimentSettings) -> None:\n",
    "        \"\"\"Run comprehensive heterogeneity study with both training modes.\"\"\"\n",
    "        for training_mode in [\"standard\", \"two_phase\"]:\n",
    "            status_key = f\"heterogeneity_study{'_two_phase' if training_mode == 'two_phase' else ''}\"\n",
    "\n",
    "            for nc in settings.Nc:\n",
    "                for j in settings.J_configs.keys():\n",
    "                    variant = f\"{'iid' if nc is None else f'noniid_{nc}cls'}_J{j}\"\n",
    "\n",
    "                    if self.is_completed(status_key, variant):\n",
    "                        logging.info(\n",
    "                            f\"Skipping completed study: {variant} ({training_mode})\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    logging.info(\n",
    "                        f\"Running study: {variant} with {training_mode} training\"\n",
    "                    )\n",
    "                    config = FederatedConfig(\n",
    "                        NUM_CLIENTS=settings.K,\n",
    "                        PARTICIPATION_RATE=settings.C,\n",
    "                        LOCAL_EPOCHS=j,\n",
    "                        NUM_ROUNDS=settings.get_rounds(j),\n",
    "                        CLASSES_PER_CLIENT=nc,\n",
    "                        TWO_PHASE=(training_mode == \"two_phase\"),\n",
    "                    )\n",
    "\n",
    "                    model = LeNet(config)\n",
    "                    trainer = FederatedTrainer(\n",
    "                        model=model,\n",
    "                        train_dataset=self.data.train_dataset,\n",
    "                        val_loader=self.data.val_loader,\n",
    "                        test_loader=self.data.test_loader,\n",
    "                        config=config,\n",
    "                        experiment_name=f\"heterogeneity_{variant}_{training_mode}\",\n",
    "                    )\n",
    "\n",
    "                    trainer.train()\n",
    "                    self.mark_completed(status_key, variant)\n",
    "                    cleanup_memory()\n",
    "\n",
    "    def run_all(self) -> None:\n",
    "        \"\"\"Run all experiments sequentially.\"\"\"\n",
    "        try:\n",
    "            # Baselines\n",
    "            self.run_centralized_baseline()\n",
    "            self.run_federated_baseline()\n",
    "\n",
    "            # Studies\n",
    "            self.run_participation_studies()\n",
    "\n",
    "            settings = ExperimentSettings(\n",
    "                # The default value of the settings are as follows:\n",
    "                # K=100,  # Number of clients\n",
    "                # C=0.1,  # Client fraction\n",
    "                # Nc=(None, 1, 5, 10, 50),  # None = IID\n",
    "                # J_configs={\n",
    "                #     4: 2000,  # Base configuration\n",
    "                #     8: 1000,  # Halved rounds\n",
    "                #     16: 500,  # Quarter rounds\n",
    "                # },\n",
    "            )\n",
    "            self.run_heterogeneity_study(settings)\n",
    "\n",
    "            # Generate final comparisons\n",
    "            compare_experiments(self.config.RUNS_DIR)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during experiments: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            cleanup_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "\n",
    "# Run all experiments\n",
    "runner = ExperimentRunner(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
